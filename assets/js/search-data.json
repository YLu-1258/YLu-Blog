{
  
    
        "post0": {
            "title": "An Empirical Study On Performance Measures Of Collaborative Filtering Recommendation Algorithms",
            "content": "An Empirical Study on Performance Measures of Collaborative Filtering Recommendation Algorithms . Alex Lu . maodou1258@gmail.com . Abstract . In recent times, recommendation engines have become increasingly popular within many industries. The focus of such an engine is to implement an algorithm to successfully make recommendations based on user preferences. Because of the utility provided by such tools, many industries heavily rely on such engines to provide accurate product recommendations to customers. Some popular algorithms implemented include clustering, matrix factorization, and deep learning implementations. Such algorithms are utilized within Collaborative Filtering methods and are simulated using pre-made datasets containing user and item information. Performance metrics are then applied to the algorithm to test the accuracy of the model. Results show that deep learning algorithms provide greater . performance when compared to other applications. . Keywords - Collaborative filtering, RMSE, Deep Learning, MAE, matrix factorization, Performance Measure . 1. Introduction . Latterly, recommendation engines serve a great purpose in many online services and enhance the consumer experience by providing lists of recommended items to users. Many big corporations such as Netflix, Amazon, Youtube, and others provide item recommendations through such methods to promote the sales and usage of their goods. The general idea of a recommender system is to return a list of items that the user would find interesting. Implementing various different types of engines provides insight towards which algorithm is the most suitable for certain scenarios. . Different approaches by different algorithms could have varying performances depending on the type of recommendation required, and the core nature of the dataset used. This research is primarily conducted on collaborative-filtering techniques, but similar research could be conducted on content-based implementations. Examples of potential implementations could be clustering and matrix factorization approaches. The main accuracy metrics utilized in the research are the RMSE and MAE metrics which are further explained later in the paper. . Two main research questions would serve as the focus of the research. More conclusions could be drawn from the data collected, but the main aim of the research are as follows: . I) What CF implementation provides the accuracy measure for a standard data set containing user, item and rating information? II) How does data sparsity affect the prediction accuracy of the models implemented? . 2. Background . Recommendation engines are programs used to provide item recommendations to users based on filtering each item and returning a . possible predicted rating for the user. The two main implementations of recommendation engines are content-based filtering and collaborative filtering. . 2.1 Content-based Filtering . A content-based filtering engine takes into account the user‚Äôs own item history and focuses on keywords in items rather than similarity between users [1]. However, such implementation can lead to a scenario where providing a broad range of recommendations would become impossible. Because of the nature of content-based filtering engines, items that were utilized by similar users wouldn‚Äôt be recommended to the main user purely because of the lack of a keyword or phrase. A dataset for such an engine would incorporate items along with a detailed profile for each item. . Despite its narrower scope when providing recommendations, content-based filtering systems often reduce the amount of data needed to make accurate predictions. For collaborative filtering, a greater range of data is required for a proper calculation of an item. To put it simply, the more data the engine has, the more . accurate the prediction is. However, with the nature of content-based filtering, any amount of data would suffice, providing that there are items that match the recommendation requirements. . 2.2 Collaborative Filtering . A collaborative filtering engine takes into account other user‚Äôs ratings, and returns . algorithms. This paper would primarily focus on model-based algorithms and approaches. The similarity between users could be found in various methods. For this study, a total of twelve algorithms were implemented. The main focus was placed on nearest neighbors, matrix factorization, and deep learning algorithms. Figure 2.1 maps out the various . . recommendations based on similarity. Collaborative filtering is split into two main approaches, model-based and memory based . algorithms implemented in this research. 2.3 Nearest Neighbor . Nearest neighbor, or more specifically, k nearest neighbors is a collaborative filtering . algorithm used to find similarities between items and users based on the total distance between two neighbors [2]. A weight system could also be applied so that a neighbor closer to the main user would have more weight on the recommendations than one who is further away. . Four of the twelve algorithms used in the research were from this category. KNNBasic, KNNWithZScore, KNNWithMeans, and KNNBaseline were such implementations of this category of algorithms. . 2.4 Matrix Factorization . Matrix Factorization is yet again another implementation of collaborative filtering. Simply put, a matrix factorization relates two separate values together under a specific value to create a grid or matrix of the data [3]. However, as data is not evenly distributed, some cells in the matrix are left empty and would need to be filled in to provide recommendations. Such values are known as ‚Äúlatent features‚Äù. . Some matrix factorizations implemented in the research were Negative Matrix Factorization, Singular Value Decomposition, and SVD++. . 2.5 Deep Learning . A deep learning model utilizes a neural network to process and calculate information. Much like a human brain, deep learning attempts to make predictions based on data much like a human brain. . The deep learning model implemented in this experiment utilizes an embedding layer structure [4]. Using such a structure could organize data into a vector of discrete values which could then produce similarity results and tests through the distance between vectors. These types of embedding layers could be generated through frameworks such as ‚ÄúPytorch‚Äù or ‚ÄúTensorflow‚Äù, however, this research would be implemented in Pytorch. The model could then be trained through a series of epochs and eventually provide predictions. . 2.6 Miscellaneous surprise algorithms . The miscellaneous category comprises the leftover algorithms in the surprise package that do not fit into any of the other categories in the research. Such algorithms were as listed: Co . Clustering, Normal Predictor, Baseline-Only, and Slope-One. . According to Sahar Karat Co-Clustering is a collaborative filtering algorithm used to provide recommendations by ‚ÄúA simultaneous clustering of the rows and columns of a matrix‚Äù [5]. Classical clustering algorithms only focus on one specific type of data, while co . clustering could be used to accommodate two simultaneously. . Normal predictor is an algorithm provided by the surprise package that provides random user recommendations based on the data distribution in the dataset. The ‚ÄúMaximum Likelihood Estimation‚Äù method is utilized in this calculation. . Baseline-Only is yet another surprise algorithm that makes baseline predictions based on the data provided. There are two main ways to implement this algorithm. The first implementation method is ‚ÄúStochastic Gradient . Descent‚Äù (SGD), which calculates a gradient of the dataset by a random selection of data. The other implementation is to use ‚ÄúAlternating Least Squares‚Äù (ALS) which is a matrix factorization algorithm that works well with sparser sets of data. . Slope-One is an example of an item-based collaborative filtering recommendation algorithm. Predictions made with this model are generally based on personal ratings as well as similar community ratings. . Further information and implementations on the methods listed could be found on the surprise documentations [6]. . 2.7 Accuracy Metrics . Two accuracy metrics were used in the process of this research. Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE) [11]. . RMSE is a quadratic model that assigns much larger weights to larger errors observed. Accuracy is calculated through RMSE by taking the square root of the mean of the sum of the squared difference of each predicted . value and its corresponding actual value. The formula is displayed in Figure 2.2 . . (Figure 2.2) . MAE is a linear metric model that linearly scales error. This model is typically used when higher errors are not important to the observation. Accuracy is calculated through MAE by taking the mean of the sum of the absolute values of the differences between the predicted and actual values. The formula is displayed in Figure 2.3 . . (Figure 2.3) . 3. Related Work . Various research topics have already been analyzed in the field of collaborative filtering. A few similar pieces of research provide similar methods and implementations. A plethora of CF models has been implemented and researched. . 3.1 Similar Research . Similar research was conducted by Mojdeh Saadati et al. [7] on the implementations of various models on movie recommender systems. The two models used in the experiment were the matrix factorization model SVD and a deep learning implementation of the Restricted Boltzmann Machine. The performance of the implementations was measured in the RMSE metric. However, in this research, we go over a broader spectrum of algorithms from each group of implementations and produce results for a larger picture between the different implementations. . 3.2 Further Research . He et al. [8] Conducted Research on how Matrix Factorization could be implemented with Deep Learning to create a better performing model for collaborative filtering. The proposed idea was to use two main models, the Generalized Matrix Factorization (GMF), and the Multi-Layer Perceptron (MLP) to create a hybrid between deep learning and matrix factorization implementations to create a new neural matrix factorization model . dubbed ‚ÄúNeuMF‚Äù. NeuMf was then compared to other well-known models such as KNN and ALS implementations and proved to have better accuracy and less training loss. . Yedder et al. [9] Researched the performance of the restricted Boltzmann machine. Various hidden units, learning rates, and other factors were incorporated into the research. Problems such as data sparsity were also encountered in the research and required other methods of implementation. This research also utilized the RMSE accuracy metric to rate the performance of the model. The research indicated an excellent RMSE measure of 0.46. . 4. Approach . This study was broken up into three parts, cleaning, implementation, and evaluation. We will first check for invalid values throughout the dataset, then begin to implement algorithms, and finally calculate performance values. . 4.1 Dataset . The data that would be primarily used in this study is the Kaggle dataset ‚ÄúAnime . Recommendations Database‚Äù [10] based on the data collected from myanimelist.net and is included in the reference section of the study. The two files contained within the database are the ‚Äúratings.csv‚Äù and ‚Äúanime.csv‚Äù. The research would mostly work with the ratings file, but the anime dataset could be further implemented in future research incorporating variables such as show genre and overall ratings. The anime dataset is separated into three separate columns. The user_id, anime_id, and ratings columns are provided for analyzing similarities between users. The format and example of the file are illustrated in Figure 4.1. . (Figure 4.1) . The python module ‚Äúpandas‚Äù was used to import the data into a data frame which could . then be cleaned and analyzed. Making a copy of the dataset to work on is recommended. Maintaining two different data frames would help compare the results on the original to the results in the cleaned dataset. . The first thing to take note of while cleaning is to remove all rows with a rating of ‚Äú-1‚Äù. Such rows represent user data that does not have a specified rating for the specific show that they watched. This step is crucial as having such data in the project would result in a sparse dataset, which directly decreases performance as shown later in the results. A total of 7.8 million rows of data were in the dataset, after cleaning, about 6.3 million rows remained. The distribution of this data is shown in figure 4.2. . . (Figure 4.2) . Another issue arose again while trying to apply the dataset to our engine. Certain cells in . our dataset possibly had ‚ÄòNaN‚Äô values, which would in turn cause an error in actual predictions where a value of ‚ÄòNaN‚Äô would be returned instead of an integer of the predicted rating. To resolve this issue, we first applied the ‚Äòto_numeric‚Äô method from the pandas module to convert each value into workable integer or float values. After, we could then finally lower the runtime of our experiment by decreasing the size of our dataset from 6 million down to 125,000. This number was picked for easier splitting while implementing other algorithms and deep learning which require testing and training datasets. From there, we then calculated the distribution of the ratings by counting the number of each rating in our dataset. After clearing, it‚Äôs paramount to re index the rows in the data frame. Removing the invalid A distribution of the ratings is demonstrated in Figure 4.3. . . (Figure 4.3) . A correlation heatmap could also be used to detect any possible correlations between the values given, however, as of now, there is no apparent relation. . . (Figure 4.4) . 4.2 Implementation . The python package scikit-surprise contains most of the algorithms implemented in this research. First and foremost, we must declare a rating scale for our dataset by utilizing the reader class, and also initializing our dataset with the Dataset class. We set a new . ‚Äúratings‚Äú object over the range of one to ten, which is represented in our data. Using the three columns from our pandas data frame, we could then load the data into surprise using the Dataset.load_from_df() method. . After the data is loaded, we could then specify a ‚Äúparam_list‚Äù dictionary. In the dictionary, the name of the specific parameters would be stored as the keys and the possible values as a list in the values. Further explanation of what specific params do for each algorithm is also provided in the surprise documentation. . The param_list dictionary could then be imputed into the GridSearchCV() method along with the algorithm name, performance measures, and a cross-validation iterator of 3. There are other arguments as well, but for the sake of our research, these four should be enough. The final step is to then fit our premade data with our GridSearchCV method with the .fit() function. Once everything is set up, the accuracy metrics could then just be retrieved via the best_score iterator. . The deep learning algorithm specifically was generated using an embedding layer with . dropout layers. The entire network consists of 4 total layers. After setting up the net, we can begin to train our model and loop over varying amounts of epochs to find the accuracy that is desired. . To determine the RMSE and the MAE values of this approach, we could separate our dataset into predictions and truth arrays. We can then apply the formulas for both RMSE and MAE as shown in Figures 2.2 and 2.3. We can implement these metrics in two ways. The first method is to use a NumPy array to subject each vector of values from each other and then to apply the formula to our newly generated values. Our second approach is more basic and rudimentary. We could subtract each truth value from each prediction value in our function by declaring a function and then apply the formula to our sum. . 5. Results . In this research, two sets of results were collected. The data collected on the cleaned dataset would serve to be our solution to the first question. However, the data collected on the original dataset would be utilized to answer . the second question concerning the performance measures on sparse datasets. 5.1 Cleaned Dataset . We have gathered both RMSE and MAE values for each algorithm used on our cleaned dataset from our implementations. In the first part of this experiment, the data set has already been cleaned of any invalid values and has reduced sparsity. The majority of the algorithms implemented all showed similar results except a couple of outliers and certain points. A table of the data collected is shown in Figure 5.1 . . (Figure 5.1) . Using the RMSE as the x-axis and MAE for the y-axis, we can plot a scatter plot of our data (Figure 5.2). . . (Figure 5.2) . Removing the Normal Predictor outlier value provides us with a clearer image of the differences in the lower valued points (Figure 5.3). . . (Figure 5.3) . Our data shows that the worst-performing algorithm that we had implemented was the Normal Predictor algorithm from the surprise package, while the best-performing implementation was the Deep Learning algorithm based on neural networks. . 5.1 Pre-cleaned Dataset . Loading up a new dataset, we can effectively run all our algorithms again while maintaining the sparsity of the original dataset. The only cleaning that had to be done was to remove all NaN values and also convert all data to numeric values. The results of running the . implementations on the sparser dataset are recorded in the figure below (Figure 4.5) . . (Figure 5.4) . The results of the various algorithms on this sparse dataset were also plotted on a scatter plot as shown below in Figure 5.5. . . (Figure 5.5) . Once again, removing the normal predictor outlier gives a clearer representation of our other data points (Figure 5.6). . . (Figure 5.6) . Although the values have increased by a considerable amount in the sparse dataset, the common trend remains between the various data points, and no changes are observed in the best and worst algorithms. Comparing the . results of the cleaned and original data sets, a clear difference could be observed in the RMSE and MAE measures (Figures 5.7-8) . (Figure 5.7) . (Figure 5.8) . 6. Discussion . After analyzing the data collected from the research, the deep learning algorithm and the KNNBaseline implementations were observed to be the best performing with the least error observed with both RMSE and MAE metrics. . A possible reason for the results could be the usage of randomness in the calculation of the Normal Predictor algorithm. The deep learning algorithm may have performed the best because of the various training cycles allocated to it which helped to create a more accurate model after each iteration. Vice versa, the opposite could also be applied to the two worst performing algorithms KNNBasic and Normal Predictor. Such implementations . had basic calculations and weren‚Äôt able to take into account outliers and other potential biases in the data. . There was an attempt in the research to implement a Restricted Boltzmann Machine (RBM) model, however, the implementation gave varying results and was difficult to judge the extra ratings of the implementation. This . research could be further pursued in the future with the addition of more deep learning implementations and a narrower focus on the subject of deep learning as a whole. From the results acquired, deep learning has been shown to have improved results compared to other algorithms. Focused research on deep learning implementations would provide the reasoning behind deep learning accuracy. . 7. Conclusion . In this research, we explored various machine learning algorithms, K-Nearest neighbors, Matrix Factorization, Deep learning, etc. Several approaches were implemented from the categories mentioned then tested for accuracy measures. . In both the cleaned and original datasets, the deep learning implementation was shown to the least margin of error when making recommendations. From this, it could be deduced that deep learning is a viable method for collaborative filtering engines working with user, item and rating data. Although different sets of data have varying optimal algorithms, . deep learning was still shown to be extremely accurate compared to other tested algorithms. Analyzing the data collected from the sparse dataset, we can conclude that a sparser set of data would result in less accurate recommendations, sometimes up to double the margin of error observed. Because of this observation, it can be concluded that collaborative filtering best performs with dense datasets. . References . [1] Kirzhner, Elena. ‚ÄúMachine Learning. Explanation of Collaborative . Filtering vs Content Based . Filtering.‚Äù Medium, Codeburst, 11 . May 2018, codeburst.io/explanation of-recommender-systems-in . information-retrieval-13077e1d916c. [2] Harrison, Onel. ‚ÄúMachine Learning Basics with the K-Nearest Neighbors Algorithm.‚Äù Medium, Towards Data Science, 14 July 2019, . towardsdatascience.com/machine . learning-basics-with-the-k-nearest . neighbors-algorithm-6a6e71d01761. . [3] Chen, Denise. ‚ÄúRecommendation System‚Ää-‚ÄäMatrix Factorization.‚Äù . Medium, Towards Data Science, 9 . July 2020, . towardsdatascience.com/recommend ation-system-matrix-factorization . d61978660b4b. . [4] Sivanantham, Balavivek. . ‚ÄúRecommendation System Implementation With Deep Learning and PyTorch.‚Äù Medium, The Startup, 18 Aug. 2020, . medium.com/swlh/recommendation system-implementation-with-deep learning-and-pytorch-a03ee84a96f4. . [5] Karat, Sahar. ‚ÄúCo . Clustering.‚Äù Data Science . Made Simpler, 5 Mar. 2016, datasciencemadesimpler.wor dpress.com/tag/co . clustering/. . [6] Hug, Nicholas. ‚ÄúWelcome to Surprise‚Äô Documentation.‚Äù Welcome to Surprise‚Äô . Documentation! - Surprise 1 Documentation, 2015, . surprise.readthedocs.io/en/sta ble/. . [7] Mojdeh Saadati, Syed Shihab, Mohammed Shaiqur Rahman ‚ÄúMovie Recommender Systems: Implementation and Performance . Evaluation.‚Äù Semantic Scholar, 2019, . www.semanticscholar.org/paper/Mo vie-Recommender-Systems%3A Implementation-and-Saadati Shihab/01470f39285213e53f365ce0 1417b18d12467563#citing-papers. . [8] Xiangnan, He, et al. ‚ÄúNeural Collaborative Filtering.‚Äù . International World Wide Web Conference Committee, 3 Apr. 2017. . [9] Yedder, Hanene Ben, et al. ‚ÄúModeling Prediction in . Recommender Systems Using Restricted Boltzmann Machine.‚Äù . IEEE Explore, IEEE, 5 Oct. 2017, ieeexplore.ieee.org/abstract/documen t/8122923. . [10] CooperUnion. ‚ÄúAnime . Recommendations Database.‚Äù . Kaggle, Kaggle, 21 Dec. 2016, . www.kaggle.com/CooperUnion/ani me-recommendations-database. . [11] Kampakis, Stylianos. ‚ÄúPerformance Measures: RMSE and MAE.‚Äù The Data Scientist, The Data Scientist, 26 Nov. 2020, . thedatascientist.com/performance measures-rmse-mae/. .",
            "url": "https://ylu-1258.github.io/YLu-Blog/2022/08/28/An-Empirical-Study-on-Performance-Measures-of-Collaborative-Filtering-Recommendation-Algorithms.html",
            "relUrl": "/2022/08/28/An-Empirical-Study-on-Performance-Measures-of-Collaborative-Filtering-Recommendation-Algorithms.html",
            "date": " ‚Ä¢ Aug 28, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Installation Checks for Alex Lu",
            "content": "Defining color vars . RED=&#39; 033[0;31m&#39; GREEN=&#39; 033[0;32m&#39; BLUE=&#39; 033[0;34m&#39; MAGENTA=&#39; 033[0;35m&#39; CYAN=&#39; 033[0;36m&#39; NC=&#39; 033[0;0m&#39; . Checking for python installation . function CheckPythonInstall() { echo -e &quot;${BLUE}Checking python version${NC}&quot; if [[ $(python --version) ]]; then pyversion=$(python --version | cut -d&quot; &quot; -f2) echo -e &quot;${GREEN} - Python version ${MAGENTA}$pyversion${GREEN} has been installed! ${NC}&quot; else echo -e &quot;${RED} - Python not found ${NC}&quot; fi } CheckPythonInstall #python --version #python3 --version . Checking python version - Python version 3.9.12 has been installed! . Checking for Java intsallation . function CheckJavaInstall() { echo -e &quot;${CYAN}Checking java version${NC}&quot; if [[ $(java --version) ]]; then javaversion=$(java --version | head -n 1 | cut -d&quot; &quot; -f2) echo -e &quot;${GREEN} - Java version ${MAGENTA}$javaversion${GREEN} has been installed! ${NC}&quot; else echo -e &quot;${RED} - Java not found ${NC}&quot; fi } function CheckJavaCInstall() { echo -e &quot;${CYAN}Checking java compiler version${NC}&quot; if [[ $(javac --version) ]]; then javacversion=$(javac --version | head -n 1 | cut -d&quot; &quot; -f2) echo -e &quot;${GREEN} - JavaC version ${MAGENTA}$javacversion${GREEN} has been installed! ${NC}&quot; else echo -e &quot;${RED} - JavaC not found ${NC}&quot; fi } CheckJavaInstall CheckJavaCInstall . Checking java version - Java version 11.0.16 has been installed! Checking java compiler version - JavaC version 11.0.16 has been installed! . Checking For Anaconda installation . function CheckJupyterInstall() { echo -e &quot;${CYAN}Checking anaconda version${NC}&quot; if [[ $(conda --version) ]]; then condaversion=$(conda --version | cut -d&quot; &quot; -f2) echo -e &quot;${GREEN} - Anaconda version ${MAGENTA}$condaversion${GREEN} has been installed! ${NC}&quot; else echo -e &quot;${RED} - Anaconda not found ${NC}&quot; fi } CheckJupyterInstall . Checking jupyter version - Anaconda version 4.13.0 has been installed! . Checking for Jupyter package installation . function CheckCondaPackageInstall() { echo -e &quot;${CYAN}Checking jupyter package version${NC}&quot; if [[ $(conda list | grep $1) ]]; then packageversion=$(conda list | grep $1 | awk &#39;{print $2}&#39;) echo -e &quot;${GREEN} - Conda pacakge ${MAGENTA}$1${GREEN} version ${MAGENTA}$packageversion${GREEN} has been installed! ${NC}&quot; else echo -e &quot;${RED} - Conda package $1 not found ${NC}&quot; fi } CheckCondaPackageInstall &quot;nodejs&quot; . Checking jupyter package version - Conda pacakge nodejs version 6.11.2 has been installed! . Checking installed Jupyter kernels . function CheckJupyterKernelInstall() { echo -e &quot;${CYAN}Checking jupyter ${MAGENTA}$1${CYAN} kernel installation${NC}&quot; if [[ $(jupyter kernelspec list | grep $1) ]]; then kernelpath=$(jupyter kernelspec list | grep $1 | awk &#39;{print $2}&#39;) echo -e &quot;${GREEN} - Jupyter ${MAGENTA}$1${GREEN} kernel has been found at ${MAGENTA}$kernelpath${NC}&quot; else echo -e &quot;${RED} - Jupyter ${MAGENTA}$1${GREEN} kernel not found ${NC}&quot; fi } kernels=&quot;bash javascript python3&quot; for i in $kernels; do CheckJupyterKernelInstall $i done . Checking jupyter bash kernel installation - Jupyter bash kernel has been found at /home/eris29/.local/share/jupyter/kernels/bash Checking jupyter javascript kernel installation - Jupyter javascript kernel has been found at /home/eris29/.local/share/jupyter/kernels/javascript Checking jupyter python3 kernel installation - Jupyter python3 kernel has been found at /home/eris29/.local/share/jupyter/kernels/python3 .",
            "url": "https://ylu-1258.github.io/YLu-Blog/bash_checks",
            "relUrl": "/bash_checks",
            "date": " ‚Ä¢ Aug 26, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "08/25/2022 Lecture",
            "content": "Lecture - 08/25/2022 . To build up a relationshhip with my partner, I should comment in his blog and also open up review tickets to communicate with him . When working with others, always pull before you make any additions, this ensures that we are on the latest version and that there are no difference conflicts. . There are multiple ‚Äúshells‚Äù installed on a linux machine, such as bash, dash and zsh. The commands that we enter into terminal are like a pseudo-language. We can create bash scripts to automate actions in terminal for us. . Bash - Analyzing part 1 . cd $project #cd means &quot;Change Directory&quot;, $project is a variable named project ls # lists directory ls -a # lists directory with hidden files ls -al # lists directory with hidden files in long format . By using bash we can easily create scripts that automate terminal operations for us. . The Cloud . The cloud contains all git repositories. Individual computers can clone repositories from the cloud and down to our own SSD. This creates a link between our local repository and the remote repository. . A pull action will pull any new updates made to the repository down to our local repository and update it with the latest changes. . A push action will push any new updates from our local repository up to our remote directory in the cloud and contribute to the git repository. .",
            "url": "https://ylu-1258.github.io/YLu-Blog/markdown/notes/2022/08/25/Lecture-Intro-Python.html",
            "relUrl": "/markdown/notes/2022/08/25/Lecture-Intro-Python.html",
            "date": " ‚Ä¢ Aug 25, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "08/24/2022 Lecture",
            "content": "Lecture - 08/24/2022 . Using documents as blog posts . To import past assignemnts and documents from document-based editors such as word and google docs, we can import such files as .docx documents and place them under the _word subdirectory in our blog. . NOTE: Doing so does not keep the formatting of the document, any font colors, size, styling is not preserved, additionall work with CSS is required for original effects. . Blog Front-Matter . Each markdown or jupyter post contains a set configurations at the head of the file known as Front-Matter Front-matter settings are seperated into two main groups, keys, and values. . keys: The name of the configuration or setting we wish to edit value: The value or data we grant to a specific configuration . IMPORTANT: It is crucial to always pair a key with a value, a blank value on the key overwrites the default value, making the key take on a null value and breaking the front-matter To define front matter in markdown, use the following format toc: true layout: post description: APCSP Lecture 2 categories: [markdown, notes] title: 08/24/2022 Lecture author: Alex Lu show_tags: true hide: true comments: true ... . To define front-matter in computational notebooks, use the following format # Jupyter Notebook Demonstration &gt; My first Jupyter notebook on my blog! - toc: true - title: First Jupyter Notebook - author: Alex Lu - badges: true - comments: true - categories: [jupyter] . NOTE: A title and and description must be specified with the # and &gt; characters respectively, furthermore, each front-matter key and value should be prefixed with a hyphen (-) similar to a markdown list. . Adding pages on the navbar . If we ever find the need to add a special page on the top of our site in the navbar, simply move the post into the _pages directory, and change the front matter key layout from post to page . NOTE: setting a table of contents in the front-matter does not work for a page, further tinkering with html is required. . _config.yml . Most of the blog‚Äôs default keys and values are defined within the _config.yml configuration file in the base directory of the blog. The values under _config.yml are in the standard key: value syntax prevalent in most .yml files. NOTABLE KEYS: | Key | function | | - | - | | title | Title of site in upper left hand corner | | baseurl | The url path to the blog | | show_description | Display brief description of blog post uner blog lists | | show_image | Display image on post card | .",
            "url": "https://ylu-1258.github.io/YLu-Blog/markdown/notes/2022/08/24/Lecture-site-organization-and-configuration.html",
            "relUrl": "/markdown/notes/2022/08/24/Lecture-site-organization-and-configuration.html",
            "date": " ‚Ä¢ Aug 24, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Python Quiz",
            "content": "Here is the code for my quiz . Colors really help to spruce things up a little, can you get 100% without looking at the code? . Example Playthrough: . import getpass class Colors: &#39;&#39;&#39; Colors class to print colored text to terminal, Does not work in fastpages :/ &#39;&#39;&#39; # Man I love ANSI PINK = &#39; 033[95m&#39; LILAC = &#39; 033[94m&#39; BLUE = &#39; 033[96m&#39; GREEN = &#39; 033[92m&#39; YELLOW = &#39; 033[93m&#39; RED = &#39; 033[91m&#39; ENDC = &#39; 033[0m&#39; BOLD = &#39; 033[1m&#39; UNDERLINE = &#39; 033[4m&#39; class Quiz(): # Main class for the quiz def __init__(self): # Quiz &quot;constructor&quot;, initiates list of questions and answers and other settings. self.ques = [] self.ans = [] self.colors = Colors() self.showans = False # Show correct answers after a wrong input self.total = 0 # Total questions self.correct = 0 # Total right self.skipped = 0 # Total skipped def addQues(self, question, answer): &#39;&#39;&#39; Adding Questions and answers to the quiz &#39;&#39;&#39; self.ques.append(question) self.ans.append(answer) self.total += 1 # increment total by 1 def askQues(self, idx): &#39;&#39;&#39; Helper function to ask one question, check answer, increment student data &#39;&#39;&#39; print(self.colors.PINK + &quot;QUESTION: &quot; + self.colors.ENDC + self.colors.BOLD + self.ques[idx] + self.colors.ENDC) # Print Question rsp = input(self.colors.YELLOW + &quot;What is your response? &quot; + self.colors.ENDC) # Get Response if rsp.lower() == self.ans[idx].lower(): # Check answer print(self.colors.GREEN + &quot;YOU ARE CORRECT!&quot; + self.colors.ENDC + &quot; Response &quot; + self.colors.BLUE + rsp + self.colors.ENDC + &quot; is correct!&quot;) self.correct += 1 elif rsp.lower() == &quot;/s&quot;: if self.showans: print(self.colors.YELLOW + &quot;Skipping....&quot; + &quot; The right answer was &quot; + self.colors.BLUE + self.ans[idx] + self.colors.ENDC) else: rint(self.colors.YELLOW + &quot;Skipping....&quot; + self.colors.ENDC) self.skipped += 1 else: if self.showans: print(self.colors.RED + &quot;Response &#39;&quot; + rsp + &quot;&#39; is incorrect.&quot; + self.colors.ENDC + &quot; The right answer was &quot; + self.colors.BLUE + self.ans[idx] + self.colors.ENDC) else: print(self.colors.RED + &quot;Response &#39;&quot; + rsp + &quot;&#39; is incorrect.&quot; + self.colors.ENDC) def percentage(self, x, y): &#39;&#39;&#39; Function to calculate percentage correct &#39;&#39;&#39; return 100 * float(x)/float(y) def playQuiz(self): &#39;&#39;&#39; Main Quiz loop, set settings and ask questions &#39;&#39;&#39; show_ans = input(self.colors.UNDERLINE + &quot;Would you like to show correct answers after incorrect responses? [y/n]&quot; + self.colors.ENDC + &quot; &quot;) print(self.colors.YELLOW + &quot;GOOD LUCK {0}!&quot;.format(getpass.getuser().upper()) + &quot; Type &#39;&quot; + self.colors.BLUE + &quot;/s&quot; + self.colors.YELLOW + &quot;&#39; to skip!&quot;) if show_ans in [&quot;y&quot;, &quot;yes&quot;, &quot;Y&quot;, &quot;YES&quot;]: # Multiple cases of user inputs, assume any other input / no input == False. self.showans = True for i in range(0,self.total): # Iterate over all questions, no repetitive code here self.askQues(i) # Print a little congratulations message print(self.colors.LILAC + &quot;Congratulations! you got &quot; + self.colors.GREEN + &#39;{0:.2f}&#39;.format(self.percentage(self.correct, self.total)) + &quot;%&quot; + self.colors.LILAC + &quot; and {0} questions skipped on this quiz!&quot;.format(self.colors.YELLOW + str(self.skipped) + self.colors.LILAC)) # Creating our quiz q1 = Quiz() # AYO CHEATER STOP LOOKIN HERE q1.addQues(&quot;Name the Python output command mentioned in this lesson?&quot;, &quot;print&quot;) q1.addQues(&quot;If you see many lines of code in order, what would College Board call it?&quot;, &quot;sequence&quot;) q1.addQues(&quot;What keyword in python is used to describe a function?&quot;, &quot;def&quot;) q1.addQues(&quot;What command is used to include other functions that were previously developed?&quot;, &quot;import&quot;) q1.addQues(&quot;What command is used to evaluate correct or incorrect response in this quiz?&quot;, &quot;if&quot;) q1.addQues(&quot;Each &#39;if&#39; command contains an &#39;_________&#39; to determine a true or false condition?&quot;, &quot;expression&quot;) q1.addQues(&quot;What is an input to a function or method called?&quot;, &quot;parameter&quot;) q1.addQues(&quot;If Input is data the computer receives, what is the data that the computer sends back?&quot;, &quot;output&quot;) q1.addQues(&quot;What is a reusable block of code called?&quot;, &quot;function&quot;) q1.addQues(&quot;What operator is used for string concatenation in Python?&quot;, &quot;+&quot;) q1.playQuiz() . GOOD LUCK ERIS29! Type &#39;/s&#39; to skip! QUESTION: Name the Python output command mentioned in this lesson? YOU ARE CORRECT! Response print is correct! QUESTION: If you see many lines of code in order, what would College Board call it? YOU ARE CORRECT! Response sequence is correct! QUESTION: What keyword in python is used to describe a function? YOU ARE CORRECT! Response def is correct! QUESTION: What command is used to include other functions that were previously developed? Response &#39;include&#39; is incorrect. The right answer was import QUESTION: What command is used to evaluate correct or incorrect response in this quiz? YOU ARE CORRECT! Response if is correct! QUESTION: Each &#39;if&#39; command contains an &#39;_________&#39; to determine a true or false condition? Skipping.... The right answer was expression QUESTION: What is an input to a function or method called? YOU ARE CORRECT! Response parameter is correct! QUESTION: If Input is data the computer receives, what is the data that the computer sends back? YOU ARE CORRECT! Response output is correct! QUESTION: What is a reusable block of code called? YOU ARE CORRECT! Response function is correct! QUESTION: What operator is used for string concatenation in Python? YOU ARE CORRECT! Response + is correct! Congratulations! you got 80.00% and 1 questions skipped on this quiz! .",
            "url": "https://ylu-1258.github.io/YLu-Blog/jupyter/2022/08/23/alex-quiz.html",
            "relUrl": "/jupyter/2022/08/23/alex-quiz.html",
            "date": " ‚Ä¢ Aug 23, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "First Jupyter Notebook",
            "content": "Let&#39;s Print Something! . Allow me to demonstrate the print() function in python! . var = &quot;World!&quot; print(&quot;Hello &quot; + var) . Hello World! . Let&#39;s Print some more! . Loops are really fun sometimes, let&#39;s say it a couple more times üîÅ . for i in range(5): print(&quot;Hello &quot; + var) . Hello World! Hello World! Hello World! Hello World! Hello World! . Have a Bogosort algorithm! . The real question is what time complexity this algorithm has ü§î . from random import shuffle as sh def sorted(arr): length = len(arr) for i in range(0, length-1): if (arr[i] &gt; arr[i+1] ): return False return True def shuffle(arr): sh(arr) def Bogosort(arr): while not sorted(arr): shuffle(arr) return arr array = [23,10,49,9] print(Bogosort(array)) . [9, 10, 23, 49] .",
            "url": "https://ylu-1258.github.io/YLu-Blog/jupyter/2022/08/20/example-post.html",
            "relUrl": "/jupyter/2022/08/20/example-post.html",
            "date": " ‚Ä¢ Aug 20, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "Hello World!",
            "content": "Hello World! üåé . This is my first blog post! I‚Äôm Alex Lu, and I‚Äôm excited to learn more about how to build and develop my blog. . Have some code! üíª . Ayo is that a recursive function? . def fibo(n): if n == 1: return 0 if n == 2: return 1 return fibo(n-1) + fibo(n-2) print(fibo(10)) # prints 34 . That was sick! . Thanks for visiting ü•≥ . Have a cookie üç™ .",
            "url": "https://ylu-1258.github.io/YLu-Blog/markdown/2022/08/19/hello-world.html",
            "relUrl": "/markdown/2022/08/19/hello-world.html",
            "date": " ‚Ä¢ Aug 19, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.‚Ü© . 2. This is the other footnote. You can even have a link!‚Ü© .",
            "url": "https://ylu-1258.github.io/YLu-Blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " ‚Ä¢ Feb 20, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a ‚Äúlevel 1 heading‚Äù in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here‚Äôs a footnote 1. Here‚Äôs a horizontal rule: . . Lists . Here‚Äôs a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes ‚Ä¶and‚Ä¶ . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote.¬†&#8617; . |",
            "url": "https://ylu-1258.github.io/YLu-Blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " ‚Ä¢ Jan 14, 2020"
        }
        
    
  

  
  

  

  
      ,"page2": {
          "title": "Vocab",
          "content": "Vocab . The Vocab should be oriented in this fashion . Vocab | Definition | Example | . Week 0 Vocab . Vocab Definition Example . Output | Data that is returned by the computer to the user | print(‚ÄúHello world!‚Äù) | . Input | Data that is taken in by the computer from the user | input(‚ÄúWhat is your age‚Äù) | .",
          "url": "https://ylu-1258.github.io/YLu-Blog/vocab/",
          "relUrl": "/vocab/",
          "date": ""
      }
      
  

  
      ,"page3": {
          "title": "Lecture Notes",
          "content": "What is this page? . This page contains all of the notes I‚Äôve taken during Mr. Mortensen‚Äôs lectures, this might be useful while I‚Äôm trouble shooting my blog or other bugs in my code. If you‚Äôre another APCSP student, feel free to use this page to further your knowledge or review any points you missed in class! . Lectures TOC . Date Link and subject . 08 - 25 - 2022 | Bash and Cloud Lecture | . 08 - 24 - 2022 | Blog posting and configuration Lecture | .",
          "url": "https://ylu-1258.github.io/YLu-Blog/notes/",
          "relUrl": "/notes/",
          "date": ""
      }
      
  

  
      ,"page4": {
          "title": "About Me",
          "content": "Hello There! . My name is Alex Lu and if you want to reach out to me about anything, you can contact me through the methods below! . Discord: Eris29#2693 üéÆ | Email: maodou1258@gmail.com üìß | Phone: (858)-688-4567 üì± | . About Me üìì: . I am currently a Junior at Del Norte High School taking the APCSP course, I hope to major in computer science in the future and also pursue it as a prospective carreer path in the future . My Hobbies üéæ . I have many hobbies that I do to pass my time, here are just a few of them: . Reading | Programming | Playing tennis | Playing video games | Watching youtube videos | And many more! | . My Interests üî¨: . I am interested in anything related to computers, I am currently working on various projects centered around: . Machine Learning ü§ñ | Image processing üñºÔ∏è | Webscraping üåê | Cyber Security üê±‚Äçüíª | .",
          "url": "https://ylu-1258.github.io/YLu-Blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  

  
  

  
  

  
  

  
  

  
  

  
      ,"page12": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "https://ylu-1258.github.io/YLu-Blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}