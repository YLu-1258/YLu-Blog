{
  
    
        "post0": {
            "title": "Unit 2.4b Using Programs with Data, SQL",
            "content": "Database Programming is Program with Data . Each Tri 2 Final Project should be an example of a Program with Data. . Prepare to use SQLite in common Imperative Technique . Explore SQLite Connect object to establish database connection- Explore SQLite Cursor Object to fetch data from a table within a database | . Schema of Users table in Sqlite.db . Uses PRAGMA statement to read schema. . Describe Schema, here is resource Resource- What is a database schema? . - **A database schema serves as the outline and a blueprint of all tables and associated attributes within a database.** . What is the purpose of identity Column in SQL database? The Identity colun is a numeric column in databases that can be automatically populated with numeric values. Typically, to declare an identity column in the database schema, we would define a seed (intial value) and an incrementation value. Each table can only have 1 identity column. | . | What is the purpose of a primary key in SQL database? The primary key in a SQL database serves to act as an identifier for a particular database entry. The values in this column must be unique and non-NULL. Similar to the identity column, each table in a database can only have 1 primary key. On the other hand, a composite key is one that operates with multiple columns in a table, and serves to uniquely identify a row based on the contents of the corresponding columns. EX: ProductID and UserID, these columns could have non-unique IDs, but every pair in the database must be unique. | . | . | What are the Data Types in SQL table? There are many data types for SQL. The most common types represent string and numeric data | . | . Examples: | Type | Name | Purpose | | - | - | - | | String | CHAR(size) | Stores a fixed length of strings. Default length is 1 | | String | VARCHAR(size) | Stores are variable length string. | | String | TEXT(size) | Holds a string with a maximum size of 65,535 bytes | | | | | | Numeric | INTEGER(size) | Stores a signed integer (-2147483648 to 2147483647) or unsigned integer (0 to 4294967295) | | Numeric | FLOAT(p) or DOUBLE() | Stores a floating point number, double can store more bits than FLOAT | . import sqlite3 database = &#39;instance/sqlite.db&#39; # this is location of database def schema(): # Connect to the database file conn = sqlite3.connect(database) # Create a cursor object to execute SQL queries cursor = conn.cursor() # Fetch results of Schema results = cursor.execute(&quot;PRAGMA table_info(&#39;users&#39;)&quot;).fetchall() # Print the results for row in results: print(row) # Close the database connection conn.close() schema() . (0, &#39;id&#39;, &#39;INTEGER&#39;, 1, None, 1) (1, &#39;_name&#39;, &#39;VARCHAR(255)&#39;, 1, None, 0) (2, &#39;_uid&#39;, &#39;VARCHAR(255)&#39;, 1, None, 0) (3, &#39;_password&#39;, &#39;VARCHAR(255)&#39;, 1, None, 0) (4, &#39;_dob&#39;, &#39;DATE&#39;, 0, None, 0) . Reading Users table in Sqlite.db . Uses SQL SELECT statement to read data . What is a connection object? After you google it, what do you think it does? A connection object represents the connection to an open SQLite database. It contains all the attributes and methods that we may use to alter and manipulate the database. | . | Same for cursor object? A cursor object represents and invokes methods to execute SQL queries on a database. Using it we can fetch and alter database entries in our DB. | . | Look at conn object and cursor object in VSCode debugger. What attributes are in the object? Looking at the VSCode debugger, we can see that these objects contain certain special variables function variables and class variables for the Connection object. We can also see that it represents many of the methods we use to modifiy the database, such as execute() and accessing data fetchall() | . | Is &quot;results&quot; an object? How do you know? &quot;results&quot; is an object because we can see that it contains many attributes and methods of it&#39;s own in the debug menu | . | . import sqlite3 def read(): # Connect to the database file conn = sqlite3.connect(database) # Create a cursor object to execute SQL queries cursor = conn.cursor() # Execute a SELECT statement to retrieve data from a table results = cursor.execute(&#39;SELECT * FROM users&#39;).fetchall() # Print the results if len(results) == 0: print(&quot;Table is empty&quot;) else: for row in results: print(row) # Close the cursor and connection objects cursor.close() conn.close() read() . (1, &#39;Thomas Edison&#39;, &#39;toby&#39;, &#39;sha256$IQiLG4AbM5dcnNoL$e49522d138cacc1e98785d482fe66d039bf2e487c5b371538831102c306641db&#39;, &#39;1847-02-11&#39;) (2, &#39;Nikola Tesla&#39;, &#39;niko&#39;, &#39;sha256$S7wEhsAIHhuowABr$30eb9f4f186b88d6a642aa13590dfaf97a9dc039e5536bf86c21c81d5da32dda&#39;, &#39;2023-03-15&#39;) (3, &#39;Alexander Graham Bell&#39;, &#39;lex&#39;, &#39;sha256$bCwwzDSaMq7lWwID$d80eae2665c1fa612437db0c1fded6c1ed42fa66ba4183560661aeaa41619d02&#39;, &#39;2023-03-15&#39;) (4, &#39;Eli Whitney&#39;, &#39;whit&#39;, &#39;sha256$QhfkU8j8aR4WVAfd$d31b07e2a7a1f66498d5e1da4d30bd08873fd134058149bdd28404f4ef9e200a&#39;, &#39;2023-03-15&#39;) (5, &#39;Indiana Jones&#39;, &#39;indi&#39;, &#39;sha256$geO9CNSTl4xs8gg9$c30ee2df54229ad7ac640c89ecacf38d00ec3e48b64515f7fe285ce4e1c4a5c1&#39;, &#39;1920-10-21&#39;) (6, &#39;Marion Ravenwood&#39;, &#39;raven&#39;, &#39;sha256$79mFg8xP1wDlScb1$bd6a81f36bd3bca1f841385aed7dd4faefc820ace67175fa0d6b54d59b674ff1&#39;, &#39;1921-10-21&#39;) (7, &#39;alex&#39;, &#39;&#39;, &#39;sha256$is4jvpXEdcAguC7y$9ed42aa62953f3cb3a2ddf32fd594e6db19f1feea3d3e7b68a6b47be7cbcd53d&#39;, &#39;1940-01-01&#39;) (8, &#39;Alexander Lu&#39;, &#39;1337&#39;, &#39;password&#39;, &#39;2006-11-29&#39;) . Create a new User in table in Sqlite.db . Uses SQL INSERT to add row . - Compore create() in both SQL lessons. What is better or worse in the two implementations? . Explain purpose of SQL INSERT. Is this the same as User init? | . import sqlite3 def create(): name = input(&quot;Enter your name:&quot;) uid = input(&quot;Enter your user id:&quot;) password = input(&quot;Enter your password&quot;) dob = input(&quot;Enter your date of birth &#39;YYYY-MM-DD&#39;&quot;) # Connect to the database file conn = sqlite3.connect(database) # Create a cursor object to execute SQL commands cursor = conn.cursor() try: # Execute an SQL command to insert data into a table cursor.execute(&quot;INSERT INTO users (_name, _uid, _password, _dob) VALUES (?, ?, ?, ?)&quot;, (name, uid, password, dob)) # Commit the changes to the database conn.commit() print(f&quot;A new user record {uid} has been created&quot;) except sqlite3.Error as error: print(&quot;Error while executing the INSERT:&quot;, error) # Close the cursor and connection objects cursor.close() conn.close() create() . A new user record 1337 has been created . Updating a User in table in Sqlite.db . Uses SQL UPDATE to modify password . What does the hacked part do? | Explain try/except, when would except occur? | What code seems to be repeated in each of these examples to point, why is it repeated? | . import sqlite3 def update(): uid = input(&quot;Enter user id to update&quot;) password = input(&quot;Enter updated password&quot;) if len(password) &lt; 2: message = &quot;hacked&quot; password = &#39;gothackednewpassword123&#39; else: message = &quot;successfully updated&quot; # Connect to the database file conn = sqlite3.connect(database) # Create a cursor object to execute SQL commands cursor = conn.cursor() try: # Execute an SQL command to update data in a table cursor.execute(&quot;UPDATE users SET _password = ? WHERE _uid = ?&quot;, (password, uid)) if cursor.rowcount == 0: # The uid was not found in the table print(f&quot;No uid {uid} was not found in the table&quot;) else: print(f&quot;The row with user id {uid} the password has been {message}&quot;) conn.commit() except sqlite3.Error as error: print(&quot;Error while executing the UPDATE:&quot;, error) # Close the cursor and connection objects cursor.close() conn.close() #update() . Delete a User in table in Sqlite.db . Uses a delete function to remove a user based on a user input of the id. . Is DELETE a dangerous operation? Why? | In the print statemements, what is the &quot;f&quot; and what does {uid} do? | . import sqlite3 def delete(): uid = input(&quot;Enter user id to delete&quot;) # Connect to the database file conn = sqlite3.connect(database) # Create a cursor object to execute SQL commands cursor = conn.cursor() try: cursor.execute(&quot;DELETE FROM users WHERE _uid = ?&quot;, (uid,)) if cursor.rowcount == 0: # The uid was not found in the table print(f&quot;No uid {uid} was not found in the table&quot;) else: # The uid was found in the table and the row was deleted print(f&quot;The row with uid {uid} was successfully deleted&quot;) conn.commit() except sqlite3.Error as error: print(&quot;Error while executing the DELETE:&quot;, error) # Close the cursor and connection objects cursor.close() conn.close() #delete() . Menu Interface to CRUD operations . CRUD and Schema interactions from one location by running menu. Observe input at the top of VSCode, observe output underneath code cell. . Why does the menu repeat? | Could you refactor this menu? Make it work with a List? | . def menu(): operation = input(&quot;Enter: (C)reate (R)ead (U)pdate or (D)elete or (S)chema&quot;) if operation.lower() == &#39;c&#39;: create() elif operation.lower() == &#39;r&#39;: read() elif operation.lower() == &#39;u&#39;: update() elif operation.lower() == &#39;d&#39;: delete() elif operation.lower() == &#39;s&#39;: schema() elif len(operation)==0: # Escape Key return else: print(&quot;Please enter c, r, u, or d&quot;) menu() # recursion, repeat menu try: menu() # start menu except: print(&quot;Perform Jupyter &#39;Run All&#39; prior to starting menu&quot;) . (1, &#39;Thomas Edison&#39;, &#39;toby&#39;, &#39;sha256$IQiLG4AbM5dcnNoL$e49522d138cacc1e98785d482fe66d039bf2e487c5b371538831102c306641db&#39;, &#39;1847-02-11&#39;) (2, &#39;Nikola Tesla&#39;, &#39;niko&#39;, &#39;sha256$S7wEhsAIHhuowABr$30eb9f4f186b88d6a642aa13590dfaf97a9dc039e5536bf86c21c81d5da32dda&#39;, &#39;2023-03-15&#39;) (3, &#39;Alexander Graham Bell&#39;, &#39;lex&#39;, &#39;sha256$bCwwzDSaMq7lWwID$d80eae2665c1fa612437db0c1fded6c1ed42fa66ba4183560661aeaa41619d02&#39;, &#39;2023-03-15&#39;) (4, &#39;Eli Whitney&#39;, &#39;whit&#39;, &#39;sha256$QhfkU8j8aR4WVAfd$d31b07e2a7a1f66498d5e1da4d30bd08873fd134058149bdd28404f4ef9e200a&#39;, &#39;2023-03-15&#39;) (5, &#39;Indiana Jones&#39;, &#39;indi&#39;, &#39;sha256$geO9CNSTl4xs8gg9$c30ee2df54229ad7ac640c89ecacf38d00ec3e48b64515f7fe285ce4e1c4a5c1&#39;, &#39;1920-10-21&#39;) (6, &#39;Marion Ravenwood&#39;, &#39;raven&#39;, &#39;sha256$79mFg8xP1wDlScb1$bd6a81f36bd3bca1f841385aed7dd4faefc820ace67175fa0d6b54d59b674ff1&#39;, &#39;1921-10-21&#39;) (7, &#39;alex&#39;, &#39;&#39;, &#39;sha256$is4jvpXEdcAguC7y$9ed42aa62953f3cb3a2ddf32fd594e6db19f1feea3d3e7b68a6b47be7cbcd53d&#39;, &#39;1940-01-01&#39;) (8, &#39;Alexander Lu&#39;, &#39;1337&#39;, &#39;password&#39;, &#39;2006-11-29&#39;) (9, &#39;Ethan &#39;, &#39;1338&#39;, &#39;password&#39;, &#39;2012-12-29&#39;) . Hacks . Add this Blog to you own Blogging site. In the Blog add notes and observations on each code cell. | In this implementation, do you see procedural abstraction? | In 2.4a or 2.4b lecture Do you see data abstraction? Complement this with Debugging example. | Use Imperative or OOP style to Create a new Table or do something that applies to your CPT project. | . | . Reference... sqlite documentation .",
            "url": "https://ylu-1258.github.io/YLu-Blog/2023/03/16/AP-unit2-4b.html",
            "relUrl": "/2023/03/16/AP-unit2-4b.html",
            "date": " • Mar 16, 2023"
        }
        
    
  
    
        ,"post1": {
            "title": "An Empirical Study On Performance Measures Of Collaborative Filtering Recommendation Algorithms",
            "content": "An Empirical Study on Performance Measures of Collaborative Filtering Recommendation Algorithms . Alex Lu . maodou1258@gmail.com . Abstract . In recent times, recommendation engines have become increasingly popular within many industries. The focus of such an engine is to implement an algorithm to successfully make recommendations based on user preferences. Because of the utility provided by such tools, many industries heavily rely on such engines to provide accurate product recommendations to customers. Some popular algorithms implemented include clustering, matrix factorization, and deep learning implementations. Such algorithms are utilized within Collaborative Filtering methods and are simulated using pre-made datasets containing user and item information. Performance metrics are then applied to the algorithm to test the accuracy of the model. Results show that deep learning algorithms provide greater . performance when compared to other applications. . Keywords - Collaborative filtering, RMSE, Deep Learning, MAE, matrix factorization, Performance Measure . 1. Introduction . Latterly, recommendation engines serve a great purpose in many online services and enhance the consumer experience by providing lists of recommended items to users. Many big corporations such as Netflix, Amazon, Youtube, and others provide item recommendations through such methods to promote the sales and usage of their goods. The general idea of a recommender system is to return a list of items that the user would find interesting. Implementing various different types of engines provides insight towards which algorithm is the most suitable for certain scenarios. . Different approaches by different algorithms could have varying performances depending on the type of recommendation required, and the core nature of the dataset used. This research is primarily conducted on collaborative-filtering techniques, but similar research could be conducted on content-based implementations. Examples of potential implementations could be clustering and matrix factorization approaches. The main accuracy metrics utilized in the research are the RMSE and MAE metrics which are further explained later in the paper. . Two main research questions would serve as the focus of the research. More conclusions could be drawn from the data collected, but the main aim of the research are as follows: . I) What CF implementation provides the accuracy measure for a standard data set containing user, item and rating information? II) How does data sparsity affect the prediction accuracy of the models implemented? . 2. Background . Recommendation engines are programs used to provide item recommendations to users based on filtering each item and returning a . possible predicted rating for the user. The two main implementations of recommendation engines are content-based filtering and collaborative filtering. . 2.1 Content-based Filtering . A content-based filtering engine takes into account the user’s own item history and focuses on keywords in items rather than similarity between users [1]. However, such implementation can lead to a scenario where providing a broad range of recommendations would become impossible. Because of the nature of content-based filtering engines, items that were utilized by similar users wouldn’t be recommended to the main user purely because of the lack of a keyword or phrase. A dataset for such an engine would incorporate items along with a detailed profile for each item. . Despite its narrower scope when providing recommendations, content-based filtering systems often reduce the amount of data needed to make accurate predictions. For collaborative filtering, a greater range of data is required for a proper calculation of an item. To put it simply, the more data the engine has, the more . accurate the prediction is. However, with the nature of content-based filtering, any amount of data would suffice, providing that there are items that match the recommendation requirements. . 2.2 Collaborative Filtering . A collaborative filtering engine takes into account other user’s ratings, and returns . algorithms. This paper would primarily focus on model-based algorithms and approaches. The similarity between users could be found in various methods. For this study, a total of twelve algorithms were implemented. The main focus was placed on nearest neighbors, matrix factorization, and deep learning algorithms. Figure 2.1 maps out the various . . recommendations based on similarity. Collaborative filtering is split into two main approaches, model-based and memory based . algorithms implemented in this research. 2.3 Nearest Neighbor . Nearest neighbor, or more specifically, k nearest neighbors is a collaborative filtering . algorithm used to find similarities between items and users based on the total distance between two neighbors [2]. A weight system could also be applied so that a neighbor closer to the main user would have more weight on the recommendations than one who is further away. . Four of the twelve algorithms used in the research were from this category. KNNBasic, KNNWithZScore, KNNWithMeans, and KNNBaseline were such implementations of this category of algorithms. . 2.4 Matrix Factorization . Matrix Factorization is yet again another implementation of collaborative filtering. Simply put, a matrix factorization relates two separate values together under a specific value to create a grid or matrix of the data [3]. However, as data is not evenly distributed, some cells in the matrix are left empty and would need to be filled in to provide recommendations. Such values are known as “latent features”. . Some matrix factorizations implemented in the research were Negative Matrix Factorization, Singular Value Decomposition, and SVD++. . 2.5 Deep Learning . A deep learning model utilizes a neural network to process and calculate information. Much like a human brain, deep learning attempts to make predictions based on data much like a human brain. . The deep learning model implemented in this experiment utilizes an embedding layer structure [4]. Using such a structure could organize data into a vector of discrete values which could then produce similarity results and tests through the distance between vectors. These types of embedding layers could be generated through frameworks such as “Pytorch” or “Tensorflow”, however, this research would be implemented in Pytorch. The model could then be trained through a series of epochs and eventually provide predictions. . 2.6 Miscellaneous surprise algorithms . The miscellaneous category comprises the leftover algorithms in the surprise package that do not fit into any of the other categories in the research. Such algorithms were as listed: Co . Clustering, Normal Predictor, Baseline-Only, and Slope-One. . According to Sahar Karat Co-Clustering is a collaborative filtering algorithm used to provide recommendations by “A simultaneous clustering of the rows and columns of a matrix” [5]. Classical clustering algorithms only focus on one specific type of data, while co . clustering could be used to accommodate two simultaneously. . Normal predictor is an algorithm provided by the surprise package that provides random user recommendations based on the data distribution in the dataset. The “Maximum Likelihood Estimation” method is utilized in this calculation. . Baseline-Only is yet another surprise algorithm that makes baseline predictions based on the data provided. There are two main ways to implement this algorithm. The first implementation method is “Stochastic Gradient . Descent” (SGD), which calculates a gradient of the dataset by a random selection of data. The other implementation is to use “Alternating Least Squares” (ALS) which is a matrix factorization algorithm that works well with sparser sets of data. . Slope-One is an example of an item-based collaborative filtering recommendation algorithm. Predictions made with this model are generally based on personal ratings as well as similar community ratings. . Further information and implementations on the methods listed could be found on the surprise documentations [6]. . 2.7 Accuracy Metrics . Two accuracy metrics were used in the process of this research. Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE) [11]. . RMSE is a quadratic model that assigns much larger weights to larger errors observed. Accuracy is calculated through RMSE by taking the square root of the mean of the sum of the squared difference of each predicted . value and its corresponding actual value. The formula is displayed in Figure 2.2 . . (Figure 2.2) . MAE is a linear metric model that linearly scales error. This model is typically used when higher errors are not important to the observation. Accuracy is calculated through MAE by taking the mean of the sum of the absolute values of the differences between the predicted and actual values. The formula is displayed in Figure 2.3 . . (Figure 2.3) . 3. Related Work . Various research topics have already been analyzed in the field of collaborative filtering. A few similar pieces of research provide similar methods and implementations. A plethora of CF models has been implemented and researched. . 3.1 Similar Research . Similar research was conducted by Mojdeh Saadati et al. [7] on the implementations of various models on movie recommender systems. The two models used in the experiment were the matrix factorization model SVD and a deep learning implementation of the Restricted Boltzmann Machine. The performance of the implementations was measured in the RMSE metric. However, in this research, we go over a broader spectrum of algorithms from each group of implementations and produce results for a larger picture between the different implementations. . 3.2 Further Research . He et al. [8] Conducted Research on how Matrix Factorization could be implemented with Deep Learning to create a better performing model for collaborative filtering. The proposed idea was to use two main models, the Generalized Matrix Factorization (GMF), and the Multi-Layer Perceptron (MLP) to create a hybrid between deep learning and matrix factorization implementations to create a new neural matrix factorization model . dubbed “NeuMF”. NeuMf was then compared to other well-known models such as KNN and ALS implementations and proved to have better accuracy and less training loss. . Yedder et al. [9] Researched the performance of the restricted Boltzmann machine. Various hidden units, learning rates, and other factors were incorporated into the research. Problems such as data sparsity were also encountered in the research and required other methods of implementation. This research also utilized the RMSE accuracy metric to rate the performance of the model. The research indicated an excellent RMSE measure of 0.46. . 4. Approach . This study was broken up into three parts, cleaning, implementation, and evaluation. We will first check for invalid values throughout the dataset, then begin to implement algorithms, and finally calculate performance values. . 4.1 Dataset . The data that would be primarily used in this study is the Kaggle dataset “Anime . Recommendations Database” [10] based on the data collected from myanimelist.net and is included in the reference section of the study. The two files contained within the database are the “ratings.csv” and “anime.csv”. The research would mostly work with the ratings file, but the anime dataset could be further implemented in future research incorporating variables such as show genre and overall ratings. The anime dataset is separated into three separate columns. The user_id, anime_id, and ratings columns are provided for analyzing similarities between users. The format and example of the file are illustrated in Figure 4.1. . (Figure 4.1) . The python module “pandas” was used to import the data into a data frame which could . then be cleaned and analyzed. Making a copy of the dataset to work on is recommended. Maintaining two different data frames would help compare the results on the original to the results in the cleaned dataset. . The first thing to take note of while cleaning is to remove all rows with a rating of “-1”. Such rows represent user data that does not have a specified rating for the specific show that they watched. This step is crucial as having such data in the project would result in a sparse dataset, which directly decreases performance as shown later in the results. A total of 7.8 million rows of data were in the dataset, after cleaning, about 6.3 million rows remained. The distribution of this data is shown in figure 4.2. . . (Figure 4.2) . Another issue arose again while trying to apply the dataset to our engine. Certain cells in . our dataset possibly had ‘NaN’ values, which would in turn cause an error in actual predictions where a value of ‘NaN’ would be returned instead of an integer of the predicted rating. To resolve this issue, we first applied the ‘to_numeric’ method from the pandas module to convert each value into workable integer or float values. After, we could then finally lower the runtime of our experiment by decreasing the size of our dataset from 6 million down to 125,000. This number was picked for easier splitting while implementing other algorithms and deep learning which require testing and training datasets. From there, we then calculated the distribution of the ratings by counting the number of each rating in our dataset. After clearing, it’s paramount to re index the rows in the data frame. Removing the invalid A distribution of the ratings is demonstrated in Figure 4.3. . . (Figure 4.3) . A correlation heatmap could also be used to detect any possible correlations between the values given, however, as of now, there is no apparent relation. . . (Figure 4.4) . 4.2 Implementation . The python package scikit-surprise contains most of the algorithms implemented in this research. First and foremost, we must declare a rating scale for our dataset by utilizing the reader class, and also initializing our dataset with the Dataset class. We set a new . “ratings“ object over the range of one to ten, which is represented in our data. Using the three columns from our pandas data frame, we could then load the data into surprise using the Dataset.load_from_df() method. . After the data is loaded, we could then specify a “param_list” dictionary. In the dictionary, the name of the specific parameters would be stored as the keys and the possible values as a list in the values. Further explanation of what specific params do for each algorithm is also provided in the surprise documentation. . The param_list dictionary could then be imputed into the GridSearchCV() method along with the algorithm name, performance measures, and a cross-validation iterator of 3. There are other arguments as well, but for the sake of our research, these four should be enough. The final step is to then fit our premade data with our GridSearchCV method with the .fit() function. Once everything is set up, the accuracy metrics could then just be retrieved via the best_score iterator. . The deep learning algorithm specifically was generated using an embedding layer with . dropout layers. The entire network consists of 4 total layers. After setting up the net, we can begin to train our model and loop over varying amounts of epochs to find the accuracy that is desired. . To determine the RMSE and the MAE values of this approach, we could separate our dataset into predictions and truth arrays. We can then apply the formulas for both RMSE and MAE as shown in Figures 2.2 and 2.3. We can implement these metrics in two ways. The first method is to use a NumPy array to subject each vector of values from each other and then to apply the formula to our newly generated values. Our second approach is more basic and rudimentary. We could subtract each truth value from each prediction value in our function by declaring a function and then apply the formula to our sum. . 5. Results . In this research, two sets of results were collected. The data collected on the cleaned dataset would serve to be our solution to the first question. However, the data collected on the original dataset would be utilized to answer . the second question concerning the performance measures on sparse datasets. 5.1 Cleaned Dataset . We have gathered both RMSE and MAE values for each algorithm used on our cleaned dataset from our implementations. In the first part of this experiment, the data set has already been cleaned of any invalid values and has reduced sparsity. The majority of the algorithms implemented all showed similar results except a couple of outliers and certain points. A table of the data collected is shown in Figure 5.1 . . (Figure 5.1) . Using the RMSE as the x-axis and MAE for the y-axis, we can plot a scatter plot of our data (Figure 5.2). . . (Figure 5.2) . Removing the Normal Predictor outlier value provides us with a clearer image of the differences in the lower valued points (Figure 5.3). . . (Figure 5.3) . Our data shows that the worst-performing algorithm that we had implemented was the Normal Predictor algorithm from the surprise package, while the best-performing implementation was the Deep Learning algorithm based on neural networks. . 5.1 Pre-cleaned Dataset . Loading up a new dataset, we can effectively run all our algorithms again while maintaining the sparsity of the original dataset. The only cleaning that had to be done was to remove all NaN values and also convert all data to numeric values. The results of running the . implementations on the sparser dataset are recorded in the figure below (Figure 4.5) . . (Figure 5.4) . The results of the various algorithms on this sparse dataset were also plotted on a scatter plot as shown below in Figure 5.5. . . (Figure 5.5) . Once again, removing the normal predictor outlier gives a clearer representation of our other data points (Figure 5.6). . . (Figure 5.6) . Although the values have increased by a considerable amount in the sparse dataset, the common trend remains between the various data points, and no changes are observed in the best and worst algorithms. Comparing the . results of the cleaned and original data sets, a clear difference could be observed in the RMSE and MAE measures (Figures 5.7-8) . (Figure 5.7) . (Figure 5.8) . 6. Discussion . After analyzing the data collected from the research, the deep learning algorithm and the KNNBaseline implementations were observed to be the best performing with the least error observed with both RMSE and MAE metrics. . A possible reason for the results could be the usage of randomness in the calculation of the Normal Predictor algorithm. The deep learning algorithm may have performed the best because of the various training cycles allocated to it which helped to create a more accurate model after each iteration. Vice versa, the opposite could also be applied to the two worst performing algorithms KNNBasic and Normal Predictor. Such implementations . had basic calculations and weren’t able to take into account outliers and other potential biases in the data. . There was an attempt in the research to implement a Restricted Boltzmann Machine (RBM) model, however, the implementation gave varying results and was difficult to judge the extra ratings of the implementation. This . research could be further pursued in the future with the addition of more deep learning implementations and a narrower focus on the subject of deep learning as a whole. From the results acquired, deep learning has been shown to have improved results compared to other algorithms. Focused research on deep learning implementations would provide the reasoning behind deep learning accuracy. . 7. Conclusion . In this research, we explored various machine learning algorithms, K-Nearest neighbors, Matrix Factorization, Deep learning, etc. Several approaches were implemented from the categories mentioned then tested for accuracy measures. . In both the cleaned and original datasets, the deep learning implementation was shown to the least margin of error when making recommendations. From this, it could be deduced that deep learning is a viable method for collaborative filtering engines working with user, item and rating data. Although different sets of data have varying optimal algorithms, . deep learning was still shown to be extremely accurate compared to other tested algorithms. Analyzing the data collected from the sparse dataset, we can conclude that a sparser set of data would result in less accurate recommendations, sometimes up to double the margin of error observed. Because of this observation, it can be concluded that collaborative filtering best performs with dense datasets. . References . [1] Kirzhner, Elena. “Machine Learning. Explanation of Collaborative . Filtering vs Content Based . Filtering.” Medium, Codeburst, 11 . May 2018, codeburst.io/explanation of-recommender-systems-in . information-retrieval-13077e1d916c. [2] Harrison, Onel. “Machine Learning Basics with the K-Nearest Neighbors Algorithm.” Medium, Towards Data Science, 14 July 2019, . towardsdatascience.com/machine . learning-basics-with-the-k-nearest . neighbors-algorithm-6a6e71d01761. . [3] Chen, Denise. “Recommendation System - Matrix Factorization.” . Medium, Towards Data Science, 9 . July 2020, . towardsdatascience.com/recommend ation-system-matrix-factorization . d61978660b4b. . [4] Sivanantham, Balavivek. . “Recommendation System Implementation With Deep Learning and PyTorch.” Medium, The Startup, 18 Aug. 2020, . medium.com/swlh/recommendation system-implementation-with-deep learning-and-pytorch-a03ee84a96f4. . [5] Karat, Sahar. “Co . Clustering.” Data Science . Made Simpler, 5 Mar. 2016, datasciencemadesimpler.wor dpress.com/tag/co . clustering/. . [6] Hug, Nicholas. “Welcome to Surprise’ Documentation.” Welcome to Surprise’ . Documentation! - Surprise 1 Documentation, 2015, . surprise.readthedocs.io/en/sta ble/. . [7] Mojdeh Saadati, Syed Shihab, Mohammed Shaiqur Rahman “Movie Recommender Systems: Implementation and Performance . Evaluation.” Semantic Scholar, 2019, . www.semanticscholar.org/paper/Mo vie-Recommender-Systems%3A Implementation-and-Saadati Shihab/01470f39285213e53f365ce0 1417b18d12467563#citing-papers. . [8] Xiangnan, He, et al. “Neural Collaborative Filtering.” . International World Wide Web Conference Committee, 3 Apr. 2017. . [9] Yedder, Hanene Ben, et al. “Modeling Prediction in . Recommender Systems Using Restricted Boltzmann Machine.” . IEEE Explore, IEEE, 5 Oct. 2017, ieeexplore.ieee.org/abstract/documen t/8122923. . [10] CooperUnion. “Anime . Recommendations Database.” . Kaggle, Kaggle, 21 Dec. 2016, . www.kaggle.com/CooperUnion/ani me-recommendations-database. . [11] Kampakis, Stylianos. “Performance Measures: RMSE and MAE.” The Data Scientist, The Data Scientist, 26 Nov. 2020, . thedatascientist.com/performance measures-rmse-mae/. .",
            "url": "https://ylu-1258.github.io/YLu-Blog/2023/03/15/An-Empirical-Study-on-Performance-Measures-of-Collaborative-Filtering-Recommendation-Algorithms.html",
            "relUrl": "/2023/03/15/An-Empirical-Study-on-Performance-Measures-of-Collaborative-Filtering-Recommendation-Algorithms.html",
            "date": " • Mar 15, 2023"
        }
        
    
  
    
        ,"post2": {
            "title": "Alex Lu   Lab #1_ Measurements And Graphical Analysis",
            "content": "Lab #1: Measurements and Graphical Analysis . Alex Lu . Purpose: . Given disks of different radii, determine the relationship between the mass and the radius of the disks through graphical method and calculate the uncertainty associated with the measured value. We will learn about linearization and use it to create a mathematical model. . Materials and Equipment: . Balance . Meter stick . Circular disks (identical thickness and uniform density but different radii) . Graphic Calculator or online graphing tool . Procedure: . Substitute equations to get a relationship between mass and radius. . | Use the relationship between mass and radius to determine what variable should be processed to linearize the data . | Measure the radius (in cm) of the cylinder with the ruler with one end of the ruler at the center of the disk and the other on the edge. . | Zero out the balance and then measure the mass (in grams) of the cylinder. . | Stack the metal cylinders and measure the collective height of all cylinders. Divide by the total number of cylinders multiplied by 2 to the power of the number of folds to find the height of each individual cylinder. . | Organize the collected data into a data table . | Plot the original radius and mass . | Plot the processed radius and mass . | Plug height and other constants to create a relationship between mass and radius. . | Equations and calculations: . Let: . ( rho) be density . | (m) be mass . | (v) be volume . | (a) be the surface area of the cylinder . | (r) be the radii of the cylinder . | (h) be the height of the cylinder . | . Thus: . If ( rho = frac{m}{v}) and (v = ah) then . (m = rho v) . (m = rho text{ah}) . If (a = pi r^{2}), then . (m = rho pi r^{2}h) . Since both ( rho) and ( pi) are constants and (h) is negligible, ∴ (m propto r^{2}) . Since mass is directly proportional to radius squared, the graph could be linearized if (r^{2}) is plotted instead. . Precision and Uncertainty . The balanced used to measure the mass of the metal disks was accurate up to a hundredth of a gram, while the ruler used to measure the radius and height of the disks was accurate up to 1 millimeter (0.1 cm). As such, the radius was measured to the nearest hundredth of a centimeter or a tenth of a millimeter. . | Data Tables . m = mass . r = radii . H = height . Mass in Grams, Radius and height in cm, and Radius Squared of Each Metal Disk . Disk m (g) h (cm) r (cm)   r^2 (cm^2) . 1 | 0.07 | 0.0025 | 2.30 |   | 5.29 | . 2 | 0.14 | 0.0025 | 3.00 |   | 9.0 | . 3 | 0.25 | 0.0025 | 4.30 |   | 18.49 | . 4 | 0.37 | 0.0025 | 4.90 |   | 24.01 | . 5 | 0.73 | 0.0025 | 7.10 |   | 50.41 | . Graph of Non-Linearized Data: . . Equation: (y = 0.0133921x^{2} + 0.112825x - 0.0238674) . R2 : (0.9967) . This graph has non-linear data as it’s represented by a quadratic model. . Graph of Linearized Data: . . Line of the best fit equation: (y = 0.0145569x - 0.000100527) . r = (0.9983) . r2 = (0.9966 ) . This graph has linear data as it’s represented by a linear model. . Analysis Questions: . 1) What is the independent variable in your y = mx +b formula? . Considering the mathematic relationship between disk radius r and disk mass m, the independent variable from my line of best fit represents the radius of the disk squared. . 2) What does the slope represent in your y = mx +b formula? Show dimensionally that indeed that is what your slope represents and that the formula is valid dimensionally. . Using the (m = rho v) equation and the volume relationship, we can express the disk’s mass in terms of the radius and the height in this relationship: (m = rho pi r^{2}h). Since density (( rho)) and ( pi) are constants, and height ((h)) is negligible, we can essentially group these 3 constants into one value serving as the coefficient to the radius squared term. Thus, the slope is the product of the density, height, and ( pi). Since height has its units expressed in cm, density has units expressed in g/cm³, and ( pi) is a constant wth no units, the slope is a combination of all 3 constants and has units of g/cm2. . Because our slope has units of g/cm2, and x represents the radius squared expressed by cm2, and since our desired output ((y)) is mass in grams (g), the y-intercept ((b)) must have a unit of grams (g) in order for both sides of the equation to have the same units (grams). . (g = frac{g}{cm^{2}} cdot cm^{2} + g) . (g = g + g) . (g = g) . 3) Should the “b” in your y = mx + b formula be zero? Explain your answer. . The y-intercept ((b)) in my (y = mx + b) formula shouldn’t be zero unless the data naturally generates a line of best fit that passes through the origin. Forcing the y-intercept to pass through the origin would either alter the slope ((m)) or remove the y-intercept completely. This would result in an inaccurate interpretation of the model as the line is no longer set evenly between the data points. Thus we cannot guarantee the model’s integrity for future data points, rendering our model useless. . 4) Measure/estimate the “thickness” of your cylinders. Use that value to find the experimental density of your cylinders. Find a percent difference between your found density and the actual density. The actual material is aluminum. . (y = 0.0145569x - 0.000100527) . (slope = pi rho h) . ( rho = density) . ( rho = frac{ text{slope}}{h pi} = frac{0.0145569 frac{g}{cm^{2}}}{ pi(0.0025cm)} = 1.85 frac{g}{cm^{3}}) . The density of the metal disks according to my linear model is (1.85 frac{g}{cm^{3}}) , Since the metal used in the disk is Aluminum (density = (2.70 frac{g}{cm^{3}})) we can use the following calculations to determine our percent error . ( % Error = left | frac{Actual - Expected}{ text{Expected}} right | *100 % ) | . ( % Error = left | frac{1.85 frac{g}{cm^{3}} - 2.70 frac{g}{cm^{3}}}{2.70 frac{g}{cm^{3}}} right | *100 % = 31.5 % ) | . ∴ Percent difference = 31.5% . 5) Errors. Make sure you explain why your number is bigger or smaller than (if positive or negative difference.) . The number that I obtained is smaller than the actual value (31.5% error) because the instruments that we used to measure the disks’ dimensions had inaccuracies. The ruler that we used to measure the disks’ radius is only precise up to millimeters, and the balanced used to measure mass is only accurate up to the hundredth’s place, while actual instruments used to measure the actual value are much more precise. Additionally, the method I used to measure the height of the disks also had inherent inaccuracies, as folding the aluminum disks over each other may create tiny air pockets that add additional magnitude to my measured height, resulting in an inaccurate height measurement. The uncertainty of the ruler and also the inaccurate way of measuring height may have contributed to a greater height value than the actual value, resulting in a lower density value when I divided. Lastly, the disks weren’t in a perfectly circular shape, thus the direction that I measured the disk in might also have affected the end result, as measuring in different directions would produce different radii. . Synthesis Questions: . 1) In this experiment, if we had used disks with a greater thickness, would the slope of your best fit line have been different? Would your experimental value for density be the same? Explain. . Since slope represents the product of thickness, ( pi), and density, having a greater measured thickness would result in a much larger slope value. However, our experimental value of density would still be around the same, as an increase in thickness would also result in a proportionally large increase in mass, which will offset the thickness increase. . 2. How would your graph of m versus r2 be different if you had used disks of the same . thickness but made out of steel? Draw a second line on your m versus r2 plot that . represents disks made of steel. . The graph would be of m versus r2 would be different in that the slope of the graph would be much greater, this is because steel has a much higher density of (7.85 frac{g}{cm^{3}}) compared to aluminum’s (2.7 frac{g}{cm^{3}}) . . (Note: The blue line is a rough sketch of what the line of best fit would look like on the same scale as the aluminum graph is steel was used instead. This is not an accurate representation, just an approximation) . 3. Another group of students has acquired data for the exact same experiment; however, their disks are made of an unknown material that they are trying to determine. The group’s m versus r2 data produced a line of best fit with slope equal to 122 kg/m2. Each disk they measured had the same 0.5 cm thickness. Calculate the density of the unknown material and use the table below to help determine what material their disks are made of. . Work: . (Slope = 122kg/m^{2} , h = 0.5cm, slope = rho pi h) . (slope = 122 frac{ text{kg}}{m^{2}}( frac{1000g}{1kg})( frac{1m}{100cm})^{2} = 12.2 frac{g}{cm^{2}} = rho pi h) . ( rho = frac{ text{slope}}{ pi h} = frac{12.2 frac{g}{cm^{2}}}{ pi*0.5cm} = 7.77 frac{g}{cm^{3}}) . The closet material to a density of (7.77 frac{g}{cm^{3}}) is iron, which has a density of (7.8 frac{g}{cm^{3}}). The disks are most likely made out of iron. . Multiple Choice Questions: . 1. You perform the same experiment, but this time you plot a linear relationship between mass and the circumference of the disks rather than the radius. What is the slope of the linear plot? . Work: . (slope = rho pi h) . (m = rho v = rho pi r^{2}h) . (v = pi r^{2}h) . Let (c) be circumference . (c = 2 pi r) . (c^{2} = 4 pi^{2}r^{2}) . (r^{2} = frac{c^{2}}{4 pi^{2}}) . (m = rho pi h( frac{c^{2}}{4 pi^{2}}) = frac{ rho hc^{2}}{4 pi} = ( frac{ rho h}{4 pi})c^{2}) . () . Since ( rho) (density), h (height), and (4 pi) are all constants, these 3 values could be combined to form the slope for the linear plot, which is (( frac{ text{ph}}{4 pi})), Hence, E is the correct answer choice. . 2. Skipped . 3. Consider an experiment in which a student measures the mass and diameter of 10 . different-sized spheres, all made of the same material of uniform density ρ. For this . student to create a linear graph relating the mass of the sphere to its radius r, the . student would need to plot mass m versus which quantity: . ( text{let v be the volume of a sphere}) . (v = frac{4}{3} pi r^{3}) . (m = pv = ( frac{4p pi}{3})r^{3}) . Since (( frac{4p pi}{3})) is a constant, the student must plot a linear graph relating m versus the quality of radius cubed. Hence, option C is correct. .",
            "url": "https://ylu-1258.github.io/YLu-Blog/2023/03/15/Alex-Lu-Lab-1_-Measurements-and-Graphical-Analysis.html",
            "relUrl": "/2023/03/15/Alex-Lu-Lab-1_-Measurements-and-Graphical-Analysis.html",
            "date": " • Mar 15, 2023"
        }
        
    
  
    
        ,"post3": {
            "title": "Individual MCQ Review",
            "content": "Overall Scores: . . . . Reflection on the 3 Quizzes: . Overall, I feel like the classes that we had prepared me pretty well for these 3 quizzes. I was able to answer all quizzes with a high degree of accuracy, with the exception of the “Extracting Information From Data” Quiz, where I answerd one of the questions inccorectly. The classes as a whole were really helpful in reinforcing the concepts, as we had hands-on experisnce with image compressoin with PIL and data extraction with pandas. . Analysis of Hard/Missed Questions: . . On this problem, I answered the question incorrectly by assuming that the overall quantity of the items boughtat a concert would correlate to the audience’s retention rate. However, this is incorrect as this piece of data can fluctuate on more factors than just the amount of participants at a concert. The correct answer should’ve been A, where we could’ve used the average ticket price in conjunction with the total amount of dollars spent at the event to determine an approximate of how many people actually attended the concert. This problem was a huge oversight on my part because I failed to consider other alternative methods to calculate the number of attendees, including ones with mathematical operations .",
            "url": "https://ylu-1258.github.io/YLu-Blog/markdown/apcsp/tri3/2023/03/13/MCQ-Review.html",
            "relUrl": "/markdown/apcsp/tri3/2023/03/13/MCQ-Review.html",
            "date": " • Mar 13, 2023"
        }
        
    
  
    
        ,"post4": {
            "title": "Unit 2.4a Using Programs with Data, SQLAlchemy",
            "content": "Database and SQLAlchemy . In this blog we will explore using programs with data, focused on Databases. We will use SQLite Database to learn more about using Programs with Data. Use Debugging through these examples to examine Objects created in Code. . College Board talks about ideas like . Program Usage. &quot;iterative and interactive way when processing information&quot; | Managing Data. &quot;classifying data are part of the process in using programs&quot;, &quot;data files in a Table&quot; | Insight &quot;insight and knowledge can be obtained from ... digitally represented information&quot; | Filter systems. &#39;tools for finding information and recognizing patterns&quot; | Application. &quot;the preserve has two databases&quot;, &quot;an employee wants to count the number of book&quot; | . | PBL, Databases, Iterative/OOP . Iterative. Refers to a sequence of instructions or code being repeated until a specific end result is achieved | OOP. A computer programming model that organizes software design around data, or objects, rather than functions and logic | SQL. Structured Query Language, abbreviated as SQL, is a language used in programming, managing, and structuring data | . | . Imports and Flask Objects . Defines and key object creations . Comment on where you have observed these working? Provide a defintion of purpose. Flask app object I saw the Flask app object in our CPT project, where it managed many if not all operations that we wanted to perform with flask | | SQLAlchemy db object The db object was also created in our CPT project, where it was used to establish a connection and perform database operatoins and manipulations on our data | | | . &quot;&quot;&quot; These imports define the key objects &quot;&quot;&quot; from flask import Flask from flask_sqlalchemy import SQLAlchemy &quot;&quot;&quot; These object and definitions are used throughout the Jupyter Notebook. &quot;&quot;&quot; # Setup of key Flask object (app) app = Flask(__name__) # Setup SQLAlchemy object and properties for the database (db) database = &#39;sqlite:///sqlite.db&#39; # path and filename of database app.config[&#39;SQLALCHEMY_TRACK_MODIFICATIONS&#39;] = False app.config[&#39;SQLALCHEMY_DATABASE_URI&#39;] = database app.config[&#39;SECRET_KEY&#39;] = &#39;SECRET_KEY&#39; db = SQLAlchemy() # This belongs in place where it runs once per project db.init_app(app) . Model Definition . Define columns, initialization, and CRUD methods for users table in sqlite.db . Comment on these items in the class, purpose and defintion. . class User The creates an abstractiion for our User, and contains all the attributes of the user (info such as uid and name) to potential methods such as create, add delete and update. It serves the purpose of containing all of our necessary operations and information under one name, which we can use to easily edit users and update their entry later on in our program. | . | db.Model inheritance This inherits all the methods and attributes from the db.Model class. Doing this enables us to use datatypes for our DB schema along with the other operations for CRUD | . | init method This is our class constructor, it serves to instantiate the attributes of our user upon the creation of our object, and then allows us to use this data later on in our operations | . | @property, @&lt;column&gt;.setter These decorators are used to establish the getters and setters within a class or object, it serves to create references for us to use to reference clas methods and attributes outside of the class | . | create, read, update, delete methods . | . | Methods are used to help to solve problems with a class . | . &quot;&quot;&quot; database dependencies to support sqlite examples &quot;&quot;&quot; import datetime from datetime import datetime import json from sqlalchemy.exc import IntegrityError from werkzeug.security import generate_password_hash, check_password_hash &#39;&#39;&#39; Tutorial: https://www.sqlalchemy.org/library.html#tutorials, try to get into a Python shell and follow along &#39;&#39;&#39; # Define the User class to manage actions in the &#39;users&#39; table # -- Object Relational Mapping (ORM) is the key concept of SQLAlchemy # -- a.) db.Model is like an inner layer of the onion in ORM # -- b.) User represents data we want to store, something that is built on db.Model # -- c.) SQLAlchemy ORM is layer on top of SQLAlchemy Core, then SQLAlchemy engine, SQL class User(db.Model): # Inherits from db.Model, allows for our User to not only be used as an Object to store information, but also give it the ability to perform database operations. __tablename__ = &#39;users&#39; # table name is plural, class name is singular # Define the User schema with &quot;vars&quot; from object id = db.Column(db.Integer, primary_key=True) _name = db.Column(db.String(255), unique=False, nullable=False) _uid = db.Column(db.String(255), unique=True, nullable=False) _password = db.Column(db.String(255), unique=False, nullable=False) _dob = db.Column(db.Date) # constructor of a User object, initializes the instance variables within object (self) def __init__(self, name, uid, password=&quot;123qwerty&quot;, dob=datetime.today()): # This is the class constructor, it initializes our values within the class. In python __init__() is used to instantiate an object self._name = name # variables with self prefix become part of the object, self._uid = uid self.set_password(password) if isinstance(dob, str): # not a date type dob = date=datetime.today() self._dob = dob # a name getter method, extracts name from object @property def name(self): return self._name # a setter function, allows name to be updated after initial object creation @name.setter def name(self, name): self._name = name # a getter method, extracts uid from object @property def uid(self): return self._uid # a setter function, allows uid to be updated after initial object creation @uid.setter def uid(self, uid): self._uid = uid # check if uid parameter matches user id in object, return boolean def is_uid(self, uid): return self._uid == uid @property def password(self): return self._password[0:10] + &quot;...&quot; # because of security only show 1st characters # update password, this is conventional method used for setter def set_password(self, password): &quot;&quot;&quot;Create a hashed password.&quot;&quot;&quot; self._password = generate_password_hash(password, method=&#39;sha256&#39;) # check password parameter against stored/encrypted password def is_password(self, password): &quot;&quot;&quot;Check against hashed password.&quot;&quot;&quot; result = check_password_hash(self._password, password) return result # dob property is returned as string, a string represents date outside object @property def dob(self): dob_string = self._dob.strftime(&#39;%m-%d-%Y&#39;) return dob_string # dob setter, verifies date type before it is set or default to today @dob.setter def dob(self, dob): if isinstance(dob, str): # not a date type dob = date=datetime.today() self._dob = dob # age is calculated field, age is returned according to date of birth @property def age(self): today = datetime.today() return today.year - self._dob.year - ((today.month, today.day) &lt; (self._dob.month, self._dob.day)) # output content using str(object) is in human readable form # output content using json dumps, this is ready for API response def __str__(self): return json.dumps(self.read()) # CRUD create/add a new record to the table # returns self or None on error def create(self): try: # creates a person object from User(db.Model) class, passes initializers db.session.add(self) # add prepares to persist person object to Users table db.session.commit() # SqlAlchemy &quot;unit of work pattern&quot; requires a manual commit return self except IntegrityError: db.session.remove() return None # CRUD read converts self to dictionary # returns dictionary def read(self): return { &quot;id&quot;: self.id, &quot;name&quot;: self.name, &quot;uid&quot;: self.uid, &quot;dob&quot;: self.dob, &quot;age&quot;: self.age, } # CRUD update: updates user name, password, phone # returns self def update(self, name=&quot;&quot;, uid=&quot;&quot;, password=&quot;&quot;): &quot;&quot;&quot;only updates values with length&quot;&quot;&quot; if len(name) &gt; 0: self.name = name if len(uid) &gt; 0: self.uid = uid if len(password) &gt; 0: self.set_password(password) db.session.commit() return self # CRUD delete: remove self # None def delete(self): db.session.delete(self) db.session.commit() return None . Initial Data . Uses SQLALchemy db.create_all() to initialize rows into sqlite.db . Comment on how these work? Create All Tables from db Object | User Object Constructors | Try / Except | | . &quot;&quot;&quot;Database Creation and Testing &quot;&quot;&quot; # Builds working data for testing def initUsers(): with app.app_context(): &quot;&quot;&quot;Create database and tables&quot;&quot;&quot; db.create_all() &quot;&quot;&quot;Tester data for table&quot;&quot;&quot; u1 = User(name=&#39;Thomas Edison&#39;, uid=&#39;toby&#39;, password=&#39;123toby&#39;, dob=datetime(1847, 2, 11)) u2 = User(name=&#39;Nikola Tesla&#39;, uid=&#39;niko&#39;, password=&#39;123niko&#39;) u3 = User(name=&#39;Alexander Graham Bell&#39;, uid=&#39;lex&#39;, password=&#39;123lex&#39;) u4 = User(name=&#39;Eli Whitney&#39;, uid=&#39;whit&#39;, password=&#39;123whit&#39;) u5 = User(name=&#39;Indiana Jones&#39;, uid=&#39;indi&#39;, dob=datetime(1920, 10, 21)) u6 = User(name=&#39;Marion Ravenwood&#39;, uid=&#39;raven&#39;, dob=datetime(1921, 10, 21)) users = [u1, u2, u3, u4, u5, u6] &quot;&quot;&quot;Builds sample user/note(s) data&quot;&quot;&quot; for user in users: try: &#39;&#39;&#39;add user to table&#39;&#39;&#39; object = user.create() print(f&quot;Created new uid {object.uid}&quot;) except: # error raised if object nit created &#39;&#39;&#39;fails with bad or duplicate data&#39;&#39;&#39; print(f&quot;Records exist uid {user.uid}, or error.&quot;) initUsers() . Created new uid toby Created new uid niko Created new uid lex Created new uid whit Created new uid indi Created new uid raven . Check for given Credentials in users table in sqlite.db . Use of ORM Query object and custom methods to identify user to credentials uid and password . Comment on purpose of following . User.query.filter_by | user.password | | ORM (Object Relational Mapping) . | . def find_by_uid(uid): with app.app_context(): user = User.query.filter_by(_uid=uid).first() return user # returns user object # Check credentials by finding user and verify password def check_credentials(uid, password): # query email and return user record user = find_by_uid(uid) if user == None: return False if (user.is_password(password)): return True return False #check_credentials(&quot;indi&quot;, &quot;123qwerty&quot;) . Create a new User in table in Sqlite.db . Uses SQLALchemy and custom user.create() method to add row. . Comment on purpose of following user.find_by_uid() and try/except | user = User(...) | user.dob and try/except | user.create() and try/except | | . def create(): # optimize user time to see if uid exists uid = input(&quot;Enter your user id:&quot;) user = find_by_uid(uid) try: print(&quot;Found n&quot;, user.read()) return except: pass # keep going # request value that ensure creating valid object name = input(&quot;Enter your name:&quot;) password = input(&quot;Enter your password&quot;) # Initialize User object before date user = User(name=name, uid=uid, password=password ) # create user.dob, fail with today as dob dob = input(&quot;Enter your date of birth &#39;YYYY-MM-DD&#39;&quot;) try: user.dob = datetime.strptime(dob, &#39;%Y-%m-%d&#39;).date() except ValueError: user.dob = datetime.today() print(f&quot;Invalid date {dob} require YYYY-mm-dd, date defaulted to {user.dob}&quot;) # write object to database with app.app_context(): try: object = user.create() print(&quot;Created n&quot;, object.read()) except: # error raised if object not created print(&quot;Unknown error uid {uid}&quot;) create() . Created {&#39;id&#39;: 7, &#39;name&#39;: &#39;alex&#39;, &#39;uid&#39;: &#39;&#39;, &#39;dob&#39;: &#39;01-01-1940&#39;, &#39;age&#39;: 83} . Reading users table in sqlite.db . Uses SQLALchemy query.all method to read data . Comment on purpose of following User.query.all | json_ready assignment, google List Comprehension | | . # SQLAlchemy extracts all users from database, turns each user into JSON def read(): with app.app_context(): table = User.query.all() json_ready = [user.read() for user in table] # &quot;List Comprehensions&quot;, for each user add user.read() to list return json_ready read() . [{&#39;id&#39;: 1, &#39;name&#39;: &#39;Thomas Edison&#39;, &#39;uid&#39;: &#39;toby&#39;, &#39;dob&#39;: &#39;02-11-1847&#39;, &#39;age&#39;: 176}, {&#39;id&#39;: 2, &#39;name&#39;: &#39;Nikola Tesla&#39;, &#39;uid&#39;: &#39;niko&#39;, &#39;dob&#39;: &#39;03-15-2023&#39;, &#39;age&#39;: 0}, {&#39;id&#39;: 3, &#39;name&#39;: &#39;Alexander Graham Bell&#39;, &#39;uid&#39;: &#39;lex&#39;, &#39;dob&#39;: &#39;03-15-2023&#39;, &#39;age&#39;: 0}, {&#39;id&#39;: 4, &#39;name&#39;: &#39;Eli Whitney&#39;, &#39;uid&#39;: &#39;whit&#39;, &#39;dob&#39;: &#39;03-15-2023&#39;, &#39;age&#39;: 0}, {&#39;id&#39;: 5, &#39;name&#39;: &#39;Indiana Jones&#39;, &#39;uid&#39;: &#39;indi&#39;, &#39;dob&#39;: &#39;10-21-1920&#39;, &#39;age&#39;: 102}, {&#39;id&#39;: 6, &#39;name&#39;: &#39;Marion Ravenwood&#39;, &#39;uid&#39;: &#39;raven&#39;, &#39;dob&#39;: &#39;10-21-1921&#39;, &#39;age&#39;: 101}, {&#39;id&#39;: 7, &#39;name&#39;: &#39;alex&#39;, &#39;uid&#39;: &#39;&#39;, &#39;dob&#39;: &#39;01-01-1940&#39;, &#39;age&#39;: 83}] . Hacks . Add this Blog to you own Blogging site. In the Blog add notes and observations on each code cell. | Change blog to your own database. | Add additional CRUD Add Update functionality to this blog. | Add Delete functionality to this blog. | . | .",
            "url": "https://ylu-1258.github.io/YLu-Blog/2023/03/13/AP-unit2-4a.html",
            "relUrl": "/2023/03/13/AP-unit2-4a.html",
            "date": " • Mar 13, 2023"
        }
        
    
  
    
        ,"post5": {
            "title": "Intro to Pandas",
            "content": "Pandas . I will use a dataset of Fortune 1000 from Kaggle datasets to explore pandas library here. . The Fortune 1000 dataset is from the Fortune website. It contains U.S. company data for the year 2021. The dataset is 1000 rows and 18 columns. . Features: . Company - values are the name of the company Rank - The 2021 rank established by Fortune (1-1000) | Rank Change - The change in the rank from 2020 to 2021. There is only a rank change listed if the company is currently in the top 500 and was previously in the top 500. | Revenue - Revenue of each company in millions. This is the criteria used to rank each company. | Profit - Profit of each company in millions. Num. of Employees - The number of employees each company employs. | Sector - The sector of the market the company operates in. | City - The city where the company&#39;s headquarters is located. | State - The state where the company&#39;s headquarters is located | Newcomer - Indicates whether or not the company is new to the top Fortune 500 (&quot;yes&quot; or &quot;no&quot;). No value will be listed for companies outside of the top 500. | CEO Founder - Indicates whether the CEO of the company is also the founder (&quot;yes&quot; or &quot;no&quot;). | CEO Woman - Indicates whether the CEO of the company is a woman (&quot;yes&quot; or &quot;no&quot;). | Profitable - Indicates whether the company is profitable or not (&quot;yes&quot; or &quot;no&quot;). | Prev. Rank - The 2020 rank of the company, as established by Fortune. There will only be previous rank data for the top 500 companies. | CEO - The name of the CEO of the company | Website - The url of the company website | Ticker - The stock ticker symbol of public companies. Some rows will have empty values because the company is a private corporation. | Market Cap - The market cap (or value) of the company in millions. Some rows will have empty values because the company is private. Market valuations were determined on January 20, 2021. | . !wget -nc /content/ https://datasets21.s3-us-west-1.amazonaws.com/Fortune_1000.csv . /content/: Scheme missing. File ‘Fortune_1000.csv’ already there; not retrieving. . import pandas as pd f1000 = pd.read_csv(&#39;Fortune_1000.csv&#39;,index_col=0) . f1000.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Index: 1000 entries, Walmart to Liberty Oilfield Services Data columns (total 17 columns): # Column Non-Null Count Dtype -- -- 0 rank 1000 non-null int64 1 rank_change 1000 non-null float64 2 revenue 1000 non-null float64 3 profit 998 non-null float64 4 num. of employees 1000 non-null int64 5 sector 1000 non-null object 6 city 1000 non-null object 7 state 1000 non-null object 8 newcomer 500 non-null object 9 ceo_founder 1000 non-null object 10 ceo_woman 1000 non-null object 11 profitable 1000 non-null object 12 prev_rank 1000 non-null object 13 CEO 992 non-null object 14 Website 1000 non-null object 15 Ticker 938 non-null object 16 Market Cap 960 non-null object dtypes: float64(3), int64(2), object(12) memory usage: 140.6+ KB . f1000.head(3) . rank rank_change revenue profit num. of employees sector city state newcomer ceo_founder ceo_woman profitable prev_rank CEO Website Ticker Market Cap . company . Walmart 1 | 0.0 | 523964.0 | 14881.0 | 2200000 | Retailing | Bentonville | AR | no | no | no | yes | 1.0 | C. Douglas McMillon | https://www.stock.walmart.com | WMT | 411690 | . Amazon 2 | 3.0 | 280522.0 | 11588.0 | 798000 | Retailing | Seattle | WA | no | yes | no | yes | 5.0 | Jeffrey P. Bezos | https://www.amazon.com | AMZN | 1637405 | . Exxon Mobil 3 | -1.0 | 264938.0 | 14340.0 | 74900 | Energy | Irving | TX | no | no | no | yes | 2.0 | Darren W. Woods | https://www.exxonmobil.com | XOM | 177923 | . Select data using those labels . Because the axes in pandas have labels, I can select data using those labels — unlike in NumPy, where I needed to know the exact index location. To do this, I can use the DataFrame.loc[] attribute. The syntax for DataFrame.loc[] is: . df.loc[row_label, column_label] . Select Single Column . companies = f1000.loc[:,&#39;profit&#39;] print(companies) print(type(f1000)) print(type(companies)) . company Walmart 14881.0 Amazon 11588.0 Exxon Mobil 14340.0 Apple 55256.0 CVS Health 6634.0 ... Mr. Cooper Group 274.0 Herc Holdings 47.5 Healthpeak Properties 45.5 SPX FLOW -95.1 Liberty Oilfield Services 39.0 Name: profit, Length: 1000, dtype: float64 &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; &lt;class &#39;pandas.core.series.Series&#39;&gt; . Series object . print(f1000.loc[:,&#39;revenue&#39;]) print(f1000.loc[&#39;Apple&#39;,&#39;revenue&#39;]) . company Walmart 523964.0 Amazon 280522.0 Exxon Mobil 264938.0 Apple 260174.0 CVS Health 256776.0 ... Mr. Cooper Group 2007.0 Herc Holdings 1999.0 Healthpeak Properties 1997.4 SPX FLOW 1996.3 Liberty Oilfield Services 1990.3 Name: revenue, Length: 1000, dtype: float64 260174.0 . Select multiple columns . List of columns | Slice of columns | . f1000[[&#39;rank&#39;,&#39;revenue&#39;]] #f1000.loc[:,&#39;rank&#39;:&#39;sector&#39;] . rank revenue . company . Walmart 1 | 523964.0 | . Amazon 2 | 280522.0 | . Exxon Mobil 3 | 264938.0 | . Apple 4 | 260174.0 | . CVS Health 5 | 256776.0 | . ... ... | ... | . Mr. Cooper Group 996 | 2007.0 | . Herc Holdings 997 | 1999.0 | . Healthpeak Properties 998 | 1997.4 | . SPX FLOW 999 | 1996.3 | . Liberty Oilfield Services 1000 | 1990.3 | . 1000 rows × 2 columns . Select rows by labels . f1000.loc[[&#39;Amazon&#39;, &#39;Apple&#39;]] #f1000.loc[&#39;Amazon&#39;:&#39;Apple&#39;] . rank rank_change revenue profit num. of employees sector city state newcomer ceo_founder ceo_woman profitable prev_rank CEO Website Ticker Market Cap . company . Amazon 2 | 3.0 | 280522.0 | 11588.0 | 798000 | Retailing | Seattle | WA | no | yes | no | yes | 5.0 | Jeffrey P. Bezos | https://www.amazon.com | AMZN | 1637405 | . Apple 4 | -1.0 | 260174.0 | 55256.0 | 137000 | Technology | Cupertino | CA | no | no | no | yes | 3.0 | Timothy D. Cook | https://www.apple.com | AAPL | 2221176 | . Series.value_counts() method . Series.value_counts() method. This method displays each unique non-null value in a column and their counts in order. . sector_value_counts = f1000[&#39;sector&#39;].value_counts(ascending=True) print(sector_value_counts) . Food &amp; Drug Stores 10 Telecommunications 11 Apparel 16 Motor Vehicles &amp; Parts 22 Aerospace &amp; Defense 22 Media 25 Household Products 26 Hotels, Restaurants &amp; Leisure 27 Chemicals 27 Engineering &amp; Construction 30 Wholesalers 35 Food, Beverages &amp; Tobacco 37 Transportation 38 Materials 46 Industrials 50 Business Services 52 Health Care 71 Retailing 75 Technology 109 Energy 109 Financials 162 Name: sector, dtype: int64 . f1000[&#39;sector&#39;].value_counts().loc[&#39;Technology&#39;] . 109 . We can also try to locate what the largest increase or decrease in compay rank were. . #f1000[&#39;rank_change&#39;].max() #f1000[&#39;rank_change&#39;].min() f1000[&#39;rank_change&#39;].describe() . count 1000.000000 mean 0.426000 std 22.424169 min -186.000000 25% 0.000000 50% 0.000000 75% 0.000000 max 224.000000 Name: rank_change, dtype: float64 . f1000[&#39;rank_change&#39;].value_counts() . 0.0 544 -1.0 22 -2.0 18 2.0 16 4.0 15 ... -30.0 1 -43.0 1 98.0 1 86.0 1 -87.0 1 Name: rank_change, Length: 118, dtype: int64 . Example: . List out the numbers of companies in the Fortune 1000 of the top 3 states . top_3_states = f1000[&#39;state&#39;].value_counts().head(3) print(top_3_states) . CA 121 TX 95 NY 89 Name: state, dtype: int64 . Example: . find the company in California that employs the most people in the dataset. . I can use the DataFrame.sort_values() method to sort the rows on the employees column . f1000[f1000[&#39;state&#39;]==&#39;CA&#39;].sort_values(&#39;num. of employees&#39;,ascending=False).head(1) . rank rank_change revenue profit num. of employees sector city state newcomer ceo_founder ceo_woman profitable prev_rank CEO Website Ticker Market Cap . company . Wells Fargo 30 | -1.0 | 103915.0 | 19549.0 | 259800 | Financials | San Francisco | CA | no | no | no | yes | 29.0 | Charles W. Scharf | https://www.wellsfargo.com | WFC | 99941 | . Exercise: . find the unique list of states in the dataset . To identify the unique states, I can use the Series.unique() method. This method returns an array of unique values from any series. . states = f1000[&#39;state&#39;].unique() print(states) . [&#39;AR&#39; &#39;WA&#39; &#39;TX&#39; &#39;CA&#39; &#39;RI&#39; &#39;NE&#39; &#39;MN&#39; &#39;PA&#39; &#39;MI&#39; &#39;CT&#39; &#39;OH&#39; &#39;NY&#39; &#39;IL&#39; &#39;DC&#39; &#39;NC&#39; &#39;GA&#39; &#39;IN&#39; &#39;MA&#39; &#39;NJ&#39; &#39;VA&#39; &#39;MO&#39; &#39;TN&#39; &#39;KY&#39; &#39;ID&#39; &#39;MD&#39; &#39;OR&#39; &#39;FL&#39; &#39;WI&#39; &#39;CO&#39; &#39;OK&#39; &#39;LA&#39; &#39;DE&#39; &#39;AZ&#39; &#39;IA&#39; &#39;NV&#39; &#39;KS&#39; &#39;AL&#39; &#39;SC&#39; &#39;ND&#39; &#39;NH&#39; &#39;MS&#39; &#39;PR&#39; &#39;UT&#39; &#39;HI&#39; &#39;VT&#39; &#39;ME&#39;] . Demonstration: . I&#39;m going to produce the following dictionary of the top employer in each state: . create an empty dictionary, top_employer_by_state to store the results of the exercise. . | Use the Series.unique() method to create an array of unique values from the state column . | Use a for loop to iterate over the array unique states. In each iteration: . | Select only the rows that have a state name equal to the current iteration. | Use DataFrame.sort_values() to sort those rows by the num. of employees column in descending order. | Select the first row from the sorted dataframe and convert the Dataframe into a series using DataFrame.squeeze() | Extract the company name from the index label company by Series.name. | Assign the results to the top_employer_by_state dictionary, using the state name as the key, and the company name as the value. | . top_exmployer_by_state = {} states = f1000[&#39;state&#39;].unique() for state in states: selected_companies = f1000[f1000[&#39;state&#39;]==state] top_exmployer_by_state[state] = selected_companies.sort_values(&#39;num. of employees&#39;, ascending=False).head(1).squeeze().name . for key in top_exmployer_by_state: print(key, &#39; : &#39;, top_exmployer_by_state[key]) . AR : Walmart WA : Amazon TX : Yum China Holdings CA : Wells Fargo RI : CVS Health NE : Berkshire Hathaway MN : Target PA : Aramark MI : Ford Motor CT : XPO Logistics OH : Kroger NY : IBM IL : Walgreens Boots Alliance DC : Danaher NC : Lowe&amp;#8217;s GA : Home Depot IN : Anthem MA : TJX NJ : Cognizant Technology Solutions VA : Hilton Worldwide Holdings MO : Emerson Electric TN : FedEx KY : Humana ID : Albertsons MD : Marriott International OR : Nike FL : Publix Super Markets WI : Kohl&amp;#8217;s CO : VF OK : Helmerich &amp; Payne LA : Lumen Technologies DE : DuPont AZ : Republic Services IA : Casey&amp;#8217;s General Stores NV : MGM Resorts International KS : Yellow AL : Encompass Health SC : Sonoco Products ND : MDU Resources Group NH : PC Connection MS : Sanderson Farms PR : Popular UT : Nu Skin Enterprises HI : Hawaiian Holdings VT : NLV Financial ME : IDEXX Laboratories . Reference . pandas API reference . pandas vs. NumPy .",
            "url": "https://ylu-1258.github.io/YLu-Blog/jupyter/interests/2023/03/12/Pandas-hack.html",
            "relUrl": "/jupyter/interests/2023/03/12/Pandas-hack.html",
            "date": " • Mar 12, 2023"
        }
        
    
  
    
        ,"post6": {
            "title": "Unit 2.3 Extracting Information from Data, Pandas",
            "content": "Files To Get . Save this file to your _notebooks folder . wget https://raw.githubusercontent.com/nighthawkcoders/APCSP/master/_notebooks/2023-03-09-AP-unit2-3.ipynb . Save these files into a subfolder named files in your _notebooks folder . wget https://raw.githubusercontent.com/nighthawkcoders/APCSP/master/_notebooks/files/data.csv . wget https://raw.githubusercontent.com/nighthawkcoders/APCSP/master/_notebooks/files/grade.json . Save this image into a subfolder named images in your _notebooks folder . wget https://raw.githubusercontent.com/nighthawkcoders/APCSP/master/_notebooks/images/table_dataframe.png . Pandas and DataFrames . In this lesson we will be exploring data analysis using Pandas. . College Board talks about ideas like Tools. &quot;the ability to process data depends on users capabilities and their tools&quot; | Combining Data. &quot;combine county data sets&quot; | Status on Data&quot;determining the artist with the greatest attendance during a particular month&quot; | Data poses challenge. &quot;the need to clean data&quot;, &quot;incomplete data&quot; | . | . From Pandas Overview -- When working with tabular data, such as data stored in spreadsheets or databases, pandas is the right tool for you. pandas will help you to explore, clean, and process your data. In pandas, a data table is called a DataFrame. | . . &#39;&#39;&#39;Pandas is used to gather data sets through its DataFrames implementation&#39;&#39;&#39; import pandas as pd . Cleaning Data . When looking at a data set, check to see what data needs to be cleaned. Examples include: . Missing Data Points | Invalid Data | Inaccurate Data | . Run the following code to see what needs to be cleaned . df = pd.read_json(&#39;files/grade.json&#39;) print(df) # What part of the data set needs to be cleaned? # From PBL learning, what is a good time to clean data? Hint, remember Garbage in, Garbage out? . Student ID Year in School GPA 0 123 12 3.57 1 246 10 4.00 2 578 12 2.78 3 469 11 3.45 4 324 Junior 4.75 5 313 20 3.33 6 145 12 2.95 7 167 10 3.90 8 235 9th Grade 3.15 9 nil 9 2.80 10 469 11 3.45 11 456 10 2.75 . Extracting Info . Take a look at some features that the Pandas library has that extracts info from the dataset . DataFrame Extract Column . print(df[[&#39;GPA&#39;]]) print() #try two columns and remove the index from print statement print(df[[&#39;Student ID&#39;,&#39;GPA&#39;]].to_string(index=False)) . GPA 0 3.57 1 4.00 2 2.78 3 3.45 4 4.75 5 3.33 6 2.95 7 3.90 8 3.15 9 2.80 10 3.45 11 2.75 Student ID GPA 123 3.57 246 4.00 578 2.78 469 3.45 324 4.75 313 3.33 145 2.95 167 3.90 235 3.15 nil 2.80 469 3.45 456 2.75 . DataFrame Sort . print(df.sort_values(by=[&#39;GPA&#39;])) print() #sort the values in reverse order print(df.sort_values(by=[&#39;GPA&#39;], ascending=False)) . Student ID Year in School GPA 11 456 10 2.75 2 578 12 2.78 9 nil 9 2.80 6 145 12 2.95 8 235 9th Grade 3.15 5 313 20 3.33 3 469 11 3.45 10 469 11 3.45 0 123 12 3.57 7 167 10 3.90 1 246 10 4.00 4 324 Junior 4.75 Student ID Year in School GPA 4 324 Junior 4.75 1 246 10 4.00 7 167 10 3.90 0 123 12 3.57 3 469 11 3.45 10 469 11 3.45 5 313 20 3.33 8 235 9th Grade 3.15 6 145 12 2.95 9 nil 9 2.80 2 578 12 2.78 11 456 10 2.75 . DataFrame Selection or Filter . print(df[df.GPA &gt; 3.00]) . Student ID Year in School GPA 0 123 12 3.57 1 246 10 4.00 3 469 11 3.45 4 324 Junior 4.75 5 313 20 3.33 7 167 10 3.90 8 235 9th Grade 3.15 10 469 11 3.45 . DataFrame Selection Max and Min . print(df[df.GPA == df.GPA.max()]) print() print(df[df.GPA == df.GPA.min()]) . Student ID Year in School GPA 4 324 Junior 4.75 Student ID Year in School GPA 11 456 10 2.75 . Create your own DataFrame . Using Pandas allows you to create your own DataFrame in Python. . Python Dictionary to Pandas DataFrame . import pandas as pd #the data can be stored as a python dictionary dict = { &quot;calories&quot;: [420, 380, 390, 400], &quot;duration&quot;: [50, 40, 45, 47] } #stores the data in a data frame print(&quot;-Dict_to_DF&quot;) df = pd.DataFrame(dict) print(df) print(&quot;-Dict_to_DF_labels--&quot;) #or with the index argument, you can label rows. df = pd.DataFrame(dict, index = [&quot;day1&quot;, &quot;day2&quot;, &quot;day3&quot;, &quot;day4&quot;]) print(df) . -Dict_to_DF calories duration 0 420 50 1 380 40 2 390 45 3 400 47 -Dict_to_DF_labels-- calories duration day1 420 50 day2 380 40 day3 390 45 day4 400 47 . Examine DataFrame Rows . print(&quot;-Examine Selected Rows&quot;) #use a list for multiple labels: print(df.loc[[&quot;day1&quot;, &quot;day3&quot;]]) #refer to the row index: print(&quot;--Examine Single Row--&quot;) print(df.loc[&quot;day1&quot;]) . -Examine Selected Rows calories duration day1 420 50 day3 390 45 --Examine Single Row-- calories 420 duration 50 Name: day1, dtype: int64 . Pandas DataFrame Information . print(df.info()) . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Index: 4 entries, day1 to day4 Data columns (total 2 columns): # Column Non-Null Count Dtype -- -- 0 calories 4 non-null int64 1 duration 4 non-null int64 dtypes: int64(2) memory usage: 268.0+ bytes None . Example of larger data set . Pandas can read CSV and many other types of files, run the following code to see more features with a larger data set . import pandas as pd #read csv and sort &#39;Duration&#39; largest to smallest df = pd.read_csv(&#39;files/data.csv&#39;).sort_values(by=[&#39;Duration&#39;], ascending=False) print(&quot;--Duration Top 10&quot;) print(df.head(10)) print(&quot;--Duration Bottom 10&quot;) print(df.tail(10)) . --Duration Top 10 Duration Pulse Maxpulse Calories 69 300 108 143 1500.2 79 270 100 131 1729.0 109 210 137 184 1860.4 60 210 108 160 1376.0 106 180 90 120 800.3 90 180 101 127 600.1 65 180 90 130 800.4 61 160 110 137 1034.4 62 160 109 135 853.0 67 150 107 130 816.0 --Duration Bottom 10 Duration Pulse Maxpulse Calories 68 20 106 136 110.4 100 20 95 112 77.7 89 20 83 107 50.3 135 20 136 156 189.0 94 20 150 171 127.4 95 20 151 168 229.4 139 20 141 162 222.4 64 20 110 130 131.4 112 15 124 139 124.2 93 15 80 100 50.5 . APIs are a Source for Writing Programs with Data . 3rd Party APIs are a great source for creating Pandas Data Frames. . Data can be fetched and resulting json can be placed into a Data Frame | Observe output, this looks very similar to a Database | . &#39;&#39;&#39;Pandas can be used to analyze data&#39;&#39;&#39; import pandas as pd import requests def fetch(): &#39;&#39;&#39;Obtain data from an endpoint&#39;&#39;&#39; url = &quot;https://flask.nighthawkcodingsociety.com/api/covid/&quot; fetch = requests.get(url) json = fetch.json() # filter data for requirement df = pd.DataFrame(json[&#39;countries_stat&#39;]) # filter endpoint for country stats print(df.loc[0:5, &#39;country_name&#39;:&#39;deaths&#39;]) # show row 0 through 5 and columns country_name through deaths fetch() . KeyboardInterrupt Traceback (most recent call last) Cell In[11], line 15 12 df = pd.DataFrame(json[&#39;countries_stat&#39;]) # filter endpoint for country stats 13 print(df.loc[0:5, &#39;country_name&#39;:&#39;deaths&#39;]) # show row 0 through 5 and columns country_name through deaths &gt; 15 fetch() Cell In[11], line 8, in fetch() 6 &#39;&#39;&#39;Obtain data from an endpoint&#39;&#39;&#39; 7 url = &#34;https://flask.nighthawkcodingsociety.com/api/covid/&#34; -&gt; 8 fetch = requests.get(url) 9 json = fetch.json() 11 # filter data for requirement File /usr/lib/python3/dist-packages/requests/api.py:75, in get(url, params, **kwargs) 64 r&#34;&#34;&#34;Sends a GET request. 65 66 :param url: URL for the new :class:`Request` object. (...) 71 :rtype: requests.Response 72 &#34;&#34;&#34; 74 kwargs.setdefault(&#39;allow_redirects&#39;, True) &gt; 75 return request(&#39;get&#39;, url, params=params, **kwargs) File /usr/lib/python3/dist-packages/requests/api.py:60, in request(method, url, **kwargs) 56 # By using the &#39;with&#39; statement we are sure the session is closed, thus we 57 # avoid leaving sockets open which can trigger a ResourceWarning in some 58 # cases, and look like a memory leak in others. 59 with sessions.Session() as session: &gt; 60 return session.request(method=method, url=url, **kwargs) File /usr/lib/python3/dist-packages/requests/sessions.py:533, in Session.request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json) 528 send_kwargs = { 529 &#39;timeout&#39;: timeout, 530 &#39;allow_redirects&#39;: allow_redirects, 531 } 532 send_kwargs.update(settings) --&gt; 533 resp = self.send(prep, **send_kwargs) 535 return resp File /usr/lib/python3/dist-packages/requests/sessions.py:646, in Session.send(self, request, **kwargs) 643 start = preferred_clock() 645 # Send the request --&gt; 646 r = adapter.send(request, **kwargs) 648 # Total elapsed time of the request (approximately) 649 elapsed = preferred_clock() - start File /usr/lib/python3/dist-packages/requests/adapters.py:439, in HTTPAdapter.send(self, request, stream, timeout, verify, cert, proxies) 437 try: 438 if not chunked: --&gt; 439 resp = conn.urlopen( 440 method=request.method, 441 url=url, 442 body=request.body, 443 headers=request.headers, 444 redirect=False, 445 assert_same_host=False, 446 preload_content=False, 447 decode_content=False, 448 retries=self.max_retries, 449 timeout=timeout 450 ) 452 # Send the request. 453 else: 454 if hasattr(conn, &#39;proxy_pool&#39;): File /usr/lib/python3/dist-packages/urllib3/connectionpool.py:665, in HTTPConnectionPool.urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw) 662 self._prepare_proxy(conn) 664 # Make the request on the httplib connection object. --&gt; 665 httplib_response = self._make_request( 666 conn, 667 method, 668 url, 669 timeout=timeout_obj, 670 body=body, 671 headers=headers, 672 chunked=chunked, 673 ) 675 # If we&#39;re going to release the connection in ``finally:``, then 676 # the response doesn&#39;t need to know about the connection. Otherwise 677 # it will also try to release it and we&#39;ll have a double-release 678 # mess. 679 response_conn = conn if not release_conn else None File /usr/lib/python3/dist-packages/urllib3/connectionpool.py:376, in HTTPConnectionPool._make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw) 374 # Trigger any extra validation we need to do. 375 try: --&gt; 376 self._validate_conn(conn) 377 except (SocketTimeout, BaseSSLError) as e: 378 # Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout. 379 self._raise_timeout(err=e, url=url, timeout_value=conn.timeout) File /usr/lib/python3/dist-packages/urllib3/connectionpool.py:996, in HTTPSConnectionPool._validate_conn(self, conn) 994 # Force connect early to allow us to validate the connection. 995 if not getattr(conn, &#34;sock&#34;, None): # AppEngine might not have `.sock` --&gt; 996 conn.connect() 998 if not conn.is_verified: 999 warnings.warn( 1000 ( 1001 &#34;Unverified HTTPS request is being made to host &#39;%s&#39;. &#34; (...) 1006 InsecureRequestWarning, 1007 ) File /usr/lib/python3/dist-packages/urllib3/connection.py:314, in VerifiedHTTPSConnection.connect(self) 312 def connect(self): 313 # Add certificate verification --&gt; 314 conn = self._new_conn() 315 hostname = self.host 317 # Google App Engine&#39;s httplib does not define _tunnel_host File /usr/lib/python3/dist-packages/urllib3/connection.py:159, in HTTPConnection._new_conn(self) 156 extra_kw[&#34;socket_options&#34;] = self.socket_options 158 try: --&gt; 159 conn = connection.create_connection( 160 (self._dns_host, self.port), self.timeout, **extra_kw 161 ) 163 except SocketTimeout: 164 raise ConnectTimeoutError( 165 self, 166 &#34;Connection to %s timed out. (connect timeout=%s)&#34; 167 % (self.host, self.timeout), 168 ) File /usr/lib/python3/dist-packages/urllib3/util/connection.py:74, in create_connection(address, timeout, source_address, socket_options) 72 if source_address: 73 sock.bind(source_address) &gt; 74 sock.connect(sa) 75 return sock 77 except socket.error as e: KeyboardInterrupt: . Hacks . Early Seed award . Add this Blog to you own Blogging site. | Have all lecture files saved to your files directory before Tech Talk starts. Have data.csv open in vscode. Don&#39;t tell anyone. Show to Teacher. | . AP Prep . Add this Blog to you own Blogging site. In the Blog add notes and observations on each code cell. | In blog add College Board practice problems for 2.3. | . The next 4 weeks, Teachers want you to improve your understanding of data. Look at the blog and others on Unit 2. Your intention is to find some things to differentiate your individual College Board project. . Create or Find your own dataset. The suggestion is to use a JSON file, integrating with your PBL project would be Fambulous. | . When choosing a data set, think about the following:- Does it have a good sample size? - Is there bias in the data? - Does the data set need to be cleaned? - What is the purpose of the data set? - ... . Continue this Blog using Pandas extract info from that dataset (ex. max, min, mean, median, mode, etc.) | . Hack Helpers . Here is how Mort started on this assignment by asking ChatGPT ... Regarding Python Pandas, what are some data sets that would be good for learning Pandas? . There are many data sets that are suitable for learning pandas, depending on your interests and the skills you want to develop. Here are some suggestions:- Titanic Dataset: This is a classic dataset for data analysis and machine learning, and is often used as an introduction to pandas. The dataset contains information about passengers on the Titanic, including their demographics, ticket class, and survival status. . Iris Dataset: This is another classic dataset that is often used in machine learning and data analysis courses. The dataset contains information about iris flowers, including their sepal length, sepal width, petal length, and petal width. . | NBA Player Stats: This dataset contains information about NBA players, including their stats for various games, their positions, and their salaries. It is a great dataset for learning how to clean and manipulate data using pandas. . | Housing Prices Dataset: This dataset contains information about housing prices in a particular city or region, including variables such as the number of bedrooms, square footage, and price. It is a great dataset for learning how to manipulate and analyze numerical data using pandas. . | COVID-19 Dataset: This dataset contains information about the COVID-19 pandemic, including the number of cases, deaths, and recoveries in various countries and regions. It is a great dataset for learning how to manipulate and analyze time-series data using pandas. . | World Bank Dataset: This dataset contains information about economic indicators from various countries, such as GDP, population, and poverty rates. It is a great dataset for learning how to manipulate and analyze large datasets using pandas. . | . | These are just a few examples of the many datasets that are suitable for learning pandas. The best dataset for you will depend on your interests and the specific skills you want to develop. . Follow up question, &quot;where can I find Titanic data set?&quot; . | The Titanic dataset is a popular dataset for data analysis and machine learning, and it can be found on various websites and data repositories. Here are a few places where you can find the Titanic dataset:- Kaggle: The Titanic dataset is available on Kaggle, which is a popular platform for data scientists and machine learning enthusiasts. You can download the dataset from the Kaggle website after creating an account. . UCI Machine Learning Repository: The Titanic dataset is also available on the UCI Machine Learning Repository, which is a collection of datasets that are commonly used for machine learning research. You can download the dataset from the UCI Machine Learning Repository website. . | Seaborn library: If you have the Seaborn library installed in your Python environment, you can load the Titanic dataset directly from the library using the following code: . import seaborn as sns titanic_data = sns.load_dataset(&#39;titanic&#39;) . | . | . Titanic Data . Look at a sample of data. . import seaborn as sns # Load the titanic dataset titanic_data = sns.load_dataset(&#39;titanic&#39;) print(&quot;Titanic Data&quot;) print(titanic_data.columns) # titanic data set print(titanic_data[[&#39;survived&#39;,&#39;pclass&#39;, &#39;sex&#39;, &#39;age&#39;, &#39;sibsp&#39;, &#39;parch&#39;, &#39;class&#39;, &#39;fare&#39;, &#39;embark_town&#39;]]) # look at selected columns . Titanic Data Index([&#39;survived&#39;, &#39;pclass&#39;, &#39;sex&#39;, &#39;age&#39;, &#39;sibsp&#39;, &#39;parch&#39;, &#39;fare&#39;, &#39;embarked&#39;, &#39;class&#39;, &#39;who&#39;, &#39;adult_male&#39;, &#39;deck&#39;, &#39;embark_town&#39;, &#39;alive&#39;, &#39;alone&#39;], dtype=&#39;object&#39;) survived pclass sex age sibsp parch class fare 0 0 3 male 22.0 1 0 Third 7.2500 1 1 1 female 38.0 1 0 First 71.2833 2 1 3 female 26.0 0 0 Third 7.9250 3 1 1 female 35.0 1 0 First 53.1000 4 0 3 male 35.0 0 0 Third 8.0500 .. ... ... ... ... ... ... ... ... 886 0 2 male 27.0 0 0 Second 13.0000 887 1 1 female 19.0 0 0 First 30.0000 888 0 3 female NaN 1 2 Third 23.4500 889 1 1 male 26.0 0 0 First 30.0000 890 0 3 male 32.0 0 0 Third 7.7500 embark_town 0 Southampton 1 Cherbourg 2 Southampton 3 Southampton 4 Southampton .. ... 886 Southampton 887 Southampton 888 Southampton 889 Cherbourg 890 Queenstown [891 rows x 9 columns] . Use Pandas to clean the data. Most analysis, like Machine Learning or even Pandas in general like data to be in standardized format. This is called &#39;Training&#39; or &#39;Cleaning&#39; data. . # Preprocess the data from sklearn.preprocessing import OneHotEncoder td = titanic_data td.drop([&#39;alive&#39;, &#39;who&#39;, &#39;adult_male&#39;, &#39;class&#39;, &#39;embark_town&#39;, &#39;deck&#39;], axis=1, inplace=True) td.dropna(inplace=True) td[&#39;sex&#39;] = td[&#39;sex&#39;].apply(lambda x: 1 if x == &#39;male&#39; else 0) td[&#39;alone&#39;] = td[&#39;alone&#39;].apply(lambda x: 1 if x == True else 0) # Encode categorical variables enc = OneHotEncoder(handle_unknown=&#39;ignore&#39;) enc.fit(td[[&#39;embarked&#39;]]) onehot = enc.transform(td[[&#39;embarked&#39;]]).toarray() cols = [&#39;embarked_&#39; + val for val in enc.categories_[0]] td[cols] = pd.DataFrame(onehot) td.drop([&#39;embarked&#39;], axis=1, inplace=True) td.dropna(inplace=True) print(td) . survived pclass sex age sibsp parch fare alone embarked_C 0 0 3 1 22.0 1 0 7.2500 0 0.0 1 1 1 0 38.0 1 0 71.2833 0 1.0 2 1 3 0 26.0 0 0 7.9250 1 0.0 3 1 1 0 35.0 1 0 53.1000 0 0.0 4 0 3 1 35.0 0 0 8.0500 1 0.0 .. ... ... ... ... ... ... ... ... ... 705 0 2 1 39.0 0 0 26.0000 1 0.0 706 1 2 0 45.0 0 0 13.5000 1 0.0 707 1 1 1 42.0 0 0 26.2875 1 0.0 708 1 1 0 22.0 0 0 151.5500 1 0.0 710 1 1 0 24.0 0 0 49.5042 1 1.0 embarked_Q embarked_S 0 0.0 1.0 1 0.0 0.0 2 0.0 1.0 3 0.0 1.0 4 0.0 1.0 .. ... ... 705 0.0 1.0 706 0.0 1.0 707 1.0 0.0 708 0.0 1.0 710 0.0 0.0 [564 rows x 11 columns] . The result of &#39;Training&#39; data is making it easier to analyze or make conclusions. In looking at the Titanic, as you clean you would probably want to make assumptions on likely chance of survival. . This would involve analyzing various factors (such as age, gender, class, etc.) that may have affected a person&#39;s chances of survival, and using that information to make predictions about whether an individual would have survived or not. . Data description:- Survival - Survival (0 = No; 1 = Yes). Not included in test.csv file. - Pclass - Passenger Class (1 = 1st; 2 = 2nd; 3 = 3rd) . Name - Name | Sex - Sex | Age - Age | Sibsp - Number of Siblings/Spouses Aboard | Parch - Number of Parents/Children Aboard | Ticket - Ticket Number | Fare - Passenger Fare | Cabin - Cabin | Embarked - Port of Embarkation (C = Cherbourg; Q = Queenstown; S = Southampton) | . | Perished Mean/Average . | . print(titanic_data.query(&quot;survived == 0&quot;).mean()) . survived 0.000000 pclass 2.464072 sex 0.844311 age 31.073353 sibsp 0.562874 parch 0.398204 fare 24.835902 alone 0.616766 embarked_C 0.185629 embarked_Q 0.038922 embarked_S 0.775449 dtype: float64 . Survived Mean/Average | . print(td.query(&quot;survived == 1&quot;).mean()) . survived 1.000000 pclass 1.878261 sex 0.326087 age 28.481522 sibsp 0.504348 parch 0.508696 fare 50.188806 alone 0.456522 embarked_C 0.152174 embarked_Q 0.034783 embarked_S 0.813043 dtype: float64 . Survived Max and Min Stats . print(td.query(&quot;survived == 1&quot;).max()) print(td.query(&quot;survived == 1&quot;).min()) . survived 1.0000 pclass 3.0000 sex 1.0000 age 80.0000 sibsp 4.0000 parch 5.0000 fare 512.3292 alone 1.0000 embarked_C 1.0000 embarked_Q 1.0000 embarked_S 1.0000 dtype: float64 survived 1.00 pclass 1.00 sex 0.00 age 0.75 sibsp 0.00 parch 0.00 fare 0.00 alone 0.00 embarked_C 0.00 embarked_Q 0.00 embarked_S 0.00 dtype: float64 . Machine Learning . From Tutorials Point%20is,a%20consistence%20interface%20in%20Python). Scikit-learn (Sklearn) is the most useful and robust library for machine learning in Python. It provides a selection of efficient tools for machine learning and statistical modeling including classification, regression, clustering and dimensionality reduction via a consistence interface in Python.&gt; Description from ChatGPT. The Titanic dataset is a popular dataset for data analysis and machine learning. In the context of machine learning, accuracy refers to the percentage of correctly classified instances in a set of predictions. In this case, the testing data is a subset of the original Titanic dataset that the decision tree model has not seen during training......After training the decision tree model on the training data, we can evaluate its performance on the testing data by making predictions on the testing data and comparing them to the actual outcomes. The accuracy of the decision tree classifier on the testing data tells us how well the model generalizes to new data that it hasn&#39;t seen before......For example, if the accuracy of the decision tree classifier on the testing data is 0.8 (or 80%), this means that 80% of the predictions made by the model on the testing data were correct....Chance of survival could be done using various machine learning techniques, including decision trees, logistic regression, or support vector machines, among others. . Code Below prepares data for further analysis and provides an Accuracy. IMO, you would insert a new passenger and predict survival. Datasets could be used on various factors like prediction if a player will hit a Home Run, or a Stock will go up or down. Decision Trees, prediction by a piecewise constant approximation. - Logistic Regression, the probabilities describing the possible outcomes. | . | . from sklearn.model_selection import train_test_split from sklearn.tree import DecisionTreeClassifier from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score # Split arrays or matrices into random train and test subsets. X = td.drop(&#39;survived&#39;, axis=1) y = td[&#39;survived&#39;] X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Train a decision tree classifier dt = DecisionTreeClassifier() dt.fit(X_train, y_train) # Test the model y_pred = dt.predict(X_test) accuracy = accuracy_score(y_test, y_pred) print(&#39;DecisionTreeClassifier Accuracy:&#39;, accuracy) # Train a logistic regression model logreg = LogisticRegression() logreg.fit(X_train, y_train) # Test the model y_pred = logreg.predict(X_test) accuracy = accuracy_score(y_test, y_pred) print(&#39;LogisticRegression Accuracy:&#39;, accuracy) . DecisionTreeClassifier Accuracy: 0.7529411764705882 LogisticRegression Accuracy: 0.788235294117647 . /home/eris29/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression n_iter_i = _check_optimize_result( .",
            "url": "https://ylu-1258.github.io/YLu-Blog/2023/03/09/AP-unit2-3.html",
            "relUrl": "/2023/03/09/AP-unit2-3.html",
            "date": " • Mar 9, 2023"
        }
        
    
  
    
        ,"post7": {
            "title": "Unit 2.2 Data Compression, Images HACKS",
            "content": "The Breakdown . This program expands on the current model that we went over in class. I&#39;ve retained a similar class structures, with minor edits to one method for blue scaling and added a new method to handle brightening the images. . Bluescaling . This methods expands on the original greyscaling (black and white) method. However, instead of calculating an average, I just set all other color channels to a value of 0, and maintain the original coloration of the blue channel. . Original: . average = (pixel[0] + pixel[1] + pixel[2]) // 3 ... image[&#39;gray_data&#39;].append((average, average, average, pixel[3])) . Modified: . blue_data.append((0, 0, pixel[2], pixel[3])) . Brightness . This method was also simple to create as we&#39;ve already established a baseline for converting an image into an np array of pixel rgb values. All I needed to do to increase the brightness was to take in a percentage from a user, and multiply each pixel value by said ratio to increase the overall brightness by the user specified values. . One thing that we did need to be careful of, was to take into account that the multipled value may be greater than 255, which is the maximum accepeted value for any RGB value. To do this, we would use the min() method to perform selection . # Create Brightened Base64 representation of Image def image_to_html_brightened(self): img_brightened = self._img numpy = np.array(self._img.getdata()) # PIL image to numpy array ratio = 1+int(input(&quot;How much do you want to scale the brightness by? (0-50%)&quot;))/100 # Convert user inputted number into a percentage, then add that onto 1 to get our multiplier assert ratio &lt; 1.5 brightened_data = [] for pixel in numpy: # Now we try to apply the multiplier to each color channel, we use the min() method here to avoid values greater than 255, the upper limit try: brightened_data.append((min(int(round(pixel[0]*ratio)), 255), min(int(round(pixel[1]*ratio)), 255), min(int(round(pixel[2]*ratio)), 255), pixel[3])) # PNG format except: brightened_data.append((min(int(round(pixel[0]*ratio)), 255), min(int(round(pixel[1]*ratio)), 255), min(int(round(pixel[2]*ratio)), 255))) # end for loop for pixels print(&quot;Image scaled by {0}%&quot;.format((ratio-1)*100)) img_brightened.putdata(brightened_data) return self.image_to_html(img_brightened) . from IPython.display import HTML, display from pathlib import Path # https://medium.com/@ageitgey/python-3-quick-tip-the-easy-way-to-deal-with-file-paths-on-windows-mac-and-linux-11a072b58d5f from PIL import Image as pilImage # as pilImage is used to avoid conflicts from PIL import ImageOps from io import BytesIO import base64 import numpy as np class Image_Data: def __init__(self, source, label, file, path, baseWidth=320): self._source = source # variables with self prefix become part of the object, self._label = label self._file = file self._filename = path / file # file with path self._baseWidth = baseWidth # Open image and scale to needs self._img = pilImage.open(self._filename) self._format = self._img.format self._mode = self._img.mode self._originalSize = self.img.size self.scale_image() self._html = self.image_to_html(self._img) self._html_brightened = self.image_to_html_brightened() self._html_blue = self.image_to_html_blue() @property def source(self): return self._source @property def label(self): return self._label @property def file(self): return self._file @property def filename(self): return self._filename @property def img(self): return self._img @property def format(self): return self._format @property def mode(self): return self._mode @property def originalSize(self): return self._originalSize @property def size(self): return self._img.size @property def html(self): return self._html @property def html_blue(self): return self._html_blue @property def html_red(self): return self._html_red @property def html_green(self): return self._html_green @property def html_brightened(self): return self._html_brightened # Large image scaled to baseWidth of 320 def scale_image(self): scalePercent = (self._baseWidth/float(self._img.size[0])) scaleHeight = int((float(self._img.size[1])*float(scalePercent))) scale = (self._baseWidth, scaleHeight) self._img = self._img.resize(scale) # PIL image converted to base64 def image_to_html(self, img): with BytesIO() as buffer: img.save(buffer, self._format) return &#39;&lt;img src=&quot;data:image/png;base64,%s&quot;&gt;&#39; % base64.b64encode(buffer.getvalue()).decode() # Create Brightened Base64 representation of Image def image_to_html_brightened(self): img_brightened = self._img numpy = np.array(self._img.getdata()) # PIL image to numpy array ratio = 1+int(input(&quot;How much do you want to scale the brightness by? (0-50%)&quot;))/100 # Convert user inputted number into a percentage, then add that onto 1 to get our multiplier assert ratio &lt; 1.5 brightened_data = [] for pixel in numpy: # Now we try to apply the multiplier to each color channel, we use the min() method here to avoid values greater than 255, the upper limit try: brightened_data.append((min(int(round(pixel[0]*ratio)), 255), min(int(round(pixel[1]*ratio)), 255), min(int(round(pixel[2]*ratio)), 255), pixel[3])) # PNG format except: brightened_data.append((min(int(round(pixel[0]*ratio)), 255), min(int(round(pixel[1]*ratio)), 255), min(int(round(pixel[2]*ratio)), 255))) # end for loop for pixels print(&quot;Image scaled by {0}%&quot;.format((ratio-1)*100)) img_brightened.putdata(brightened_data) return self.image_to_html(img_brightened) # Create Grey Scale Base64 representation of Image def image_to_html_blue(self): img_blue = self._img numpy = np.array(self._img.getdata()) # PIL image to numpy array blue_data = [] for pixel in numpy: # Same syntax as before, but we set all color channels to 0 try: blue_data.append((0, 0, pixel[2], pixel[3])) # PNG format except: blue_data.append((0, 0, pixel[2])) # end for loop for pixels img_blue.putdata(blue_data) return self.image_to_html(img_blue) def mirror(self): img = np.fliplr(np.array(self._img)) # PIL image to numpy array print(img) img_mirror = pilImage.fromarray(img) return self.image_to_html(img_mirror) # prepares a series of images, provides expectation for required contents def image_data(path=Path(&quot;images/&quot;), images=None): # path of static images is defaulted if images is None: # default image images = [ {&#39;source&#39;: &quot;Internet&quot;, &#39;label&#39;: &quot;Green Square&quot;, &#39;file&#39;: &quot;green-square-16.png&quot;}, {&#39;source&#39;: &quot;Peter Carolin&quot;, &#39;label&#39;: &quot;Clouds Impression&quot;, &#39;file&#39;: &quot;clouds-impression.png&quot;}, {&#39;source&#39;: &quot;Peter Carolin&quot;, &#39;label&#39;: &quot;Lassen Volcano&quot;, &#39;file&#39;: &quot;lassen-volcano.jpg&quot;} ] return path, images # turns data into objects def image_objects(): id_Objects = [] path, images = image_data() for image in images: id_Objects.append(Image_Data(source=image[&#39;source&#39;], label=image[&#39;label&#39;], file=image[&#39;file&#39;], path=path, )) return id_Objects # Jupyter Notebook Visualization of Images if __name__ == &quot;__main__&quot;: for ido in image_objects(): # ido is an Imaged Data Object print(&quot;- meta data --&quot;) print(ido.label) print(ido.source) print(ido.file) print(ido.format) print(ido.mode) print(&quot;Original size: &quot;, ido.originalSize) print(&quot;Scaled size: &quot;, ido.size) print(&quot;-- scaled image --&quot;) display(HTML(ido.html)) # print(&quot; red image &quot;) # display(HTML(ido.html_red)) print(&quot; blue image &quot;) display(HTML(ido.html_blue)) print(&quot; brightened image &quot;) display(HTML(ido.html_brightened)) # print(&quot; green image &quot;) # display(HTML(ido.html_green)) print() . Image scaled by 42.99999999999999% Image scaled by 33.00000000000001% Image scaled by 28.000000000000004% - meta data -- Green Square Internet green-square-16.png PNG RGBA Original size: (16, 16) Scaled size: (320, 320) -- scaled image -- . blue image . brightened image . - meta data -- Clouds Impression Peter Carolin clouds-impression.png PNG RGBA Original size: (320, 234) Scaled size: (320, 234) -- scaled image -- . blue image . brightened image . - meta data -- Lassen Volcano Peter Carolin lassen-volcano.jpg JPEG RGB Original size: (2792, 2094) Scaled size: (320, 240) -- scaled image -- . blue image . brightened image . . Lossy vs Lossless . Lossy Compression: A form of compression where data is usually lost or combined with other repeatitive patterns, resulting in an endproduct that is irreversible to produce the original item. . Lossless Compression: Opposite of Lossy compression. This form for compression could compress data with greater carefullness and present a end-product that could be scaled back up to the original item. However, this typically sees performance issues as efficiency is reduced compared to lossy compression . Example . . In the examples above, although the two images may look the same, the first image is lossless because of how it utilizes the SVG (Scalable Vector Graphics) format, where the image is generated by vector graphics (representation of an image through lines, curves an dots). As such, we can scale this image however we want, and our computer or browser will always render it without any loss of data, as the mathematical operations used to render the image remains the same . However, things change for the second image. The second image is a png file of a screenshot of the first image that I took in my browser. Because this image is stored in PNG (Portable Network Graphics) format, whenever we scale this image, we are bound to lose data as some pixels will merge in one way or another with others, resulting in lower graphics if we apply size compression on this image. .",
            "url": "https://ylu-1258.github.io/YLu-Blog/2023/03/07/APunit2.2-hacks.html",
            "relUrl": "/2023/03/07/APunit2.2-hacks.html",
            "date": " • Mar 7, 2023"
        }
        
    
  
    
        ,"post8": {
            "title": "Unit 2.2 Data Compression, Images",
            "content": "Enumerate &quot;Data&quot; Big Idea from College Board . Some of the big ideas and vocab that you observe, talk about it with a partner ... . &quot;Data compression is the reduction of the number of bits needed to represent data&quot; | &quot;Data compression is used to save transmission time and storage space.&quot; | &quot;lossy data can reduce data but the original data is not recovered&quot; | &quot;lossless data lets you restore and recover&quot; | . The Image Lab Project contains a plethora of College Board Unit 2 data concepts. Working with Images provides many opportunities for compression and analyzing size. . Image Files and Size . Here are some Images Files. Download these files, load them into images directory under _notebooks in your Blog. - Clouds Impression . Lassen Volcano | Green Square | . Describe some of the meta data and considerations when managing Image files. Describe how these relate to Data Compression ... . File Type, PNG and JPG are two types used in this lab Different types of image files use different types of compression methods to store pixel and color data. Some other examples of such files can be SVG (Scalable Vector Graphics), and WeBP (Web Picture Format) | . | Size, height and width, number of pixels These attributes determine how large our image is, greater dimensions (width and height) can lead to images that are orders of manitudes larger than normal images | . | Visual perception, lossy compression Sometimes, simple repeating patterns in files could be compressed and reduced in size to be rendered at a later date. | Lossy compression is a form of compression where some peices of data is lost in the process of conversion. This is most commonly used in video and picture formatting, where resolution could be decreased to speed up web rendering on platforms such as instagram or youtube | . | . | . Python Libraries and Concepts used for Jupyter and Files/Directories . Introduction to displaying images in Jupyter notebook . IPython . Support visualization of data in Jupyter notebooks. Visualization is specific to View, for the web visualization needs to be converted to HTML. . pathlib . File paths are different on Windows versus Mac and Linux. This can cause problems in a project as you work and deploy on different Operating Systems (OS&#39;s), pathlib is a solution to this problem. . What are commands you use in terminal to access files? In the linux terminal, we typically use a variety of commands to move aorund the file directory and access certain files. These are a few examples: - ls: Lists files cat: prints file content | cd: &quot;change directory&quot; | strings: Lists strings | . | Certain text editors could also be used in order to edit files: ed | nano | vim | gedit | etc. | . | . | What are the command you use in Windows terminal to access files? In windows, we also have similar operations that we can use to edit files dir: Lists directory and files | tree: Lists all subdirectories and files underneathe them including upper level files in the parent directory | cd: Changes Directory | type: Lists file contents | etc. | . | . | . What are some of the major differences? The major differences between these commands are that the syntax betwen them differs. Additionally, the code used to write these commands and binares also differs because of the fundamental differnece between the windows OS and the Linux OS. Provide what you observed, struggled with, or leaned while playing with this code. | . | Why is path a big deal when working with images? Path is extremely important when working with images because: Different computer OS and architectures may have file paths and structures, thus we must be careful such that our program is cross platform | Two types of paths exist: Absolute and relative pathing. Absolute paths requires us to know the full path of the working director and the application, while the relative paths cna be used to reference a file at a ceratin position in the file system regardless of the application location | | . | How does the meta data source and label relate to Unit 5 topics? A piece of information taught in Unit 5 was the idea of personal cybersecurity and data protection. SImilar to PII (Personal Identifiable Information), image metadata could contain pieces of info and data tthat are sensitive to an individual or group&#39;s safety or privacy, making them important to protect. | . | Look up IPython, describe why this is interesting in Jupyter Notebooks for both Pandas and Images? IPython is a python library created and designed to enhance the python interactive shell experience. However, the main reason why this is interesting and important for us while working with Jupyter notebooks and Pandas/Images is that it allows us to render certain pictures, diagrams, images, and datasets within a jupyter notebook, alowing us to have a richer experience when performing data science. | . | . from IPython.display import Image, display from pathlib import Path # https://medium.com/@ageitgey/python-3-quick-tip-the-easy-way-to-deal-with-file-paths-on-windows-mac-and-linux-11a072b58d5f # prepares a series of images def image_data(path=Path(&quot;images/&quot;), images=None): # path of static images is defaulted if images is None: # default image images = [ {&#39;source&#39;: &quot;Peter Carolin&quot;, &#39;label&#39;: &quot;Clouds Impression&quot;, &#39;file&#39;: &quot;clouds-impression.png&quot;}, {&#39;source&#39;: &quot;Peter Carolin&quot;, &#39;label&#39;: &quot;Lassen Volcano&quot;, &#39;file&#39;: &quot;lassen-volcano.jpg&quot;}, {&#39;source&#39;: &quot;The Internet&quot;, &#39;label&#39;:&quot;Smile&quot;, &#39;file&#39;: &quot;smile.jpg&quot;} ] for image in images: # File to open image[&#39;filename&#39;] = path / image[&#39;file&#39;] # file with path return images def image_display(images): for image in images: display(Image(filename=image[&#39;filename&#39;])) # Run this as standalone tester to see sample data printed in Jupyter terminal if __name__ == &quot;__main__&quot;: # print parameter supplied image green_square = image_data(images=[{&#39;source&#39;: &quot;Internet&quot;, &#39;label&#39;: &quot;Green Square&quot;, &#39;file&#39;: &quot;green-square-16.png&quot;}]) image_display(green_square) # display default images from image_data() default_images = image_data() image_display(default_images) . Reading and Encoding Images (2 implementations follow) . PIL (Python Image Library) . Pillow or PIL provides the ability to work with images in Python. Geeks for Geeks shows some ideas on working with images. . base64 . Image formats (JPG, PNG) are often called *Binary File formats, it is difficult to pass these over HTTP. Thus, base64 converts binary encoded data (8-bit, ASCII/Unicode) into a text encoded scheme (24 bits, 6-bit Base64 digits). Thus base64 is used to transport and embed binary images into textual assets such as HTML and CSS.- How is Base64 similar or different to Binary and Hexadecimal? . - **Base64 is similar to Binary and Hexidecimal in the way that it is another way to represent numerical data. Similar to how Binary operates on powers of two and hexidecimal operates on powers of 16, Base64 operates on powers of 64 instead, amking it useful for storing data with large magnitudes.** . Translate first 3 letters of your name to Base64. My name is Alexander, with the first 3 letters being &quot;Ale&quot; | . We can first convert our characters into a numerical representation with ASCII encoding, then into binary: A = 65 = 01000001 l = 108 = 01101100 e = 101 = 01100101 Ale = 65 108 101 = 01000001 01101100 01100101 | . | We combine all 3 bytes into one chunk, and split it into 6 bit portions, (as 64 = 2^6, hence why we reserve 6 bits): 010000 010110 110011 010101 | . | We can then convert all 4 chunks according to a table we find online to get the following result: 010000 -&gt; 16 (Q) 010110 -&gt; 22 (W) 110011 -&gt; 51 (x) 010101 -&gt; 21 (l) | . | Our result is: QWxl | | . numpy . Numpy is described as &quot;The fundamental package for scientific computing with Python&quot;. In the Image Lab, a Numpy array is created from the image data in order to simplify access and change to the RGB values of the pixels, converting pixels to grey scale. . io, BytesIO . Input and Output (I/O) is a fundamental of all Computer Programming. Input/output (I/O) buffering is a technique used to optimize I/O operations. In large quantities of data, how many frames of input the server currently has queued is the buffer. In this example, there is a very large picture that lags. . Where have you been a consumer of buffering? Some times when I watch youtube videos, I might experience some buffering problems when the next frames of the video are being rendered to prepare a smoother viewing experience. | . | From your consumer experience, what effects have you experienced from buffering? Typically the video will play with a &quot;studder&quot;, with occasional pauses and such to tender the future frames | . | How do these effects apply to images? Sometimes, when my browser tries to render a high definition image, it will render each pixel or row of pixels individually one at a time, until the full image has been rendered. | . | . Data Structures, Imperative Programming Style, and working with Images . Introduction to creating meta data and manipulating images. Look at each procedure and explain the the purpose and results of this program. Add any insights or challenges as you explored this program. . Does this code seem like a series of steps are being performed? Yes, there is a series of steps being performed in this code. From a cursory glance, the program includes functions to load up image paths, scaling images, and conversions to base 64. However, the core of the program lies in the image_management() and image_management_add_html_grey() functions | . | Describe Grey Scale algorithm in English or Pseudo code? For each pixel in the image, the algorithm will take the average of the values in each color channel for red, blue and green. By taking the average, the algorith now has a singular value to assign to the black and white channel to describe an image in terms of black and white. | . | Describe scale image? What is before and after on pixels in three images? The scaling of an image is done by calculating a ratio between the original image and a set base width. After the ratio is calculated, we would calculate a scale tuple to plug into the resize() method provided by the pillow library | . | Is scale image a type of compression? If so, line it up with College Board terms described? This scale image is a type of compression, as we are essentially melding the pixels with their neighbors to reduce the size of the image. In collegeboard terms, this would be known as lossy compression, as it would be nigh dificult to attempt to reconstruct the original color values of the image from an average. | . | . from IPython.display import HTML, display from pathlib import Path # https://medium.com/@ageitgey/python-3-quick-tip-the-easy-way-to-deal-with-file-paths-on-windows-mac-and-linux-11a072b58d5f from PIL import Image as pilImage # as pilImage is used to avoid conflicts from io import BytesIO import base64 import numpy as np # prepares a series of images def image_data(path=Path(&quot;images/&quot;), images=None): # path of static images is defaulted if images is None: # default image images = [ {&#39;source&#39;: &quot;Internet&quot;, &#39;label&#39;: &quot;Green Square&quot;, &#39;file&#39;: &quot;green-square-16.png&quot;}, {&#39;source&#39;: &quot;Peter Carolin&quot;, &#39;label&#39;: &quot;Clouds Impression&quot;, &#39;file&#39;: &quot;clouds-impression.png&quot;}, {&#39;source&#39;: &quot;Peter Carolin&quot;, &#39;label&#39;: &quot;Lassen Volcano&quot;, &#39;file&#39;: &quot;lassen-volcano.jpg&quot;} ] for image in images: # File to open image[&#39;filename&#39;] = path / image[&#39;file&#39;] # file with path return images # Large image scaled to baseWidth of 320 def scale_image(img): baseWidth = 320 scalePercent = (baseWidth/float(img.size[0])) scaleHeight = int((float(img.size[1])*float(scalePercent))) scale = (baseWidth, scaleHeight) return img.resize(scale) # PIL image converted to base64 def image_to_base64(img, format): with BytesIO() as buffer: img.save(buffer, format) return base64.b64encode(buffer.getvalue()).decode() # Set Properties of Image, Scale, and convert to Base64 def image_management(image): # path of static images is defaulted # Image open return PIL image object img = pilImage.open(image[&#39;filename&#39;]) # Python Image Library operations image[&#39;format&#39;] = img.format image[&#39;mode&#39;] = img.mode image[&#39;size&#39;] = img.size # Scale the Image img = scale_image(img) image[&#39;pil&#39;] = img image[&#39;scaled_size&#39;] = img.size # Scaled HTML image[&#39;html&#39;] = &#39;&lt;img src=&quot;data:image/png;base64,%s&quot;&gt;&#39; % image_to_base64(image[&#39;pil&#39;], image[&#39;format&#39;]) # Create Grey Scale Base64 representation of Image def image_management_add_html_grey(image): # Image open return PIL image object img = image[&#39;pil&#39;] format = image[&#39;format&#39;] img_data = img.getdata() # Reference https://www.geeksforgeeks.org/python-pil-image-getdata/ image[&#39;data&#39;] = np.array(img_data) # PIL image to numpy array image[&#39;gray_data&#39;] = [] # key/value for data converted to gray scale # &#39;data&#39; is a list of RGB data, the list is traversed and hex and binary lists are calculated and formatted for pixel in image[&#39;data&#39;]: # create gray scale of image, ref: https://www.geeksforgeeks.org/convert-a-numpy-array-to-an-image/ average = (pixel[0] + pixel[1] + pixel[2]) // 3 # average pixel values and use // for integer division if len(pixel) &gt; 3: image[&#39;gray_data&#39;].append((average, average, average, pixel[3])) # PNG format else: image[&#39;gray_data&#39;].append((average, average, average)) # end for loop for pixels img.putdata(image[&#39;gray_data&#39;]) image[&#39;html_grey&#39;] = &#39;&lt;img src=&quot;data:image/png;base64,%s&quot;&gt;&#39; % image_to_base64(img, format) # Jupyter Notebook Visualization of Images if __name__ == &quot;__main__&quot;: # Use numpy to concatenate two arrays images = image_data() # Display meta data, scaled view, and grey scale for each image for image in images: image_management(image) print(&quot;- meta data --&quot;) print(image[&#39;label&#39;]) print(image[&#39;source&#39;]) print(image[&#39;format&#39;]) print(image[&#39;mode&#39;]) print(&quot;Original size: &quot;, image[&#39;size&#39;]) print(&quot;Scaled size: &quot;, image[&#39;scaled_size&#39;]) print(&quot;-- original image --&quot;) display(HTML(image[&#39;html&#39;])) print(&quot; grey image -&quot;) image_management_add_html_grey(image) display(HTML(image[&#39;html_grey&#39;])) print() . - meta data -- Green Square Internet PNG RGBA Original size: (16, 16) Scaled size: (320, 320) -- original image -- . grey image - . - meta data -- Clouds Impression Peter Carolin PNG RGBA Original size: (320, 234) Scaled size: (320, 234) -- original image -- . grey image - . - meta data -- Lassen Volcano Peter Carolin JPEG RGB Original size: (2792, 2094) Scaled size: (320, 240) -- original image -- . grey image - . . Data Structures and OOP . Most data structures classes require Object Oriented Programming (OOP). Since this class is lined up with a College Course, OOP will be talked about often. Functionality in remainder of this Blog is the same as the prior implementation. Highlight some of the key difference you see between imperative and oop styles. . Read imperative and object-oriented programming on Wikipedia | Consider how data is organized in two examples, in relations to procedures | Look at Parameters in Imperative and Self in OOP | . Additionally, review all the imports in these three demos. Create a definition of their purpose, specifically these ... . PIL:This is the Pillow Library, which is mainly used for image manipulations and opertaions in Python. It allows us to edit and control our images in a way that would be best fit for our program- numpy: This is a library priarily used to represent numerical data in vectors. This library is especially useful as it allows us to perform vector and matrix operations on much of our data, and also grainting access to higher level operations such as cross and dot products that normal arrays don&#39;t support | base64: This is a character encoding format where binary bytes could be abstracted into a reprsentation of base-64 values and data. | . from IPython.display import HTML, display from pathlib import Path # https://medium.com/@ageitgey/python-3-quick-tip-the-easy-way-to-deal-with-file-paths-on-windows-mac-and-linux-11a072b58d5f from PIL import Image as pilImage # as pilImage is used to avoid conflicts from io import BytesIO import base64 import numpy as np class Image_Data: def __init__(self, source, label, file, path, baseWidth=320): self._source = source # variables with self prefix become part of the object, self._label = label self._file = file self._filename = path / file # file with path self._baseWidth = baseWidth # Open image and scale to needs self._img = pilImage.open(self._filename) self._format = self._img.format self._mode = self._img.mode self._originalSize = self.img.size self.scale_image() self._html = self.image_to_html(self._img) self._html_grey = self.image_to_html_grey() @property def source(self): return self._source @property def label(self): return self._label @property def file(self): return self._file @property def filename(self): return self._filename @property def img(self): return self._img @property def format(self): return self._format @property def mode(self): return self._mode @property def originalSize(self): return self._originalSize @property def size(self): return self._img.size @property def html(self): return self._html @property def html_grey(self): return self._html_grey # Large image scaled to baseWidth of 320 def scale_image(self): scalePercent = (self._baseWidth/float(self._img.size[0])) scaleHeight = int((float(self._img.size[1])*float(scalePercent))) scale = (self._baseWidth, scaleHeight) self._img = self._img.resize(scale) # PIL image converted to base64 def image_to_html(self, img): with BytesIO() as buffer: img.save(buffer, self._format) return &#39;&lt;img src=&quot;data:image/png;base64,%s&quot;&gt;&#39; % base64.b64encode(buffer.getvalue()).decode() # Create Grey Scale Base64 representation of Image def image_to_html_grey(self): img_grey = self._img numpy = np.array(self._img.getdata()) # PIL image to numpy array grey_data = [] # key/value for data converted to gray scale # &#39;data&#39; is a list of RGB data, the list is traversed and hex and binary lists are calculated and formatted for pixel in numpy: # create gray scale of image, ref: https://www.geeksforgeeks.org/convert-a-numpy-array-to-an-image/ average = (pixel[0] + pixel[1] + pixel[2]) // 3 # average pixel values and use // for integer division if len(pixel) &gt; 3: grey_data.append((average, average, average, pixel[3])) # PNG format else: grey_data.append((average, average, average)) # end for loop for pixels img_grey.putdata(grey_data) return self.image_to_html(img_grey) # prepares a series of images, provides expectation for required contents def image_data(path=Path(&quot;images/&quot;), images=None): # path of static images is defaulted if images is None: # default image images = [ {&#39;source&#39;: &quot;Internet&quot;, &#39;label&#39;: &quot;Green Square&quot;, &#39;file&#39;: &quot;green-square-16.png&quot;}, {&#39;source&#39;: &quot;Peter Carolin&quot;, &#39;label&#39;: &quot;Clouds Impression&quot;, &#39;file&#39;: &quot;clouds-impression.png&quot;}, {&#39;source&#39;: &quot;Peter Carolin&quot;, &#39;label&#39;: &quot;Lassen Volcano&quot;, &#39;file&#39;: &quot;lassen-volcano.jpg&quot;} ] return path, images # turns data into objects def image_objects(): id_Objects = [] path, images = image_data() for image in images: id_Objects.append(Image_Data(source=image[&#39;source&#39;], label=image[&#39;label&#39;], file=image[&#39;file&#39;], path=path, )) return id_Objects # Jupyter Notebook Visualization of Images if __name__ == &quot;__main__&quot;: for ido in image_objects(): # ido is an Imaged Data Object print(&quot;- meta data --&quot;) print(ido.label) print(ido.source) print(ido.file) print(ido.format) print(ido.mode) print(&quot;Original size: &quot;, ido.originalSize) print(&quot;Scaled size: &quot;, ido.size) print(&quot;-- scaled image --&quot;) display(HTML(ido.html)) print(&quot; grey image &quot;) display(HTML(ido.html_grey)) print() . Hacks . Early Seed award . Add this Blog to you own Blogging site. | In the Blog add a Happy Face image. | Have Happy Face Image open when Tech Talk starts, running on localhost. Don&#39;t tell anyone. Show to Teacher. | . AP Prep . In the Blog add notes and observations on each code cell that request an answer. | In blog add College Board practice problems for 2.3 | Choose 2 images, one that will more likely result in lossy data compression and one that is more likely to result in lossless data compression. Explain. | . Project Addition . If your project has images in it, try to implement an image change that has a purpose. (Ex. An item that has been sold out could become gray scale) | . Pick a programming paradigm and solve some of the following ... . Numpy, manipulating pixels. As opposed to Grey Scale treatment, pick a couple of other types like red scale, green scale, or blue scale. We want you to be manipulating pixels in the image. | Binary and Hexadecimal reports. Convert and produce pixels in binary and Hexadecimal and display. | Compression and Sizing of images. Look for insights into compression Lossy and Lossless. Look at PIL library and see if there are other things that can be done. | There are many effects you can do as well with PIL. Blur the image or write Meta Data on screen, aka Title, Author and Image size. | . from IPython.display import HTML, display from pathlib import Path # https://medium.com/@ageitgey/python-3-quick-tip-the-easy-way-to-deal-with-file-paths-on-windows-mac-and-linux-11a072b58d5f from PIL import Image as pilImage # as pilImage is used to avoid conflicts from PIL import ImageOps from io import BytesIO import base64 import numpy as np class Image_Data: def __init__(self, source, label, file, path, baseWidth=320): self._source = source # variables with self prefix become part of the object, self._label = label self._file = file self._filename = path / file # file with path self._baseWidth = baseWidth # Open image and scale to needs self._img = pilImage.open(self._filename) self._format = self._img.format self._mode = self._img.mode self._originalSize = self.img.size self.scale_image() self._html = self.image_to_html(self._img) self._html_brightened = self.image_to_html_brightened() self._html_blue = self.image_to_html_blue() @property def source(self): return self._source @property def label(self): return self._label @property def file(self): return self._file @property def filename(self): return self._filename @property def img(self): return self._img @property def format(self): return self._format @property def mode(self): return self._mode @property def originalSize(self): return self._originalSize @property def size(self): return self._img.size @property def html(self): return self._html @property def html_blue(self): return self._html_blue @property def html_red(self): return self._html_red @property def html_green(self): return self._html_green @property def html_brightened(self): return self._html_brightened # Large image scaled to baseWidth of 320 def scale_image(self): scalePercent = (self._baseWidth/float(self._img.size[0])) scaleHeight = int((float(self._img.size[1])*float(scalePercent))) scale = (self._baseWidth, scaleHeight) self._img = self._img.resize(scale) # PIL image converted to base64 def image_to_html(self, img): with BytesIO() as buffer: img.save(buffer, self._format) return &#39;&lt;img src=&quot;data:image/png;base64,%s&quot;&gt;&#39; % base64.b64encode(buffer.getvalue()).decode() # Create Grey Scale Base64 representation of Image def image_to_html_brightened(self): img_brightened = self._img numpy = np.array(self._img.getdata()) # PIL image to numpy array ratio = 1+int(input(&quot;How much do you want to scale the brightness by? (0-50%)&quot;))/100 brightened_data = [] for pixel in numpy: # Same syntax as before, but we set all color channels to 0 except the one that we want try: brightened_data.append((min(int(round(pixel[0]*ratio)), 255), min(int(round(pixel[1]*ratio)), 255), min(int(round(pixel[2]*ratio)), 255), pixel[3])) # PNG format except: brightened_data.append((min(int(round(pixel[0]*ratio)), 255), min(int(round(pixel[1]*ratio)), 255), min(int(round(pixel[2]*ratio)), 255))) # end for loop for pixels print(&quot;Image scaled by {0}%&quot;.format(ratio)) img_brightened.putdata(brightened_data) return self.image_to_html(img_brightened) # Create Grey Scale Base64 representation of Image def image_to_html_blue(self): img_blue = self._img numpy = np.array(self._img.getdata()) # PIL image to numpy array blue_data = [] for pixel in numpy: # Same syntax as before, but we set all color channels to 0 try: blue_data.append((0, 0, pixel[2], pixel[3])) # PNG format except: blue_data.append((0, 0, pixel[2])) # end for loop for pixels img_blue.putdata(blue_data) return self.image_to_html(img_blue) def mirror(self): img = np.fliplr(np.array(self._img)) # PIL image to numpy array print(img) img_mirror = pilImage.fromarray(img) return self.image_to_html(img_mirror) # prepares a series of images, provides expectation for required contents def image_data(path=Path(&quot;images/&quot;), images=None): # path of static images is defaulted if images is None: # default image images = [ {&#39;source&#39;: &quot;Internet&quot;, &#39;label&#39;: &quot;Green Square&quot;, &#39;file&#39;: &quot;green-square-16.png&quot;}, {&#39;source&#39;: &quot;Peter Carolin&quot;, &#39;label&#39;: &quot;Clouds Impression&quot;, &#39;file&#39;: &quot;clouds-impression.png&quot;}, {&#39;source&#39;: &quot;Peter Carolin&quot;, &#39;label&#39;: &quot;Lassen Volcano&quot;, &#39;file&#39;: &quot;lassen-volcano.jpg&quot;} ] return path, images # turns data into objects def image_objects(): id_Objects = [] path, images = image_data() for image in images: id_Objects.append(Image_Data(source=image[&#39;source&#39;], label=image[&#39;label&#39;], file=image[&#39;file&#39;], path=path, )) return id_Objects # Jupyter Notebook Visualization of Images if __name__ == &quot;__main__&quot;: for ido in image_objects(): # ido is an Imaged Data Object print(&quot;- meta data --&quot;) print(ido.label) print(ido.source) print(ido.file) print(ido.format) print(ido.mode) print(&quot;Original size: &quot;, ido.originalSize) print(&quot;Scaled size: &quot;, ido.size) print(&quot;-- scaled image --&quot;) display(HTML(ido.html)) print(&quot; blue image &quot;) display(HTML(ido.html_blue)) print(&quot; brightened image &quot;) display(HTML(ido.html_brightened)) print() . Image scaled by 1.25% Image scaled by 1.25% Image scaled by 1.25% - meta data -- Green Square Internet green-square-16.png PNG RGBA Original size: (16, 16) Scaled size: (320, 320) -- scaled image -- . blue image . brightened image . - meta data -- Clouds Impression Peter Carolin clouds-impression.png PNG RGBA Original size: (320, 234) Scaled size: (320, 234) -- scaled image -- . blue image . brightened image . - meta data -- Lassen Volcano Peter Carolin lassen-volcano.jpg JPEG RGB Original size: (2792, 2094) Scaled size: (320, 240) -- scaled image -- . blue image . brightened image . .",
            "url": "https://ylu-1258.github.io/YLu-Blog/2023/03/07/AP-unit2-2.html",
            "relUrl": "/2023/03/07/AP-unit2-2.html",
            "date": " • Mar 7, 2023"
        }
        
    
  
    
        ,"post9": {
            "title": "Tri 2 MCQ Reflections",
            "content": "Quiz Overview . Overall I scored a 48/50 on this MCQ, which isn’t a bad score. However, there are definitely still more places where I can improve and reinforce my knowledge for the class. . . Question 5 . . On this problem, I incorrectly responded with choice A, when the actual correct answer was B. The main issue with me answering this problem wrong was not because of my knowledge, but simply because I was too careless in answering the question. Reading the question, I missed the part where it asked for the LEAST likely benefit offered by the newer implementation, which led me selecting the first option that seemed to offer benefit to the system as a whole. Knowing this, I will need to be careful in future tests and exams in ensuring that I read each question and choice carefully before I make my final judgment for the problem. . Question 31 . . On this problem, I made the same mistake as I did on question 5, where I didn’t read the answer choices close enough. The answer I chose, C, was very close to the correct answer, D. The only difference between the two answers was in C, where it checks if the sensor is not activiated, while response D indicates to check if the sensor IS activated. In order for the door system to work appropriately accordoing to it’s requested function and purpose, we must check if the sensor IS activated, making D right, and C wrong. .",
            "url": "https://ylu-1258.github.io/YLu-Blog/markdown/apcsp/tri2/2023/03/06/MCQ-Tri2-reflections.html",
            "relUrl": "/markdown/apcsp/tri2/2023/03/06/MCQ-Tri2-reflections.html",
            "date": " • Mar 6, 2023"
        }
        
    
  
    
        ,"post10": {
            "title": "Collegeboard rubric check",
            "content": "Rubric . Category My Score Explaintation . Program Function and Purpose | 1/1 | The overall function of the program is to provide students with a quick and efficient way of calculating, comparing and testing certain gpa combinations and outcomes. The overall purpose is to allow the student to gain a better awareness of their academic performance in relation to other students registered on the platform. | . Data Abstraction | 1/1 | One method of abstraction used is a database which I used to store user entries along with their relevant information. Examples of data stored can be the user&#39;s username, password, user id, and number of each grade point they&#39;ve achieved. | . Managing Complexity | 1/1 | My program manages the overall complexity in the frontend by creating an array that the program continuously updates with data from the backend. This list is then used to perform certain operations such as sorting and searching in order to provide an effcient way for comparison. | . Procedural Abstraction | 1/1 | The program uses many javascript and python functions in bot hthe frontend and backend respectively to carry out the operations. Notably, some functions used in the backend could be part of the model&#39;s class, where it defines databse operations, as well as fetch methods in the frontend that serve to connect the frontend with the backend application. | . Algorithm Implementation | 1/1 | The program uses a quicksort algorithm in order to iterate and sort the list that is generated in order to transform it into a form that is more easily traversible to extract necessary data such as student position, and ranking. | . Testing | 1/1 | The program includes an authentication system that properly returns different outcomes. For instance, inputting an incorrect username/password combination would result in the application being unable to authenticate the user and making changes. On the other hand, if a valid match has been found, the application will then authenticate the user and grant them access to their data. | . TOTAL: 6/6 points .",
            "url": "https://ylu-1258.github.io/YLu-Blog/markdown/apcsp/tri2/2023/02/27/collegeboard-reqs.html",
            "relUrl": "/markdown/apcsp/tri2/2023/02/27/collegeboard-reqs.html",
            "date": " • Feb 27, 2023"
        }
        
    
  
    
        ,"post11": {
            "title": "Night at the museum overview",
            "content": "My N@TM Experience . The night at the museum event was definitely a fun activity for this tri. I was able to witness other group’s projects and designs and were able to see my friend’s improvements over time. This time around, everyone else were all more professional and experienced in the field of computer science, and the systems that many of us designed were more intricate and complicated. Moreover, I personally also felt more experienced and involved this time around because my skills also improved from last tri. This tri, I could explain my projects and algorithms much better than what I could’ve done last tri, and was able to properly explain the project to parents and other students alike. We also got various praises and constructive criticism from parents and students alike, making the experience worthwhile yet enjoyable.. . First Project: . The first project that I saw was a country guessing game, which provided the user with a wide array of hints including flags and such and asks them to guess up to 18 countries. The project overall was well designed as all aspects worked properly. Moreover, the project was an interesting take on the popular wordle game, as it went outside of the original scope of the game’s design and attempted to try out something new. . Second Project: . The second project that I saw was Drew’s group’s purrdle game. The overaching project had a nice theme to it, being a website designed for a cat cafe business. The overall theme was very nice and the frontend had a great deal of work and effort put into it. The overall project works, having both frontend and backend functional and able to connect with one another. Overall, the program worked well, and was able to met most of the requirements. . Third Project: . The third project that I saw was a wordle game. Although the idea is unoriginal, the overall website works fine, and the frontend has well designed CSS to mimic the original game. The project is functional overall, and both the frontend and backend were able to connect and remain up througout the duration of the event. .",
            "url": "https://ylu-1258.github.io/YLu-Blog/markdown/apcsp/tri2/2023/02/27/Night-at-the-museum.html",
            "relUrl": "/markdown/apcsp/tri2/2023/02/27/Night-at-the-museum.html",
            "date": " • Feb 27, 2023"
        }
        
    
  
    
        ,"post12": {
            "title": "Big topic 5.5 and 5.6",
            "content": "Big Idea 5.5 Legal and Ethical Concerns . Types of Licences . There are two main types of Licenses that one can choose to use: . Open Source Licences: Allow for other people to view, access, and download the source code for a program. permissions may vary amongst some licenses but overalll idea would be the same. Some Open source licenses allow for closed source distribution, others do not. | Closed Source Licences: Does not allow for other people to access a project or program’s source code. Typically the marketer of such a program will release a closed version to the public, and keep the source code for their own development. | Examples of Licenses . Creative Commons Zero v1.0 Universal: Grants most unrestricted usage and rights, for the public domain and the author waives all copyrights to the product. | Open Source MIT License: Allows other consumers to freely use the code produced and also release closed source versions. However, in these products, people commonly would be required by the License to give credit to the original author. | Open Source GPL License: Allows for viewing and access and most other rights to a project, with the exception of releasing or distributing a closed sourced version of it | Personal Summary: . As an individual, I believe that protecting a user’s intellectual rights to something that they’ve made is highly important in ensuring that the right people receive credit for their works. Thus, licenses are really important tools to outline the rights of the creator, as well as the rights of the user to use and access code made by someone else. | Because of our projected-based learning curriculum, I feel like students should be able to make their projects open-sourced so that other students can view and see what particular edits or changes may be added to address a specfic issue. Moreover, these licenses should also restrict the usage rights of other students, such that they can still view the source code, but not copy it directly and release it for their own work. | Because many large companies distribute their products amongst a userbase, it is important to create a license to outline the consumer rights of their product, such that they can stil protect their patent rights and earn money. | . Team Selection: . As a team, we decided to pick the GNU GPLv3 License for our project. This is because the license not only allows other students to access our project if they need help, but it also prevents them from directly copying or stealing the code for themselves. We determined that this would thus be the best license type to use for such a school based project. . Big Idea 5.6 Legal and Ethical Concerns . Describe PII you have seen on project in CompSci Principles. . Some notable Personal Identifiable Information that I’ve seen in the class are forms gathering data about name and email. Typically, such information isn’t too sensitive, as there can be many people with the same name, and the email address is meant to be shared publically for communication. However, there are more sensitive forms of data stored, such as address and date of birth on some other projects. . What are your feelings about PII and your personal exposure? . I believe that PII’s are an important aspect of our internet activities. Most of our web services rely on such PII’s for providing us with the best quality and experience. However, we should also be careful with what we do with PII on the internet. Because some of these data can be identifiable and are highly private to an individual, it would be problematic if such details leaked out into the hands of an attacker. . Describe good and bad passwords? What is another step that is used to assist in authentication. . A good password typically has met certain criterias that make it harder to brute force, such as a set minimum password length (typically 15), a mix of numbers and letters, mixed-cases for alphabetical characters, as well as other special symbols. A bad password would be one that is easily brute-forced or dictionary cracked, because of how commonly used it is or because of the lack of rules imposed on it. There are other certain methdos to help with autneitcation, one being encryption, which could help us to mask and store passwords safely, and the other can be 2FA (Two Factor Authentication), which would require users to verify using another device before granting access. . Try to describe Symmetric and Asymmetric encryption. . In Symmetric encryption, only one secret key is used to encryot and decrypt encrypted content. However, asymmetric encryption may utilize a pair of related keys, known as public and private keys, in order to encrypt and decrypt a given piece of information. Thus by virtue, asymmetric encryptio is commonly more secure than symmetric encryption. . Provide an example of encryption we used in AWS deployment. . An example of an encruption that we used in AWS deployment is SSL encryption, which uses a mix of both symmetric and asymmetric encryption methods. We have also previously generated RSA keys too for our SSH deployment keys when we set up the project. . Describe a phishing scheme you have learned about the hard way. Describe some other phishing techniques. . While I personally haven’t been phished or scammed before on the internet, there are a multitude of other methods that people may use to phish others. For instance, a common way of phishing is to send an email disguised to look like an official email from a company, asking for money or a fee for a service or other business related task. For such instances, the emails are typically misspelled, carrying many other mistkaes within it’s subject and body that could be noticed amongst closer inspection. These emails often link to other sites and such that can eventually track and store user inputted data such as paswords and card info, to ultimately scam the user. .",
            "url": "https://ylu-1258.github.io/YLu-Blog/markdown/apcsp/tri2/2023/02/12/Licenses-and-concerns.html",
            "relUrl": "/markdown/apcsp/tri2/2023/02/12/Licenses-and-concerns.html",
            "date": " • Feb 12, 2023"
        }
        
    
  
    
        ,"post13": {
            "title": "Computing Bias",
            "content": "Big Idea 5.3 Computing Bias . Discussion 1: . Google “What age groups use Facebook” vs “… TikTok”? What does the data say? Is there purposeful exclusion in these platforms? Is it harmful? Should it be corrected? Is it good business? . There is a clear distinction between the age gap of Tiktok versus facebook users. The age gap shows that a majority of face book users are above the age of 25, while the majority of tiktok useres are below the age of twentity-five. Although there isn’t purposeful exclusion within these platforms, it’s obvious that both platforms advertise and appeal to a different set of the population. I do not believe that this is inherently harmful to society as both demographics still are able to acquire the platform that they wish to speak on. On the otherhand, having such an approach is certainly one viable business model, as it allows for better targeting and curation of a particular group’s demands. . Discussion 2: . Why do virtual assistants have female voices? Amazon, Alexa Google, Apple Siri. Was this purposeful? Is it harmful? Should it be corrected? Is it good business? . I feel like most virtual assistants have female voices because of a trend that started out with the first few assistents, such as Apple siri or amazon Alexa. However, although one can argue that such a decision could result in the perpetuation of the idea that women should be in a subservient position, (such as acting as secretary figures), I do not believe that it is as large as a problem now, especially as many companies are producing more assistants with different voices (some including male ones) . Discussion 3: . Talk about an algorithm that influences your decisions, think about these companies (ie FAANG - Facebook, Amazon, Apple,Netflix, Google) . One large algorithm that influences my decisions is the google search engine. The search engine relies on a complicated network of big data and machine learning algorithms in order to quickly and effectively return a set of website links for a certain query that I enter. Thus, depending on the output of this algorithm, it could very well change the information that I consume or learn on the internet. Moreover, such algorithms could also impact our experience of internet entertainment. For example, the netflix platform, which is commonly used for streaming movies and TV shows, uses a similar idea where it implements are reccomendation engine or something of the sort to reccomend shows to users based on the current trends or the watch history of the user themselves. . As pairs . Come up with some thoughts on the video and be ready to discuss them as I call on you. Here are some ideas… . Does the owner of the computer think this was intentional? I do not believe that the owner of the computer thought that this was intentional. | If yes or no, justify you conclusion. The overall video had a pretty humourous tone, and it didn’t seem like the owner was really agitated in any way. However, had he been more concerned with the quality of the product, he likely would’ve brought it up in a more official way, by filling out a ticket or sending an email to customer support directly. | How do you think this happened? I think that this likely happened because of a lack of training data for the computer to work with, as if the computer is only trained to recognize people or lighter skin tones, it might not necessarily work as well for people of other skin tones. | Is this harmful? Was it intended to be harmful or exclude? Such mistakes are certainly harmful, althought their might not be intended to be harmful or to exclude. These mistakes allow for an entire demographic to miss out on a particular technology or trend. | Should it be corrected? Yes, I believe that this issue should be corrected in order to incoporate and embrace a much larger group of consumers | What would you or should you do to produce a better outcome? I believe that if the company could use more training data (i.e. getting more people from different races and skin tones to test out the computer), performance would gradually increase to include other people as well. | Summary: . Computing Bias could have great implications in our world, weather it be in our day to day lives, or in the commercial/business realm. For instance, the Hewlett-Packard motion assisted camera demonstrated how a narrowed minded training set could neglect other forms of data and consumers, such as those with different skin tones or clothing. This is why testing our algorithms are so important, because it allows us to identify for possible biases in our programs to detect wether or not a particular type of people are being excluded from using the application. Thus, I should be careful to test my algorithms in the CPT project such that it treats all users fairly and equally. . Big Idea 5.4 Crowdsourcing . Discusssion 1: . CompSci has 150 ish principles students. Describe a crowdsource idea and how you might initiate it in our environment? . One example of a crowdsource idea that I can use is to have people enter data on a website frontend that I create, and eventually store the data in the backend. An example of this could be a program used to compare the school schedules of two users, which would be impossible without user input, but extremely efficient for students to compare with other students. Thus, if everyone can contribute their schedule to such a program, it would become an incredibly effective and efficient tool for all students. . Discussion 2: . What about Del Norte crowdsourcing? Could your project be better with crowdsourcing? . The project can certainly be improved with a a more direct approach to Del Norte crowdsourcing. For example, instead of only focusing on the AP CSP class, we can add in other classes from around campus too, this would overall open our project up to a greater range of people and ultimately imrpove the overall quality of the pgoram as users now can have access to a greater amount of information as to who is in their classes. . Discussion 3: . What kind of data could you capture at N@tM to make evening interesting? Perhaps use this data to impress Teachers during finals week. . At N@TM, I can capture specific data related to the individual schedules of each student in order to gradually build up the userbase of the program. By storing these data into a backend database, we can keep it persistently running at all times, so that more and more people could eventually add to and contribute to the program functionality as a whole. This would improve the overall quality of hte pgoram as it would then have a broader range of an audience, as well as more potential data to display. . Big Idea 5.5 Legal and Ethical Concerns . Types of Licences . There are two main types of Licenses that one can choose to use: . Open Source Licences: Allow for other people to view, access, and download the source code for a program. permissions may vary amongst some licenses but overalll idea would be the same. Some Open source licenses allow for closed source distribution, others do not. | Closed Source Licences: Does not allow for other people to access a project or program’s source code. Typically the marketer of such a program will release a closed version to the public, and keep the source code for their own development. | Examples of Licenses . Creative Commons Zero v1.0 Universal: Grants most unrestricted usage and rights, for the public domain and the author waives all copyrights to the product. | Open Source MIT License: Allows other consumers to freely use the code produced and also release closed source versions. However, in these products, people commonly would be required by the License to give credit to the original author. | Open Source GPL License: Allows for viewing and access and most other rights to a project, with the exception of releasing or distributing a closed sourced version of it | Personal Summary: . As an individual, I believe that protecting a user’s intellectual rights to something that they’ve made is highly important in ensuring that the right people receive credit for their works. Thus, licenses are really important tools to outline the rights of the creator, as well as the rights of the user to use and access code made by someone else. | Because of our projected-based learning curriculum, I feel like students should be able to make their projects open-sourced so that other students can view and see what particular edits or changes may be added to address a specfic issue. Moreover, these licenses should also restrict the usage rights of other students, such that they can still view the source code, but not copy it directly and release it for their own work. | Because many large companies distribute their products amongst a userbase, it is important to create a license to outline the consumer rights of their product, such that they can stil protect their patent rights and earn money. | . Team Selection: . As a team, we decided to pick the GNU GPLv3 License for our project. This is because the license not only allows other students to access our project if they need help, but it also prevents them from directly copying or stealing the code for themselves. We determined that this would thus be the best license type to use for such a school based project. . Big Idea 5.6 Legal and Ethical Concerns . Describe PII you have seen on project in CompSci Principles. . Some notable Personal Identifiable Information that I’ve seen in the class are forms gathering data about name and email. Typically, such information isn’t too sensitive, as there can be many people with the same name, and the email address is meant to be shared publically for communication. However, there are more sensitive forms of data stored, such as address and date of birth on some other projects. . What are your feelings about PII and your personal exposure? . I believe that PII’s are an important aspect of our internet activities. Most of our web services rely on such PII’s for providing us with the best quality and experience. However, we should also be careful with what we do with PII on the internet. Because some of these data can be identifiable and are highly private to an individual, it would be problematic if such details leaked out into the hands of an attacker. . Describe good and bad passwords? What is another step that is used to assist in authentication. . A good password typically has met certain criterias that make it harder to brute force, such as a set minimum password length (typically 15), a mix of numbers and letters, mixed-cases for alphabetical characters, as well as other special symbols. A bad password would be one that is easily brute-forced or dictionary cracked, because of how commonly used it is or because of the lack of rules imposed on it. There are other certain methdos to help with autneitcation, one being encryption, which could help us to mask and store passwords safely, and the other can be 2FA (Two Factor Authentication), which would require users to verify using another device before granting access. . Try to describe Symmetric and Asymmetric encryption. . In Symmetric encryption, only one secret key is used to encryot and decrypt encrypted content. However, asymmetric encryption may utilize a pair of related keys, known as public and private keys, in order to encrypt and decrypt a given piece of information. Thus by virtue, asymmetric encryptio is commonly more secure than symmetric encryption. . Provide an example of encryption we used in AWS deployment. . An example of an encruption that we used in AWS deployment is SSL encryption, which uses a mix of both symmetric and asymmetric encryption methods. We have also previously generated RSA keys too for our SSH deployment keys when we set up the project. . Describe a phishing scheme you have learned about the hard way. Describe some other phishing techniques. . While I personally haven’t been phished or scammed before on the internet, there are a multitude of other methods that people may use to phish others. For instance, a common way of phishing is to send an email disguised to look like an official email from a company, asking for money or a fee for a service or other business related task. For such instances, the emails are typically misspelled, carrying many other mistkaes within it’s subject and body that could be noticed amongst closer inspection. These emails often link to other sites and such that can eventually track and store user inputted data such as paswords and card info, to ultimately scam the user. .",
            "url": "https://ylu-1258.github.io/YLu-Blog/markdown/apcsp/tri2/2023/02/01/computing-bias.html",
            "relUrl": "/markdown/apcsp/tri2/2023/02/01/computing-bias.html",
            "date": " • Feb 1, 2023"
        }
        
    
  
    
        ,"post14": {
            "title": "MCQ Reflections",
            "content": "Quiz Overview . Overall I scored a 48/50 on this MCQ, which isn’t a bad score. However, there are definitely still more places where I can improve and reinforce my knowledge for the class. . . Question 30 . . On this problem, I incorrectly responded with B and D, when the actual correct answer was B and C. The main issue with me answering this problem wrong was not because of my knowledge, but simply because I was too careless in answering the last few questions. Option D is wrong because although it starts out with the right center coordinates, it starts with an initial radius of 3, but quickly decrements it to 2 for the first circle, which would ultimately scale down the true sizes of the circles. A better option would be choice C, which uses almost the same sequencing, but prints out the circle first before decrementing, which would result in the right response . Question 33 . . On this problem, I failed to recognize that the possible input to the program could be negative. By excluding negative values, I made the erroneous assumption that the given inputs will always be positive numbers, hence always greater than -1, and overwriting the intial value. However, if all inputs were negative values smaller than -1, the first program would return the wrong value as -1 would be greater than all other inpout values, and thus never updated. Thus in this scenario, the program will output -1, the initial condition, instead of the true maximum. I was correct to assume that the other algorithm would work well. .",
            "url": "https://ylu-1258.github.io/YLu-Blog/markdown/apcsp/tri2/2023/01/30/MCQ-reflections.html",
            "relUrl": "/markdown/apcsp/tri2/2023/01/30/MCQ-reflections.html",
            "date": " • Jan 30, 2023"
        }
        
    
  
    
        ,"post15": {
            "title": "Individual project Overview",
            "content": "The Goal: . The end goal of the collegeboard create performance tasks is to challenge students to write a program and expertly explain a wide range aspects convered by the program in complexity management, purpose and functionality, and algorithm implementation. Due to this, we have begun a team project in hopes of practicing such skills. I personally work on one small program of our entire project, but within this sub program still contains all of the required collegeboard aspects. . The Idea: . For our poject, we are aiming to create a student toolkit website that allows them to compare grades, schedules, and plan their academic activities throughout the day. Although the project seems simple on the surface, there are a lot of intricacies that one might have to consider when approaching such a task. For instance, a crucial part of the project would be managing the overall complexity of the data and the method though which it is stored. For persistent data, we will be using a SQLite3 database to keep track of the user data in the backend, while temporary computational data could be stored in arrays and other abstract data structures (heap, queue, stacks, trees, etc.) for better access and efficiency. There is also the case of computing algorithms that we need to consider, which would be apparent with a larger userbase. We might need to branch out from the naive method and look for certain algorithmic advantages that are present within certain design models but not within others. . My portion: . My portion of the project is a GPA comparison app. Named “MyGPA”, the program serves to reduce the difficulty of calculating one’s GPA, as well as providing them with the necessary tools to compute their admissions GPAs for other schools. Another purpose of the program is also to provide an ability for students to compare their Overall GPAs with other students at their schools, through taking the average GPA value of their classes and grade respectively. Such a program would greatly increase the average student’s awareness of their academic strength. . My Write up: . In my write-up, I will first reivew the program’s purpose, function and inputs, which is help students gauge their academic prowess, while also serving to help them calcualte their GPA through their inputted grades and such. I will additionally go over the form of data abstraction that I used in the program which would likely be an array or dictionary converted from database schema. I will then explain how such a model serves to reduce overall data complexity within my program. Needless to say, I will also consider the algorithmic implementation of my program and how I could potentially speed up the calculation process for average GPAs, especially when there are millions of potential students and users. To achieve this approach, I might use a dynammic programming model to store past results and upate them accordingly. Finally, I will also be able to test out my program outputs by entering different sets of data, which could be to calculate different types of GPAs and etc. . In the video, I’ll be clear to show how the inputs and outputs to my program work together, and how the student is able to compare their GPA to others around them. .",
            "url": "https://ylu-1258.github.io/YLu-Blog/markdown/apcsp/tri2/2023/01/30/Individual-project.html",
            "relUrl": "/markdown/apcsp/tri2/2023/01/30/Individual-project.html",
            "date": " • Jan 30, 2023"
        }
        
    
  
    
        ,"post16": {
            "title": "Digital Divides and Effects",
            "content": "Big Idea 5.1 Beneficial and Harmful Effects: . Come up with three of your own Beneficial and corresponding Harmful Effects of Computing. . Benefits: One large benefit of computing is it’s ability to make many simple calculations in a fraction of a second. This calculative nature of computers allows us to break down complex operations into simple, individual instructions for the computer to read and interpret, with these solutions eventually combining to generate an answer to a larger problem. Furthermore, the instanticity offered by the internet allows for the average person to quickly search and gain results to a particular question. Finally, computing also rpvoides us with ultra fast methods of communicatin, through online SMS spaces and video calling. These benefits ultimately serve to link up the global infrastructure and speed up collaboration. . Harms: Computing, especially in the digital world, could lead to very serious issues concerning morals and ethics. For instance, most of the internet is driven by large collections of data, that provide the user with the information that they want. Every functional program and service includes some storage of user data or input that can be nearly permenantly stored online. As such, the moment one posts something personal or undesireable to the online space, it becomes nigh difficult or nearly impossible to remove it from the internet. Furthermore, the issue of data privacy could also extend to unauthorized or unconsensual data collection. For instance, much of social media’s reccomendations to the aberage user are based on their private searches and online history, also with their own inputted interests and information. Such information could then be traded and sold online for money, which exposes one’s personal data to the world. Lastly, computing also has serious environmental effects especially due to the amount of raw materials and pollution it can cause. Chip productions in particular, require vast amounts of different metals and minerals, that all contribute to global pollution and the destruction of ecosystems. . Talk about dopamine issues. Real? Parent conspiracy? Anything that is impacting your personal study and success in High School? . The dopamine issues due to the internet is definitely real. Social media and video games are an incredibly dangerous in that they are highly addictive and are devastating to a student who has a packed schedule. Personally, I experienced this in middle school, where I was extremely addicted to video games and neglected y studies. The effects soon showed up in my grades, and I was forced to change and reevalutae my habits to improve my experiences in school. Now, my grades are much better than they were before, and despite the fact that I still use social media and play games, I’ve learned to restrict myself and place my student and family duties above my personal enjoyment in the online world. . Big Idea 5.2 Digital Divide . How does someone empower themself in a digital world? . Someone cam empower themselves in a digital world by sposting their creations and achivements online to share with others. This is typically a good way to show off one’s skills to the outside world, but it can also be harmful in that there is a select minority on the internet that will attempt to put down others for their self-gratification. . How does someone that is empowered help someone that is not empowered? Describe something you could do at Del Norte HS. . An empowered individual typically helps others who are not empowered by providing them some form of physical or mental help. For example, my friends are I are able to empower each other in out rigourous academic lives by sharing the knowledge that we learn from class as well as what we learn from out own time. Thorugh this, even if we struggle in school and might feel discouraged, we still would have a safety net that could help us at our lowest an ensure a good academic outcome or result. . Is paper or red tape blocking digital empowerment? Are there such barriers at Del Norte? Elsewhere? . Paper and red tape blocking are not forms of digital empowerment and instead serve to further restrict an individual. In some cases, forcing students to use a specific medium (whether it be on paper or digitally), could result in an additional headache for the student, as they may not hve the necessary materials at hand to complete the tasks, resulting in them unable to complete their assignments on time. Furthermore, having a red tape process or a fixed process of completing tasks would result in an overall decrease in efficiency, both in a school and work scenario. This is because forcing workers and students to complete a task a certain way neglects the other more effective and efficient methods that one could’ve used otherwise to achieve the same goal, just in better time and quality. .",
            "url": "https://ylu-1258.github.io/YLu-Blog/markdown/apcsp/tri2/2023/01/24/digital-divides-and-effects.html",
            "relUrl": "/markdown/apcsp/tri2/2023/01/24/digital-divides-and-effects.html",
            "date": " • Jan 24, 2023"
        }
        
    
  
    
        ,"post17": {
            "title": "Collegeboard Grading part 3",
            "content": "Cross-grading of submission 1 . The theme of this submission was to create a program simulates a rock-paper-sicsors game . . Analysis: . Row Score Explaintation Notes/Discrepancies . Program Function and Purpose | 0/1 | The video response does a good job of showing the input, functionality, and output of the program, which prompted the user for either rock, scissors, or paper. However, only the purpose of the program was stated in the written response, and the function of the program was neglected. | Apparently, the response was detailed enough to include both the function and the purpose. | . Data Abstraction | 0/1 | The program and response does include the usage of a list, it&#39;s name (RPS), and the purpose it serves (seving as random set of input for the computer), but the does not show how the data within the list is being used in the program. | | . Managing Complexity | 0/1 | Although the response explains why a list is used within the program, it does not indicate why the usage of the list would serve to reduce overall complexity in the program. Instead the response only gives a brief overview of what can be used in place of a list, and not how that particular replacement is not as optimal as the current usage of a list. | | . Procedural Abstraction | 1/1 | The written reponse indicates a function that takes in an user input and compares it with a computer move thorugh the rpsGame function. This function serves to output different results based on the computer and user inputs.. | The collegeboard grader said that the written response did not serve to explain how the particular procedure helped to contribute to the overall functionality of the program,m and was way too vague in stating how it allowed for the program to execute smoothly. | . Algorithm Implementation | 1/1 | The algorithm indicates the proper use of a recursive function call to implement iteration and a logical sequence of code to demontrate sequencing. Moreover, the algorithm also uses a if statements to implement selection in the code to determine if the player won against the computer or not. | | . Testing | 1/1 | The written response clearly shows the inputting of two different guesses, one comparing the user&#39;s guess of rock with the computer&#39;s guess of paper, and another one that was the opposite of the former inputs. Moreover, the response clearly details why each response gave their resutls and also properly identified each respective result of the program. | | . Cross-grading of submission 2 . The theme of this submission was to create a program that challenges the user to a 5-word hangman game to help them recognize new words and to expand their vocabulary . Analysis: . Row Score Explaintation Notes/Discrepancies . Program Function and Purpose | 1/1 | The video response does a good job of showing the input, functionality, and output of the program, challenging the user to a game of hangman by selecting a word and taking in user inputs to judge whether or not a correct guess was found. Moreover, the overall purpose and function of the program was also clearly stated within the written response, aiming to improve people&#39;s ability tp recognize new words and to expand their vocabulary. | | . Data Abstraction | 1/1 | The program and response does include the usage of a list, it&#39;s name, and the purpose it serves. The list is in the form of an array that stores the individual letters of a word (letOfGuessWord), which allows for the program to perform future comparisons. Moreover, the written response also very clearly shows the list being used in the program to fullfil it&#39;s purpose in successful program execution | | . Managing Complexity | 1/1 | The written response does an excellent job at stating how the usage of the list serves to reduce complexity. The student responds that without the usage of a list, they would likely have to resort to the use of individual variables to store each individual letter, which would not only be cumbersome and produce clutter, but would also be annoying to implement an iterative method to check if there was a match. | | . Procedural Abstraction | 1/1 | The written reponse demonstrates excellent usage of a function guessWords in order to compare the user inputted word against the randomly selected word by the program itself. This function also consists of 1 string argument to represent the user&#39;s letter guess at the hidden word, and checks each index of the word list to find a match for the letter. The students also a does a good job to describe how the algorithm works, and the steps that can be taken to reproduce it | | . Algorithm Implementation | 1/1 | The algorithm indicates the proper use of a for-loop to implement iteration, in order to properly check over each letter in the hidden word. Sequencing could also be observed in the functio, and selection is also implemented to check whether or not if there was a match in the letter and the word, and if the total number of lives is greater than 0. If the total number of lives was found to be equal to 0, the game will end and a losing screen will be displayed | | . Testing | 1/1 | The written response clearly shows the inputting of two different inputs, one containing the number &quot;1&quot; that is not in the test string &quot;hello&quot;, and another containing the letter &quot;h&quot; that is found in &quot;hello&quot;. Moreover, the response clearly details why each response gave their resutls and also properly identified each respective result of the program. Finally, the student also explicitly cited the line numbers within the code to give a detailed overview of how the program executes. | | . Cross-grading of submission 3 . The theme of this submission was to create a program that intakes a state inputed by the user and outputs certain information concerning that state. . Analysis: . Row Score Explaintation Notes/Discrepancies . Program Function and Purpose | 1/1 | The video response does a good job of showing the input, functionality, and output of the program, which had the user input a state and then select different pieces of information to display on the screen. Moreover, the written response also clear documented the purpose (function) of the program was to provide the user with information on a us state, and that the actual purpose was to help with memorization and to learn new things. | | . Data Abstraction | 0/1 | The program and response does include the usage of a list, it&#39;s name (statelist), and the purpose it serves. However, it does not show how the data is stored within the list. As such, it does not earn the point for this row of the rubric. Moreover, the description in the written response for the list was also inaccurate as the list statelist only represents the name of the states. | &lt;/tr&gt; Managing Complexity | 1/1 | The written response does an good job at stating how the usage of the list serves to reduce complexity. The student responds that without the usage of a list, they would likely have to code each peice of data independently from each other, resulting in greater code complexity. | Collegeboard says that the written response does not go a good job of explaining in detail on how the operation would be more complex to store. | . Procedural Abstraction | 0/1 | Although the written reponse demonstrates the usage of a function updatescreen in order to assign an index to each state. This function does not contain any parameters that are passed to it. Thus the program does not earn the point in this row. | Collegeboard decided that the written response also inaccurately described the main function of the procedure | . Algorithm Implementation | 0/1 | The algorithm indicates the proper use of many if statements to implement sequencing and selection. However, the program does not indicate the use of any form of a loop or anything similar to denote a use of iteration in the program. | | . Testing | 1/1 | The written response shows the results and scenario of the both proceudres to identify that their program works as expected according to the set purpose and function | The written reponse does not properly show different parameters that are plugged into the procedures. Additionally, it does not describe the result in detail, instead, it only gives a breif overview of what happens on screen instead. | . &lt;/table&gt; ## Cross-grading of submission 4: The theme of this submission was to create a program that simulates a fishing game to reduce boredom. ### Analysis: Row Score Explaintation Notes/Discrepancies . Program Function and Purpose | 1/1 | The video response does a good job of showing the input and output of the program, which was to use certain keyboard keys such as a and d to move the boat, and eventually add to a total count of fish if the hook comes into contact with a fish. THe function and purpose were also stated explicity, with the purpose being to reduce boredom, and the function being to simulate a fishing game. | | . Data Abstraction | 1/1 | The program and response does include the usage of a list, it&#39;s name (fishtypes), and the purpose it serves. The list is in the form of an array that stores the total amount of fish caught by the user. Later, the list is also shown to be used to output the total number of fish of each type caught. | | . Managing Complexity | 1/1 | The written response does an good job at stating how the usage of the list serves to reduce complexity. The student responds that without the usage of a list, they would likely have to resort to the use of individual variables to store each fish and the number of that fish caught, which would not only be cumbersome and produce clutter, but would also be hard to add new fish types in the future. The student also address and refutes the possibility of using individual variables for each type of fish and it&#39;s total. | | . Procedural Abstraction | 1/1 | The written reponse demonstrates usage of a function clone movement in order to determine the spawning of the fishes including their location, distance, and final position. This function consists of 5 arguments serving to define the range and speed of the fish. The student also explains how this specific helps the functionality of the program as a whole, as it serves to randomly spawn fishes with different attributes to increase the difficulty of the game. | | . Algorithm Implementation | 1/1 | The algorithm indicates the proper use of a loop to implement iteration to move the fish clone until it eithe reaches the end of the screen, or is touched by the hook. Sequencing could also be observed in the function as it creates a clone of a fish first before executing the loop and it&#39;s following logic. Moreover, the algorithm also uses an if statement to implement selection in the code to determine whether of not if the clone is touching the edge of the screen. | | . Testing | 1/1 | The written response clearly shows the inputting of two different inputs, one where the clone is touching the right edge of the screen, and another one where it doesn&#39;t. Moreover, the response clearly details the results of each procedure. The end results of the program are also the expected results. | According to collegeboard, this student did not earn the point for this row because they only tested for the alternate code segments, rather than actual arguments, parameters, and inputs into the function itself. | . | .",
            "url": "https://ylu-1258.github.io/YLu-Blog/markdown/apcsp/tri2/2023/01/22/collegeboard-grading-3.html",
            "relUrl": "/markdown/apcsp/tri2/2023/01/22/collegeboard-grading-3.html",
            "date": " • Jan 22, 2023"
        }
        
    
  
    
        ,"post18": {
            "title": "Database/Model, Backend, OOP, Python",
            "content": "Database and Table Terms . The foundations of database is defining one or more Tables. In Python, a database can be constructed using the foundations we learned in modeling a Class. . A &quot;Table&quot; is a Model/Schema within a Database. | A &quot;Table&quot; definition in Python/SQLAlchemy is manifested by defining a &quot;Class&quot; and &quot;Attributes&quot; in Python. | A Python Class can inherit database functionality from SQLAlchemy. This is a method Python developers use to turn a Class into a Table within a SQL Database. | Writing methods in the Class for Create, Read, Update, Delete (CRUD) is how a developer initiates database operations. | . Reference . SQLAlchemy | Python Backend with Flask, SQLite | Python Backend starting Controller | . Outline for Backend Model and Database . OOP modeling with SQLAlchemy enables CRUD operations. . Users Table Schema | Database Properties . | Initial Database Setup (call) . | Initial Database Setup (add records) . | OOP CRUD operations . | User Table Schema . The db.Model is inherited into the class User(db.model), Each db.Column is provided properties according to capabilities of SQL. See init.py for db object definition. . db = SQLAlchemy(app) . &quot;&quot;&quot; database dependencies to support sqliteDB examples &quot;&quot;&quot; from __init__ import app, db from sqlalchemy.exc import IntegrityError from werkzeug.security import generate_password_hash, check_password_hash &quot;&quot;&quot; Key additions to User Class for Schema definition &quot;&quot;&quot; # Define the User class to manage actions in the &#39;users&#39; table # -- Object Relational Mapping (ORM) is the key concept of SQLAlchemy # -- a.) db.Model is like an inner layer of the onion in ORM # -- b.) User represents data we want to store, something that is built on db.Model # -- c.) SQLAlchemy ORM is layer on top of SQLAlchemy Core, then SQLAlchemy engine, SQL class User(db.Model): __tablename__ = &#39;users&#39; # table name is plural, class name is singular # Define the User schema with &quot;vars&quot; from object id = db.Column(db.Integer, primary_key=True) _name = db.Column(db.String(255), unique=False, nullable=False) _uid = db.Column(db.String(255), unique=True, nullable=False) _password = db.Column(db.String(255), unique=False, nullable=False) _dob = db.Column(db.Date) # Defines a relationship between User record and Notes table, one-to-many (one user to many notes) posts = db.relationship(&quot;Post&quot;, cascade=&#39;all, delete&#39;, backref=&#39;users&#39;, lazy=True) # constructor of a User object, initializes the instance variables within object (self) def __init__(self, name, uid, password=&quot;123qwerty&quot;, dob=date.today()): self._name = name # variables with self prefix become part of the object, self._uid = uid self.set_password(password) self._dob = dob . User Table CRUD operations . All these def&#39;s are methods of the User Class. . def create(self): try: # creates a person object from User(db.Model) class, passes initializers db.session.add(self) # add prepares to persist person object to Users table db.session.commit() # SqlAlchemy &quot;unit of work pattern&quot; requires a manual commit return self except IntegrityError: db.session.remove() return None # CRUD read converts self to dictionary # returns dictionary def read(self): return { &quot;id&quot;: self.id, &quot;name&quot;: self.name, &quot;uid&quot;: self.uid, &quot;dob&quot;: self.dob, &quot;age&quot;: self.age, &quot;posts&quot;: [post.read() for post in self.posts] } # CRUD update: updates user name, password, phone # returns self def update(self, name=&quot;&quot;, uid=&quot;&quot;, password=&quot;&quot;): &quot;&quot;&quot;only updates values with length&quot;&quot;&quot; if len(name) &gt; 0: self.name = name if len(uid) &gt; 0: self.uid = uid if len(password) &gt; 0: self.set_password(password) db.session.commit() return self # CRUD delete: remove self # None def delete(self): db.session.delete(self) db.session.commit() return None . Outline to understand Devops and Databases . DevOps and the SQLite database requires file management and configuring Python to work in local and deployment environment. . Volumes in Deployment | Database URI (deployment) . | Review Dockerfile . | Database URI (testing) . | Exclude/Ignore Testing Database . | Port and Host (testing) . | Run locally as you develop Select main.py file in VSCode and press Play button, or press down arrow next to Play button to activate Debug testing. The below dialog will appear in Terminal. . | (base) machine:flask_portfolio user$ cd /Users/user/vscode/flask_portfolio ; /usr/bin/env /Users/user/opt/anaconda3/bin/python /Users/user/.vscode/extensions/ms-python.python-2022.20.2/pythonFiles/lib/python/debugpy/adapter/../../debugpy/launcher 61127 -- /Users/user/vscode/flask_portfolio/main.py * Serving Flask app &quot;__init__&quot; (lazy loading) * Environment: production WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Debug mode: on * Running on all addresses. WARNING: This is a development server. Do not use it in a production deployment. * Running on http://192.168.1.75:8086/ (Press CTRL+C to quit) * Restarting with watchdog (fsevents) * Debugger is active! * Debugger PIN: 403-552-045 . Run with Docker prior to commit and deployment It is much simpler to debug Docker issues in VSCode prior to Deployment on AWS. This will create production database sample, which will show up commits. Simply delete file after run session, do not commit it. | (base) machine:flask_portfolio user$ pwd # verify directory /Users/user/vscode/flask_portfolio (base) machine:flask_portfolio user$ docker-compose build # run docker build [+] Building 16.0s (10/10) FINISHED =&gt; [internal] load build definition from Dockerfile 0.0s ... =&gt; [1/6] FROM docker.io/library/python:3.10@sha256:e9f824eec86879b1ffe8da9ef3eb4677dd6e89b63e8bce8cb35c572f550751d8 0.0s =&gt; CACHED [2/6] RUN apt-get update &amp;&amp; apt-get upgrade -y &amp;&amp; apt-get install -y python3 python3-pip git 0.0s =&gt; [3/6] COPY . / 0.2s =&gt; [4/6] RUN pip install --no-cache-dir -r requirements.txt 9.7s =&gt; [5/6] RUN pip install ... =&gt; =&gt; writing image sha256:47ccd2a3ca839139727dd8c8f57fc1ad6a36e670962059d960d21b3ad0cba292 0.0s =&gt; =&gt; naming to docker.io/library/flask_port_v1 0.0s (base) machine:flask_portfolio user$ docker-compose up # enable for browser testing, no -d! This allows ctl-C to stop and all errors will appear in Terminal [+] Running 1/1 ⠿ Container flask_portfolio-web-1 Recreated 0.1s Attaching to flask_portfolio-web-1 flask_portfolio-web-1 | [2023-01-15 13:08:19 +0000] [1] [INFO] Starting gunicorn 20.1.0 flask_portfolio-web-1 | [2023-01-15 13:08:19 +0000] [1] [INFO] Listening at: http://0.0.0.0:8086 (1) flask_portfolio-web-1 | [2023-01-15 13:08:19 +0000] [1] [INFO] Using worker: sync flask_portfolio-web-1 | [2023-01-15 13:08:19 +0000] [7] [INFO] Booting worker with pid: 7 . docker-compose.yml has a volume mount. . Docker run in a container, its own virtual computer. Outside of the Docker container is where we desire to keep the SQLite database, thus there is a mount to a files accessible by developers. This ensures data is saved after Docker virtual computer up/down. The Left side of volume says ./volumes is a relative location on computer where docker-compose build/up was run, this is were database can always be seen. The Right side /volumes is where the Docker container see the volume. . version:&#39;3&#39;services: web: image: flask_port_v1 build: . ports: - &quot;8086:8086&quot; volumes: - ./volumes:/volumes restart: unless-stopped . init.py contains project settings. . This file contains app variables that can be referenced anywhere the app elements is imported (from __init__ import app, db). This is a way of sharing configuration across the many python/flask files in the project. Review the below file and become familiar with the settings ... . from flask import Flask from flask_login import LoginManager from flask_sqlalchemy import SQLAlchemy from flask_migrate import Migrate &quot;&quot;&quot; These object can be used throughout project. 1.) Objects from this file can be included in many blueprints 2.) Isolating these object definitions avoids duplication and circular dependencies &quot;&quot;&quot; # Setup of key Flask object (app) app = Flask(__name__) # Setup SQLAlchemy object and properties for the database (db) dbURI = &#39;sqlite:////volumes/flask_portfolio.db&#39; app.config[&#39;SQLALCHEMY_TRACK_MODIFICATIONS&#39;] = False app.config[&#39;SQLALCHEMY_DATABASE_URI&#39;] = dbURI app.config[&#39;SECRET_KEY&#39;] = &#39;SECRET_KEY&#39; db = SQLAlchemy(app) Migrate(app, db) # Images storage app.config[&#39;MAX_CONTENT_LENGTH&#39;] = 5 * 1024 * 1024 # maximum size of uploaded content app.config[&#39;UPLOAD_EXTENSIONS&#39;] = [&#39;.jpg&#39;, &#39;.png&#39;, &#39;.gif&#39;] # supported file types app.config[&#39;UPLOAD_FOLDER&#39;] = &#39;volumes/uploads/&#39; # location of user uploaded content . Outline of SQL Administration Tools . Learning SQL basics using SQLite tools. . Install SQLite Viewer in VSCode marketplace, click on SQL database to review schema and see rows. . | Install SQLite and review SQLite Explorer. . | SQLite3 Website describes the command line SQL administration tool. To install on Mac terminal brew install sqlite3, on WSL terminal sudo apt install sqlite3. . | SQL Cheat Sheet. These commands can be done using SQLite3 and perhaps in Market Place tooling. | Loading SQLite3 from volumes directory . (base) machine:volumes user$ sqlite3 sqlite.db . Review content in users table . sqlite&gt; select id, _name, _uid, _dob from users; 1|Thomas Edison|toby|1847-02-11 2|Nicholas Tesla|niko|2023-01-14 3|Alexander Graham Bell|lex|2023-01-14 4|Eli Whitney|whit|2023-01-14 5|John Mortensen|jm1021|1959-10-21 6|Hedy Lamar|hedy|2023-01-14 7|Hedy Lamarr|heddy|2023-01-14 8|Marie Currie|marie|2023-01-14 9|Wilma Flintstone|wilma|2023-01-14 10|Fred Flintstone|fred|0010-01-01 . Delete content in users table, redisplay . sqlite&gt; delete from users where id=7; sqlite&gt; select id, _name, _uid, _dob from users; 1|Thomas Edison|toby|1847-02-11 2|Nicholas Tesla|niko|2023-01-14 3|Alexander Graham Bell|lex|2023-01-14 4|Eli Whitney|whit|2023-01-14 5|John Mortensen|jm1021|1959-10-21 6|Hedy Lamar|hedy|2023-01-14 8|Marie Currie|marie|2023-01-14 9|Wilma Flintstone|wilma|2023-01-14 10|Fred Flintstone|fred|0010-01-01 . Quit SQLite3 and return to prompt . sqlite&gt; .quit . SQLite Explorer . id _name _uid _password _dob . 1 | Thomas Edison | toby | sha256$Yj69CENuKWaRDHw3$dcb731d06dd59b7a316a853944b6603197876aa3a9553a758865d849a2b0a502 | 1847-02-11 | . 2 | Nicholas Tesla | niko | sha256$muQZzhqAFgXLERzC$cc7a02c80fc80190cd0b052d0dd010272cc8bc81971a7fe3cad6cee4a13e55e7 | 2023-01-14 | . 3 | Alexander Graham Bell | lex | sha256$rBKtyle2TI1vY7y9$ace54fb1e44640213f4aa284d6fe05cd27bfbcd1f7ac6d2ba697400d0b78cb7e | 2023-01-14 | . 4 | Eli Whitney | whit | sha256$j31DNBdcLtDALA4x$d39828e9e016c675b6c27ce1e6a08e196ffa5b7740978d6b313d039e60756375 | 2023-01-14 | . 5 | John Mortensen | jm1021 | sha256$5gt0IT2TiQm0UWi5$3d74775d35d2e2b0865deefb8f59e22e2b35d8c19ab4b1c26c7e50a27d3bf9fd | 1959-10-21 | . Hacks . The Big Picture purpose of this hack is to build a database. Being able to create an SQLite table and populate test data within it is the major goal. To do this effectively it is imperative to show the following. . Build Schema for a table, make a new model file and use users.py as an example. Start slow and simple and build up. | Build an initXXXXX() method and use it to add preliminary/test data to the table. Once again use users.py as an example. | Make a 30-60 second video where you show a Debugging session of making new rows in the table. Use sqlite marketplace tools and/or sqlite3 command line tool to show success in creating table and adding data. | Hack Helper . Here are some tips. . Become familiar with initUsers(). Observe it is called/activated from main.py. This function activates after you run the main.py and activate the web application in the browser. Observe that the sqlite.db file will appear in the volumes directory in conjunction with home screen of site appearing in browser. . | Delete sqlite.db from volumes directory on your development machine. Set breakpoint on initUsers() and run main.py using debug. Use the step into option on the debugger and observe the creation of data. . | . @app.before_first_request def activate_job():initJokes() initUsers() . Make your own XXXXX.py file under model directory. Follow users.py and develop your own schema from the OOP code you did in last Hacks. Follow the pattern in users.py to make a initXXXX() function top populate some test data. . | In main.py, add your initXXXX() method to def activate_job() function shown above. Use this as basis of your video debugging session. Debugging is hugely important at this level to understand your database success prior to building an API. Building initXXXX() method, adding database records, and debugging will enable you to verify CRUD operations as you develop. . | .",
            "url": "https://ylu-1258.github.io/YLu-Blog/2023/01/17/PBL-database.html",
            "relUrl": "/2023/01/17/PBL-database.html",
            "date": " • Jan 17, 2023"
        }
        
    
  
    
        ,"post19": {
            "title": "Collegeboard Grading part 1",
            "content": "Cross-grading of submission 1 . The theme of this submission was to create a program that displays a set of animals an then prompts the user to remember all animals in a memory gussing game. . Analysis: . Row Score Explaintation Notes/Discrepancies . Program Function and Purpose | 0/1 | The video response does a good job of showing the input, functionality, and output of the program, which displayed a set of animals for the user to remember and eventually input back into the system to test for correctness. However, only the function of the program was stated in the written response, and the purpose of the program was neglected. | | . Data Abstraction | 1/1 | The program and response does include the usage of a list, it&#39;s name, and the purpose it serves. The list is in the form of an array that stores the pictures of each individual animal. Moreover, the type of data was clearly stated and the name of the array animalImages was clearly specified. | This project scored a 0 according to collegeboard scoring guidelines because the submissions failed to include an instance where the list was used in the code to fulfill the program purpose. | . Managing Complexity | 0/1 | Although the response explains why a list specifically would serve to manage complexity and imrpove program quality, it does indicate that additional vairables would be used in place of a list, however, it does not show a segment of code that demonstrates how the list was used to manage complexity. | | . Procedural Abstraction | 1/1 | The written reponse indicates a function that takes in an user quess as input and verifies it with the prior animalList list. This function is also shown to be called within a for-loop that will continuosly check the user&#39;s knowledge. | The collegeboard grader said that the written response did not serve to explain how the particular procedure helped to contribute to the overall functionality of the program | . Algorithm Implementation | 1/1 | The algorithm indicates the proper use of a for-loop to implement iteration and sequencing. Moreover, the algorithm also uses an if statement to implement selection in the code to determine if the player should gain a point or not. | The student did not earn the point for this row because they did not explain in detail enough on how the algorithm itself worked so that someone else may recreate it. | . Testing | 1/1 | The written response clearly shows the inputting of two different guesses, one that&#39;s correct, and another one that was false. Moreover, the response clearly details why each response gave their resutls and also properly identified each respective result of the program. | | . Cross-grading of submission 2 . The theme of this submission was to create a program that intakes certain words and ideas inputted by the user and then reassembles such words to eventually peice together a poem . Analysis: . Row Score Explaintation Notes/Discrepancies . Program Function and Purpose | 1/1 | The video response does a good job of showing the input, functionality, and output of the program, created a poem based on a set of words inputted by the user. Moreover, the overall purpose and function of the program was also clearly stated within the written response, aiming to stimulate the creative sides of people by offering a program that can assemble poems for them. | | . Data Abstraction | 1/1 | The program and response does include the usage of a list, it&#39;s name, and the purpose it serves. The list is in the form of an array that stores the inputs of words from the user, including words descritive of trees, oceans, and the sky. Moreover, the type of data was clearly stated and the name of the array nounList was clearly specified. | . Additionally, the program also does a good job of utilizing this word list to eventually select words to use in the poem. | . Managing Complexity | 1/1 | The written response does an excellent job at stating how the usage of the list serves to reduce complexity. The student responds that without the usage of a list, they would likely have to resort to the use of individual variables to store user input, which would not only be cumbersome and produce clutter, but would also be difficult to implement a radomized selection with. | | . Procedural Abstraction | 1/1 | The written reponse demonstrates excellent usage of a function createPoems in order to generate a string containing all poems demanded by the user. Moreover this function is later used in the program to directly generate a string for printing out the poems. This function also consists of 3 list arguments and 1 boolean (integer) argument to dictate the words to use for generation, and whether or not to include articles in the poem. The students also a does a good job to describe how the algorithm works, and the steps that can be taken to reproduce it | | . Algorithm Implementation | 1/1 | The algorithm indicates the proper use of a while-loop to implement iteration. Sequencing could also be observed in the function as the coutner variable is set before the loop, to generate the correct amount of poems. Moreover, the algorithm also uses an if statement to implement selection in the code to determine if the poem should contain articles or not. | | . Testing | 1/1 | The written response clearly shows the inputting of two different inputs, one desiring articles in the poem, and another one that does not. Moreover, the response clearly details why each response gave their resutls and also properly identified each respective result of the program. Finally, the student also explicitly cited the line numbers within the code to give a detailed overview of how the program executes. | | . Cross-grading of submission 3 . The theme of this submission was to create a program that intakes two marvel characters and assign certain attributes to them for the program to ultimately determine which character would win . Analysis: . Row Score Explaintation Notes/Discrepancies . Program Function and Purpose | 0/1 | The video response does a good job of showing the input, functionality, and output of the program, which displayed a pair of marvel characters to test for their respective power levels. However, only the function of the program was stated in the written response, and the purpose of the program was neglected. | | . Data Abstraction | 1/1 | The program and response does include the usage of a list, it&#39;s name (firstCharacterList), and the purpose it serves. The list is in the form of an array that stores the attribute values of each character and the url of that character. Moreover, the list was shown to be accessed by another function within the program, to ultimately calcuate that character&#39;s average attribute values. | &lt;/tr&gt; Managing Complexity | 1/1 | The written response does an good job at stating how the usage of the list serves to reduce complexity. The student responds that without the usage of a list, they would likely have to resort to the use of individual variables to store each attribute value, which would not only be cumbersome and produce clutter, but would also be hard to view as the code would contain multiple refrences to many variables, while with a list approach, the program can simply iterate over that list.. | | . Procedural Abstraction | 1/1 | The written reponse demonstrates excellent usage of a function findWinner in order to determine the overall winner of the comparison game, and to change the displayed info on the screen. This function consists of 2 list arguments to dictate the statistics and attributes of each marvle character. The students also a does a good job to describe how the algorithm works, and the steps that can be taken to reproduce it, going over the various cycles completed by the loops and the selections made by the if-else statements. | | . Algorithm Implementation | 1/1 | The algorithm indicates the proper use of a for-loop to implement iteration. Sequencing could also be observed in the function as the average attributes of both characters are calculated before the final comparison between the two. Moreover, the algorithm also uses an if statement to implement selection in the code to determine the final winner of the match. Additionally, the program also contains an edge case where the average attribute values of both characters were equal, displaying a tied screen instead. | | . Testing | 1/1 | The written response clearly shows the inputting of two different inputs, one comparing two inequal characters, and another one that comapres two similar ones. Moreover, the response clearly details why each response gave their resutls and also properly identified each respective result of the program. The student also makes various references to the findWinner() function to state how the if-else statements helps to determine the outcome of the program. | | . &lt;/table&gt; ## Cross-grading of submission 4 The theme of this submission was to create a program that simulates a wordle-esque game by having the user try to guess a word based on how close the spelling is ### Analysis: Row Score Explaintation Notes/Discrepancies . Program Function and Purpose | 1/1 | The video response does a good job of showing the input, functionality, and output of the program, which was to test the critical thinking skills of the player and to ultimately determine if the user-guessed words were correct or not | | . Data Abstraction | 1/1 | The program and response does include the usage of a list, it&#39;s name (guessess), and the purpose it serves. The list is in the form of an array that stores the user&#39;s guesses of each of their tries. Additionally, it should be noted that the list only has the maximum length of 6 to represent the 6 tries that the user has to input their data. | Collegeboard states that only the length of the array is being accessed, and not each individual element within the list, thus not meeting all of the requirements in this row | . Managing Complexity | 1/1 | The written response does an good job at stating how the usage of the list serves to reduce complexity. The student responds that without the usage of a list, they would likely have to resort to the use of individual variables to store each attribute value, which would not only be cumbersome and produce clutter, but would also be hard to calculate the overall number of guess that the program already had. However, the student failed to consider the implmentation of a counter variable to address this issue. | Collegeboard stated the same reasoning as I did, that the student can just use a counter to keep track of the number of guesses, but instead gave this row a 0/1 | . Procedural Abstraction | 1/1 | The written reponse demonstrates usage of a function isitcorrect in order to determine whether or not the user inputted choice was a correct one. This function consists of 1 argument checkanswer to dicate the answer that needs verification. The student also explains how this specific helps the functionality of the program as a whole, as it reduces the action of checking an answer down to a single procedure that can be called many times over. | | . Algorithm Implementation | 1/1 | The algorithm indicates the proper use of a loop to implement iteration to check the status of each letter in the player&#39;s guess. Sequencing could also be observed in the function as it implements a counter variable to keep track of the current letter to check before iteration. Moreover, the algorithm also uses an if statement to implement selection in the code to determine the end output of the letter, either correct, not in the right place, or not in the word. Additionally, the program also determines when the win/loss screen shows up. | | . Testing | 1/1 | The written response clearly shows the inputting of two different inputs, one comparing a letter in the right locations, and another one that comapres an existing letter in the wrong location. Moreover, the response clearly details why each response gave their resutls and also properly identified each respective result of the program. The end results of the program are also the expected results | According to collegeboard, this student did not earn the point for this row because they only tested for the conditions that would control their program, rather than actual arguments and inputs into the function itself. | . | .",
            "url": "https://ylu-1258.github.io/YLu-Blog/markdown/apcsp/tri2/2023/01/16/collegeboard-grading.html",
            "relUrl": "/markdown/apcsp/tri2/2023/01/16/collegeboard-grading.html",
            "date": " • Jan 16, 2023"
        }
        
    
  
    
        ,"post20": {
            "title": "An Empirical Study On Performance Measures Of Collaborative Filtering Recommendation Algorithms",
            "content": "An Empirical Study on Performance Measures of Collaborative Filtering Recommendation Algorithms . Alex Lu . maodou1258@gmail.com . Abstract . In recent times, recommendation engines have become increasingly popular within many industries. The focus of such an engine is to implement an algorithm to successfully make recommendations based on user preferences. Because of the utility provided by such tools, many industries heavily rely on such engines to provide accurate product recommendations to customers. Some popular algorithms implemented include clustering, matrix factorization, and deep learning implementations. Such algorithms are utilized within Collaborative Filtering methods and are simulated using pre-made datasets containing user and item information. Performance metrics are then applied to the algorithm to test the accuracy of the model. Results show that deep learning algorithms provide greater . performance when compared to other applications. . Keywords - Collaborative filtering, RMSE, Deep Learning, MAE, matrix factorization, Performance Measure . 1. Introduction . Latterly, recommendation engines serve a great purpose in many online services and enhance the consumer experience by providing lists of recommended items to users. Many big corporations such as Netflix, Amazon, Youtube, and others provide item recommendations through such methods to promote the sales and usage of their goods. The general idea of a recommender system is to return a list of items that the user would find interesting. Implementing various different types of engines provides insight towards which algorithm is the most suitable for certain scenarios. . Different approaches by different algorithms could have varying performances depending on the type of recommendation required, and the core nature of the dataset used. This research is primarily conducted on collaborative-filtering techniques, but similar research could be conducted on content-based implementations. Examples of potential implementations could be clustering and matrix factorization approaches. The main accuracy metrics utilized in the research are the RMSE and MAE metrics which are further explained later in the paper. . Two main research questions would serve as the focus of the research. More conclusions could be drawn from the data collected, but the main aim of the research are as follows: . I) What CF implementation provides the accuracy measure for a standard data set containing user, item and rating information? II) How does data sparsity affect the prediction accuracy of the models implemented? . 2. Background . Recommendation engines are programs used to provide item recommendations to users based on filtering each item and returning a . possible predicted rating for the user. The two main implementations of recommendation engines are content-based filtering and collaborative filtering. . 2.1 Content-based Filtering . A content-based filtering engine takes into account the user’s own item history and focuses on keywords in items rather than similarity between users [1]. However, such implementation can lead to a scenario where providing a broad range of recommendations would become impossible. Because of the nature of content-based filtering engines, items that were utilized by similar users wouldn’t be recommended to the main user purely because of the lack of a keyword or phrase. A dataset for such an engine would incorporate items along with a detailed profile for each item. . Despite its narrower scope when providing recommendations, content-based filtering systems often reduce the amount of data needed to make accurate predictions. For collaborative filtering, a greater range of data is required for a proper calculation of an item. To put it simply, the more data the engine has, the more . accurate the prediction is. However, with the nature of content-based filtering, any amount of data would suffice, providing that there are items that match the recommendation requirements. . 2.2 Collaborative Filtering . A collaborative filtering engine takes into account other user’s ratings, and returns . algorithms. This paper would primarily focus on model-based algorithms and approaches. The similarity between users could be found in various methods. For this study, a total of twelve algorithms were implemented. The main focus was placed on nearest neighbors, matrix factorization, and deep learning algorithms. Figure 2.1 maps out the various . . recommendations based on similarity. Collaborative filtering is split into two main approaches, model-based and memory based . algorithms implemented in this research. 2.3 Nearest Neighbor . Nearest neighbor, or more specifically, k nearest neighbors is a collaborative filtering . algorithm used to find similarities between items and users based on the total distance between two neighbors [2]. A weight system could also be applied so that a neighbor closer to the main user would have more weight on the recommendations than one who is further away. . Four of the twelve algorithms used in the research were from this category. KNNBasic, KNNWithZScore, KNNWithMeans, and KNNBaseline were such implementations of this category of algorithms. . 2.4 Matrix Factorization . Matrix Factorization is yet again another implementation of collaborative filtering. Simply put, a matrix factorization relates two separate values together under a specific value to create a grid or matrix of the data [3]. However, as data is not evenly distributed, some cells in the matrix are left empty and would need to be filled in to provide recommendations. Such values are known as “latent features”. . Some matrix factorizations implemented in the research were Negative Matrix Factorization, Singular Value Decomposition, and SVD++. . 2.5 Deep Learning . A deep learning model utilizes a neural network to process and calculate information. Much like a human brain, deep learning attempts to make predictions based on data much like a human brain. . The deep learning model implemented in this experiment utilizes an embedding layer structure [4]. Using such a structure could organize data into a vector of discrete values which could then produce similarity results and tests through the distance between vectors. These types of embedding layers could be generated through frameworks such as “Pytorch” or “Tensorflow”, however, this research would be implemented in Pytorch. The model could then be trained through a series of epochs and eventually provide predictions. . 2.6 Miscellaneous surprise algorithms . The miscellaneous category comprises the leftover algorithms in the surprise package that do not fit into any of the other categories in the research. Such algorithms were as listed: Co . Clustering, Normal Predictor, Baseline-Only, and Slope-One. . According to Sahar Karat Co-Clustering is a collaborative filtering algorithm used to provide recommendations by “A simultaneous clustering of the rows and columns of a matrix” [5]. Classical clustering algorithms only focus on one specific type of data, while co . clustering could be used to accommodate two simultaneously. . Normal predictor is an algorithm provided by the surprise package that provides random user recommendations based on the data distribution in the dataset. The “Maximum Likelihood Estimation” method is utilized in this calculation. . Baseline-Only is yet another surprise algorithm that makes baseline predictions based on the data provided. There are two main ways to implement this algorithm. The first implementation method is “Stochastic Gradient . Descent” (SGD), which calculates a gradient of the dataset by a random selection of data. The other implementation is to use “Alternating Least Squares” (ALS) which is a matrix factorization algorithm that works well with sparser sets of data. . Slope-One is an example of an item-based collaborative filtering recommendation algorithm. Predictions made with this model are generally based on personal ratings as well as similar community ratings. . Further information and implementations on the methods listed could be found on the surprise documentations [6]. . 2.7 Accuracy Metrics . Two accuracy metrics were used in the process of this research. Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE) [11]. . RMSE is a quadratic model that assigns much larger weights to larger errors observed. Accuracy is calculated through RMSE by taking the square root of the mean of the sum of the squared difference of each predicted . value and its corresponding actual value. The formula is displayed in Figure 2.2 . . (Figure 2.2) . MAE is a linear metric model that linearly scales error. This model is typically used when higher errors are not important to the observation. Accuracy is calculated through MAE by taking the mean of the sum of the absolute values of the differences between the predicted and actual values. The formula is displayed in Figure 2.3 . . (Figure 2.3) . 3. Related Work . Various research topics have already been analyzed in the field of collaborative filtering. A few similar pieces of research provide similar methods and implementations. A plethora of CF models has been implemented and researched. . 3.1 Similar Research . Similar research was conducted by Mojdeh Saadati et al. [7] on the implementations of various models on movie recommender systems. The two models used in the experiment were the matrix factorization model SVD and a deep learning implementation of the Restricted Boltzmann Machine. The performance of the implementations was measured in the RMSE metric. However, in this research, we go over a broader spectrum of algorithms from each group of implementations and produce results for a larger picture between the different implementations. . 3.2 Further Research . He et al. [8] Conducted Research on how Matrix Factorization could be implemented with Deep Learning to create a better performing model for collaborative filtering. The proposed idea was to use two main models, the Generalized Matrix Factorization (GMF), and the Multi-Layer Perceptron (MLP) to create a hybrid between deep learning and matrix factorization implementations to create a new neural matrix factorization model . dubbed “NeuMF”. NeuMf was then compared to other well-known models such as KNN and ALS implementations and proved to have better accuracy and less training loss. . Yedder et al. [9] Researched the performance of the restricted Boltzmann machine. Various hidden units, learning rates, and other factors were incorporated into the research. Problems such as data sparsity were also encountered in the research and required other methods of implementation. This research also utilized the RMSE accuracy metric to rate the performance of the model. The research indicated an excellent RMSE measure of 0.46. . 4. Approach . This study was broken up into three parts, cleaning, implementation, and evaluation. We will first check for invalid values throughout the dataset, then begin to implement algorithms, and finally calculate performance values. . 4.1 Dataset . The data that would be primarily used in this study is the Kaggle dataset “Anime . Recommendations Database” [10] based on the data collected from myanimelist.net and is included in the reference section of the study. The two files contained within the database are the “ratings.csv” and “anime.csv”. The research would mostly work with the ratings file, but the anime dataset could be further implemented in future research incorporating variables such as show genre and overall ratings. The anime dataset is separated into three separate columns. The user_id, anime_id, and ratings columns are provided for analyzing similarities between users. The format and example of the file are illustrated in Figure 4.1. . (Figure 4.1) . The python module “pandas” was used to import the data into a data frame which could . then be cleaned and analyzed. Making a copy of the dataset to work on is recommended. Maintaining two different data frames would help compare the results on the original to the results in the cleaned dataset. . The first thing to take note of while cleaning is to remove all rows with a rating of “-1”. Such rows represent user data that does not have a specified rating for the specific show that they watched. This step is crucial as having such data in the project would result in a sparse dataset, which directly decreases performance as shown later in the results. A total of 7.8 million rows of data were in the dataset, after cleaning, about 6.3 million rows remained. The distribution of this data is shown in figure 4.2. . . (Figure 4.2) . Another issue arose again while trying to apply the dataset to our engine. Certain cells in . our dataset possibly had ‘NaN’ values, which would in turn cause an error in actual predictions where a value of ‘NaN’ would be returned instead of an integer of the predicted rating. To resolve this issue, we first applied the ‘to_numeric’ method from the pandas module to convert each value into workable integer or float values. After, we could then finally lower the runtime of our experiment by decreasing the size of our dataset from 6 million down to 125,000. This number was picked for easier splitting while implementing other algorithms and deep learning which require testing and training datasets. From there, we then calculated the distribution of the ratings by counting the number of each rating in our dataset. After clearing, it’s paramount to re index the rows in the data frame. Removing the invalid A distribution of the ratings is demonstrated in Figure 4.3. . . (Figure 4.3) . A correlation heatmap could also be used to detect any possible correlations between the values given, however, as of now, there is no apparent relation. . . (Figure 4.4) . 4.2 Implementation . The python package scikit-surprise contains most of the algorithms implemented in this research. First and foremost, we must declare a rating scale for our dataset by utilizing the reader class, and also initializing our dataset with the Dataset class. We set a new . “ratings“ object over the range of one to ten, which is represented in our data. Using the three columns from our pandas data frame, we could then load the data into surprise using the Dataset.load_from_df() method. . After the data is loaded, we could then specify a “param_list” dictionary. In the dictionary, the name of the specific parameters would be stored as the keys and the possible values as a list in the values. Further explanation of what specific params do for each algorithm is also provided in the surprise documentation. . The param_list dictionary could then be imputed into the GridSearchCV() method along with the algorithm name, performance measures, and a cross-validation iterator of 3. There are other arguments as well, but for the sake of our research, these four should be enough. The final step is to then fit our premade data with our GridSearchCV method with the .fit() function. Once everything is set up, the accuracy metrics could then just be retrieved via the best_score iterator. . The deep learning algorithm specifically was generated using an embedding layer with . dropout layers. The entire network consists of 4 total layers. After setting up the net, we can begin to train our model and loop over varying amounts of epochs to find the accuracy that is desired. . To determine the RMSE and the MAE values of this approach, we could separate our dataset into predictions and truth arrays. We can then apply the formulas for both RMSE and MAE as shown in Figures 2.2 and 2.3. We can implement these metrics in two ways. The first method is to use a NumPy array to subject each vector of values from each other and then to apply the formula to our newly generated values. Our second approach is more basic and rudimentary. We could subtract each truth value from each prediction value in our function by declaring a function and then apply the formula to our sum. . 5. Results . In this research, two sets of results were collected. The data collected on the cleaned dataset would serve to be our solution to the first question. However, the data collected on the original dataset would be utilized to answer . the second question concerning the performance measures on sparse datasets. 5.1 Cleaned Dataset . We have gathered both RMSE and MAE values for each algorithm used on our cleaned dataset from our implementations. In the first part of this experiment, the data set has already been cleaned of any invalid values and has reduced sparsity. The majority of the algorithms implemented all showed similar results except a couple of outliers and certain points. A table of the data collected is shown in Figure 5.1 . . (Figure 5.1) . Using the RMSE as the x-axis and MAE for the y-axis, we can plot a scatter plot of our data (Figure 5.2). . . (Figure 5.2) . Removing the Normal Predictor outlier value provides us with a clearer image of the differences in the lower valued points (Figure 5.3). . . (Figure 5.3) . Our data shows that the worst-performing algorithm that we had implemented was the Normal Predictor algorithm from the surprise package, while the best-performing implementation was the Deep Learning algorithm based on neural networks. . 5.1 Pre-cleaned Dataset . Loading up a new dataset, we can effectively run all our algorithms again while maintaining the sparsity of the original dataset. The only cleaning that had to be done was to remove all NaN values and also convert all data to numeric values. The results of running the . implementations on the sparser dataset are recorded in the figure below (Figure 4.5) . . (Figure 5.4) . The results of the various algorithms on this sparse dataset were also plotted on a scatter plot as shown below in Figure 5.5. . . (Figure 5.5) . Once again, removing the normal predictor outlier gives a clearer representation of our other data points (Figure 5.6). . . (Figure 5.6) . Although the values have increased by a considerable amount in the sparse dataset, the common trend remains between the various data points, and no changes are observed in the best and worst algorithms. Comparing the . results of the cleaned and original data sets, a clear difference could be observed in the RMSE and MAE measures (Figures 5.7-8) . (Figure 5.7) . (Figure 5.8) . 6. Discussion . After analyzing the data collected from the research, the deep learning algorithm and the KNNBaseline implementations were observed to be the best performing with the least error observed with both RMSE and MAE metrics. . A possible reason for the results could be the usage of randomness in the calculation of the Normal Predictor algorithm. The deep learning algorithm may have performed the best because of the various training cycles allocated to it which helped to create a more accurate model after each iteration. Vice versa, the opposite could also be applied to the two worst performing algorithms KNNBasic and Normal Predictor. Such implementations . had basic calculations and weren’t able to take into account outliers and other potential biases in the data. . There was an attempt in the research to implement a Restricted Boltzmann Machine (RBM) model, however, the implementation gave varying results and was difficult to judge the extra ratings of the implementation. This . research could be further pursued in the future with the addition of more deep learning implementations and a narrower focus on the subject of deep learning as a whole. From the results acquired, deep learning has been shown to have improved results compared to other algorithms. Focused research on deep learning implementations would provide the reasoning behind deep learning accuracy. . 7. Conclusion . In this research, we explored various machine learning algorithms, K-Nearest neighbors, Matrix Factorization, Deep learning, etc. Several approaches were implemented from the categories mentioned then tested for accuracy measures. . In both the cleaned and original datasets, the deep learning implementation was shown to the least margin of error when making recommendations. From this, it could be deduced that deep learning is a viable method for collaborative filtering engines working with user, item and rating data. Although different sets of data have varying optimal algorithms, . deep learning was still shown to be extremely accurate compared to other tested algorithms. Analyzing the data collected from the sparse dataset, we can conclude that a sparser set of data would result in less accurate recommendations, sometimes up to double the margin of error observed. Because of this observation, it can be concluded that collaborative filtering best performs with dense datasets. . References . [1] Kirzhner, Elena. “Machine Learning. Explanation of Collaborative . Filtering vs Content Based . Filtering.” Medium, Codeburst, 11 . May 2018, codeburst.io/explanation of-recommender-systems-in . information-retrieval-13077e1d916c. [2] Harrison, Onel. “Machine Learning Basics with the K-Nearest Neighbors Algorithm.” Medium, Towards Data Science, 14 July 2019, . towardsdatascience.com/machine . learning-basics-with-the-k-nearest . neighbors-algorithm-6a6e71d01761. . [3] Chen, Denise. “Recommendation System - Matrix Factorization.” . Medium, Towards Data Science, 9 . July 2020, . towardsdatascience.com/recommend ation-system-matrix-factorization . d61978660b4b. . [4] Sivanantham, Balavivek. . “Recommendation System Implementation With Deep Learning and PyTorch.” Medium, The Startup, 18 Aug. 2020, . medium.com/swlh/recommendation system-implementation-with-deep learning-and-pytorch-a03ee84a96f4. . [5] Karat, Sahar. “Co . Clustering.” Data Science . Made Simpler, 5 Mar. 2016, datasciencemadesimpler.wor dpress.com/tag/co . clustering/. . [6] Hug, Nicholas. “Welcome to Surprise’ Documentation.” Welcome to Surprise’ . Documentation! - Surprise 1 Documentation, 2015, . surprise.readthedocs.io/en/sta ble/. . [7] Mojdeh Saadati, Syed Shihab, Mohammed Shaiqur Rahman “Movie Recommender Systems: Implementation and Performance . Evaluation.” Semantic Scholar, 2019, . www.semanticscholar.org/paper/Mo vie-Recommender-Systems%3A Implementation-and-Saadati Shihab/01470f39285213e53f365ce0 1417b18d12467563#citing-papers. . [8] Xiangnan, He, et al. “Neural Collaborative Filtering.” . International World Wide Web Conference Committee, 3 Apr. 2017. . [9] Yedder, Hanene Ben, et al. “Modeling Prediction in . Recommender Systems Using Restricted Boltzmann Machine.” . IEEE Explore, IEEE, 5 Oct. 2017, ieeexplore.ieee.org/abstract/documen t/8122923. . [10] CooperUnion. “Anime . Recommendations Database.” . Kaggle, Kaggle, 21 Dec. 2016, . www.kaggle.com/CooperUnion/ani me-recommendations-database. . [11] Kampakis, Stylianos. “Performance Measures: RMSE and MAE.” The Data Scientist, The Data Scientist, 26 Nov. 2020, . thedatascientist.com/performance measures-rmse-mae/. .",
            "url": "https://ylu-1258.github.io/YLu-Blog/2023/01/11/An-Empirical-Study-on-Performance-Measures-of-Collaborative-Filtering-Recommendation-Algorithms.html",
            "relUrl": "/2023/01/11/An-Empirical-Study-on-Performance-Measures-of-Collaborative-Filtering-Recommendation-Algorithms.html",
            "date": " • Jan 11, 2023"
        }
        
    
  
    
        ,"post21": {
            "title": "Alex Lu   Lab #1_ Measurements And Graphical Analysis",
            "content": "Lab #1: Measurements and Graphical Analysis . Alex Lu . Purpose: . Given disks of different radii, determine the relationship between the mass and the radius of the disks through graphical method and calculate the uncertainty associated with the measured value. We will learn about linearization and use it to create a mathematical model. . Materials and Equipment: . Balance . Meter stick . Circular disks (identical thickness and uniform density but different radii) . Graphic Calculator or online graphing tool . Procedure: . Substitute equations to get a relationship between mass and radius. . | Use the relationship between mass and radius to determine what variable should be processed to linearize the data . | Measure the radius (in cm) of the cylinder with the ruler with one end of the ruler at the center of the disk and the other on the edge. . | Zero out the balance and then measure the mass (in grams) of the cylinder. . | Stack the metal cylinders and measure the collective height of all cylinders. Divide by the total number of cylinders multiplied by 2 to the power of the number of folds to find the height of each individual cylinder. . | Organize the collected data into a data table . | Plot the original radius and mass . | Plot the processed radius and mass . | Plug height and other constants to create a relationship between mass and radius. . | Equations and calculations: . Let: . ( rho) be density . | (m) be mass . | (v) be volume . | (a) be the surface area of the cylinder . | (r) be the radii of the cylinder . | (h) be the height of the cylinder . | . Thus: . If ( rho = frac{m}{v}) and (v = ah) then . (m = rho v) . (m = rho text{ah}) . If (a = pi r^{2}), then . (m = rho pi r^{2}h) . Since both ( rho) and ( pi) are constants and (h) is negligible, ∴ (m propto r^{2}) . Since mass is directly proportional to radius squared, the graph could be linearized if (r^{2}) is plotted instead. . Precision and Uncertainty . The balanced used to measure the mass of the metal disks was accurate up to a hundredth of a gram, while the ruler used to measure the radius and height of the disks was accurate up to 1 millimeter (0.1 cm). As such, the radius was measured to the nearest hundredth of a centimeter or a tenth of a millimeter. . | Data Tables . m = mass . r = radii . H = height . Mass in Grams, Radius and height in cm, and Radius Squared of Each Metal Disk . Disk m (g) h (cm) r (cm)   r^2 (cm^2) . 1 | 0.07 | 0.0025 | 2.30 |   | 5.29 | . 2 | 0.14 | 0.0025 | 3.00 |   | 9.0 | . 3 | 0.25 | 0.0025 | 4.30 |   | 18.49 | . 4 | 0.37 | 0.0025 | 4.90 |   | 24.01 | . 5 | 0.73 | 0.0025 | 7.10 |   | 50.41 | . Graph of Non-Linearized Data: . . Equation: (y = 0.0133921x^{2} + 0.112825x - 0.0238674) . R2 : (0.9967) . This graph has non-linear data as it’s represented by a quadratic model. . Graph of Linearized Data: . . Line of the best fit equation: (y = 0.0145569x - 0.000100527) . r = (0.9983) . r2 = (0.9966 ) . This graph has linear data as it’s represented by a linear model. . Analysis Questions: . 1) What is the independent variable in your y = mx +b formula? . Considering the mathematic relationship between disk radius r and disk mass m, the independent variable from my line of best fit represents the radius of the disk squared. . 2) What does the slope represent in your y = mx +b formula? Show dimensionally that indeed that is what your slope represents and that the formula is valid dimensionally. . Using the (m = rho v) equation and the volume relationship, we can express the disk’s mass in terms of the radius and the height in this relationship: (m = rho pi r^{2}h). Since density (( rho)) and ( pi) are constants, and height ((h)) is negligible, we can essentially group these 3 constants into one value serving as the coefficient to the radius squared term. Thus, the slope is the product of the density, height, and ( pi). Since height has its units expressed in cm, density has units expressed in g/cm³, and ( pi) is a constant wth no units, the slope is a combination of all 3 constants and has units of g/cm2. . Because our slope has units of g/cm2, and x represents the radius squared expressed by cm2, and since our desired output ((y)) is mass in grams (g), the y-intercept ((b)) must have a unit of grams (g) in order for both sides of the equation to have the same units (grams). . (g = frac{g}{cm^{2}} cdot cm^{2} + g) . (g = g + g) . (g = g) . 3) Should the “b” in your y = mx + b formula be zero? Explain your answer. . The y-intercept ((b)) in my (y = mx + b) formula shouldn’t be zero unless the data naturally generates a line of best fit that passes through the origin. Forcing the y-intercept to pass through the origin would either alter the slope ((m)) or remove the y-intercept completely. This would result in an inaccurate interpretation of the model as the line is no longer set evenly between the data points. Thus we cannot guarantee the model’s integrity for future data points, rendering our model useless. . 4) Measure/estimate the “thickness” of your cylinders. Use that value to find the experimental density of your cylinders. Find a percent difference between your found density and the actual density. The actual material is aluminum. . (y = 0.0145569x - 0.000100527) . (slope = pi rho h) . ( rho = density) . ( rho = frac{ text{slope}}{h pi} = frac{0.0145569 frac{g}{cm^{2}}}{ pi(0.0025cm)} = 1.85 frac{g}{cm^{3}}) . The density of the metal disks according to my linear model is (1.85 frac{g}{cm^{3}}) , Since the metal used in the disk is Aluminum (density = (2.70 frac{g}{cm^{3}})) we can use the following calculations to determine our percent error . ( % Error = left | frac{Actual - Expected}{ text{Expected}} right | *100 % ) | . ( % Error = left | frac{1.85 frac{g}{cm^{3}} - 2.70 frac{g}{cm^{3}}}{2.70 frac{g}{cm^{3}}} right | *100 % = 31.5 % ) | . ∴ Percent difference = 31.5% . 5) Errors. Make sure you explain why your number is bigger or smaller than (if positive or negative difference.) . The number that I obtained is smaller than the actual value (31.5% error) because the instruments that we used to measure the disks’ dimensions had inaccuracies. The ruler that we used to measure the disks’ radius is only precise up to millimeters, and the balanced used to measure mass is only accurate up to the hundredth’s place, while actual instruments used to measure the actual value are much more precise. Additionally, the method I used to measure the height of the disks also had inherent inaccuracies, as folding the aluminum disks over each other may create tiny air pockets that add additional magnitude to my measured height, resulting in an inaccurate height measurement. The uncertainty of the ruler and also the inaccurate way of measuring height may have contributed to a greater height value than the actual value, resulting in a lower density value when I divided. Lastly, the disks weren’t in a perfectly circular shape, thus the direction that I measured the disk in might also have affected the end result, as measuring in different directions would produce different radii. . Synthesis Questions: . 1) In this experiment, if we had used disks with a greater thickness, would the slope of your best fit line have been different? Would your experimental value for density be the same? Explain. . Since slope represents the product of thickness, ( pi), and density, having a greater measured thickness would result in a much larger slope value. However, our experimental value of density would still be around the same, as an increase in thickness would also result in a proportionally large increase in mass, which will offset the thickness increase. . 2. How would your graph of m versus r2 be different if you had used disks of the same . thickness but made out of steel? Draw a second line on your m versus r2 plot that . represents disks made of steel. . The graph would be of m versus r2 would be different in that the slope of the graph would be much greater, this is because steel has a much higher density of (7.85 frac{g}{cm^{3}}) compared to aluminum’s (2.7 frac{g}{cm^{3}}) . . (Note: The blue line is a rough sketch of what the line of best fit would look like on the same scale as the aluminum graph is steel was used instead. This is not an accurate representation, just an approximation) . 3. Another group of students has acquired data for the exact same experiment; however, their disks are made of an unknown material that they are trying to determine. The group’s m versus r2 data produced a line of best fit with slope equal to 122 kg/m2. Each disk they measured had the same 0.5 cm thickness. Calculate the density of the unknown material and use the table below to help determine what material their disks are made of. . Work: . (Slope = 122kg/m^{2} , h = 0.5cm, slope = rho pi h) . (slope = 122 frac{ text{kg}}{m^{2}}( frac{1000g}{1kg})( frac{1m}{100cm})^{2} = 12.2 frac{g}{cm^{2}} = rho pi h) . ( rho = frac{ text{slope}}{ pi h} = frac{12.2 frac{g}{cm^{2}}}{ pi*0.5cm} = 7.77 frac{g}{cm^{3}}) . The closet material to a density of (7.77 frac{g}{cm^{3}}) is iron, which has a density of (7.8 frac{g}{cm^{3}}). The disks are most likely made out of iron. . Multiple Choice Questions: . 1. You perform the same experiment, but this time you plot a linear relationship between mass and the circumference of the disks rather than the radius. What is the slope of the linear plot? . Work: . (slope = rho pi h) . (m = rho v = rho pi r^{2}h) . (v = pi r^{2}h) . Let (c) be circumference . (c = 2 pi r) . (c^{2} = 4 pi^{2}r^{2}) . (r^{2} = frac{c^{2}}{4 pi^{2}}) . (m = rho pi h( frac{c^{2}}{4 pi^{2}}) = frac{ rho hc^{2}}{4 pi} = ( frac{ rho h}{4 pi})c^{2}) . () . Since ( rho) (density), h (height), and (4 pi) are all constants, these 3 values could be combined to form the slope for the linear plot, which is (( frac{ text{ph}}{4 pi})), Hence, E is the correct answer choice. . 2. Skipped . 3. Consider an experiment in which a student measures the mass and diameter of 10 . different-sized spheres, all made of the same material of uniform density ρ. For this . student to create a linear graph relating the mass of the sphere to its radius r, the . student would need to plot mass m versus which quantity: . ( text{let v be the volume of a sphere}) . (v = frac{4}{3} pi r^{3}) . (m = pv = ( frac{4p pi}{3})r^{3}) . Since (( frac{4p pi}{3})) is a constant, the student must plot a linear graph relating m versus the quality of radius cubed. Hence, option C is correct. .",
            "url": "https://ylu-1258.github.io/YLu-Blog/2023/01/11/Alex-Lu-Lab-1_-Measurements-and-Graphical-Analysis.html",
            "relUrl": "/2023/01/11/Alex-Lu-Lab-1_-Measurements-and-Graphical-Analysis.html",
            "date": " • Jan 11, 2023"
        }
        
    
  
    
        ,"post22": {
            "title": "Model and Object-Oriented Programming",
            "content": "Class and Object Terms . The foundations of Object-Oriented Programming is defining a Class . In Object-Oriented Programming (OOP), a class is a blueprint for creating an Object. (a data structure). An Object is used like many other Python variables. | A Class has ... a collection of data, these are called Attributes and in Python are pre-fixed using the keyword self | a collection of Functions/Procedures. These are called *Methods when they exist inside a Class definition. | . | An Object is created from the Class/Template. Characteristics of objects ... an Object is an Instance of the Class/Template | there can be many Objects created from the same Class | each Object contains its own Instance Data | the data is setup by the Constructor, this is the &quot;init&quot; method in a Python class | all methods in the Class/Template become part of the Object, methods are accessed using dot notation (object.method()) | . | A Python Class allow for the definition of @ decorators, these allow access to instance data without the use of functions ... @property decorator (aka getter). This enables developers to reference/get instance data in a shorthand fashion (object.name versus object.get_name()) | @name.setter decorator (aka setter). This enables developers to update/set instance data in a shorthand fashion (object.name = &quot;John&quot; versus object.set_name(&quot;John&quot;)) | observe all instance data (self._name, self.email ...) are prefixed with &quot;&quot;, this convention allows setters and getters to work with more natural variable name (name, email ...) | . | . Class and Object Code . # Werkzeug is a collection of libraries that can be used to create a WSGI (Web Server Gateway Interface) # A gateway in necessary as a web server cannot communicate directly with Python. # In this case, imports are focused on generating hash code to protect passwords. from werkzeug.security import generate_password_hash, check_password_hash from datetime import date import json # Define a User Class/Template # -- A User represents the data we want to manage class User: # constructor of a User object, initializes the instance variables within object (self) def __init__(self, name, uid, password, bm=1, bd=1, by=1970): self._name = name # variables with self prefix become part of the object, self._uid = uid self._dob = date(by, bm, bd) self.set_password(password) # a name getter method, extracts name from object @property def name(self): return self._name @property def dob(self): month = {1:&quot;Jan&quot;, 2:&quot;Feb&quot;, 3:&quot;Mar&quot;, 4:&quot;Apr&quot;, 5:&quot;May&quot;, 6:&quot;Jun&quot;, 7:&quot;Jul&quot;, 8:&quot;Aug&quot;, 9:&quot;Sep&quot;, 10:&quot;Oct&quot;, 11:&quot;Nov&quot;, 12:&quot;Dec&quot;} return &quot;{0} {1}, {2}&quot;.format(str(month[self._dob.month]), str(self._dob.day), str(self._dob.year)) # a setter function, allows name to be updated after initial object creation @name.setter def name(self, name): self._name = name @dob.setter def dob(self, by, bm, bd): self._dob.year = int(by) self._dob.month = int(bm) self._dob.day = int(bd) # a getter method, extracts email from object @property def uid(self): return self._uid # a setter function, allows name to be updated after initial object creation @uid.setter def uid(self, uid): self._uid = uid # check if uid parameter matches user id in object, return boolean def is_uid(self, uid): return self._uid == uid @property def password(self): return self._password[0:10] + &quot;...&quot; # because of security only show 1st characters # update password, this is conventional setter def set_password(self, password): &quot;&quot;&quot;Create a hashed password.&quot;&quot;&quot; self._password = generate_password_hash(password, method=&#39;sha256&#39;) def calculate_age(self): day = date.today() return day.year - self._dob.year if ((self._dob.month , self._dob.day) &lt; (day.month, day.year)) else day.year - self._dob.year- + 1 # check password parameter versus stored/encrypted password def is_password(self, password): &quot;&quot;&quot;Check against hashed password.&quot;&quot;&quot; result = check_password_hash(self._password, password) return result # output content using str(object) in human readable form, uses getter def __str__(self): return f&#39;name: &quot;{self.name}&quot;, id: &quot;{self.uid}&quot;, psw: &quot;{self.password}&quot;&#39; # output command to recreate the object, uses attribute directly def __repr__(self): return f&#39;Person(name={self._name}, uid={self._uid}, password={self._password})&#39; # tester method to print users def tester(users, uid, psw): result = None for user in users: # test for match in database if user.uid == uid and user.is_password(psw): # check for match print(&quot;* &quot;, end=&quot;&quot;) result = user # print using __str__ method print(str(user)) return result # place tester code inside of special if! This allows include without tester running if __name__ == &quot;__main__&quot;: # define user objects u1 = User(name=&#39;Thomas Edison&#39;, uid=&#39;toby&#39;, password=&#39;123toby&#39;) u2 = User(name=&#39;Nicholas Tesla&#39;, uid=&#39;nick&#39;, password=&#39;123nick&#39;) u3 = User(name=&#39;Alexander Graham Bell&#39;, uid=&#39;lex&#39;, password=&#39;123lex&#39;) u4 = User(name=&#39;Eli Whitney&#39;, uid=&#39;eli&#39;, password=&#39;123eli&#39;) u5 = User(name=&#39;Hedy Lemarr&#39;, uid=&#39;hedy&#39;, password=&#39;123hedy&#39;) # put user objects in list for convenience users = [u1, u2, u3, u4, u5] # Find user print(&quot;Test 1, find user 3&quot;) u = tester(users, u3.uid, &quot;123lex&quot;) # Change user print(&quot;Test 2, change user 3&quot;) u.name = &quot;John Mortensen&quot; u.uid = &quot;jm1021&quot; u.set_password(&quot;123qwerty&quot;) u = tester(users, u.uid, &quot;123qwerty&quot;) # Make dictionary &#39;&#39;&#39; The __dict__ in Python represents a dictionary or any mapping object that is used to store the attributes of the object. Every object in Python has an attribute that is denoted by __dict__. Use the json.dumps() method to convert the list of Users to a JSON string. &#39;&#39;&#39; print(&quot;Test 3, make a dictionary&quot;) for user in users: user.__dict__[&quot;_dob&quot;] = user.dob json_string = json.dumps(user.__dict__) print(json_string) print(u1.__dict__) print(&quot;Test 4, make a dictionary&quot;) json_string = json.dumps([vars(user) for user in users]) print(json_string) . Test 1, find user 3 name: &#34;Thomas Edison&#34;, id: &#34;toby&#34;, psw: &#34;sha256$s78...&#34; name: &#34;Nicholas Tesla&#34;, id: &#34;nick&#34;, psw: &#34;sha256$YjP...&#34; * name: &#34;Alexander Graham Bell&#34;, id: &#34;lex&#34;, psw: &#34;sha256$ODJ...&#34; name: &#34;Eli Whitney&#34;, id: &#34;eli&#34;, psw: &#34;sha256$HRf...&#34; name: &#34;Hedy Lemarr&#34;, id: &#34;hedy&#34;, psw: &#34;sha256$YgR...&#34; Test 2, change user 3 name: &#34;Thomas Edison&#34;, id: &#34;toby&#34;, psw: &#34;sha256$s78...&#34; name: &#34;Nicholas Tesla&#34;, id: &#34;nick&#34;, psw: &#34;sha256$YjP...&#34; * name: &#34;John Mortensen&#34;, id: &#34;jm1021&#34;, psw: &#34;sha256$Wj4...&#34; name: &#34;Eli Whitney&#34;, id: &#34;eli&#34;, psw: &#34;sha256$HRf...&#34; name: &#34;Hedy Lemarr&#34;, id: &#34;hedy&#34;, psw: &#34;sha256$YgR...&#34; Test 3, make a dictionary {&#34;_name&#34;: &#34;Thomas Edison&#34;, &#34;_uid&#34;: &#34;toby&#34;, &#34;_dob&#34;: &#34;Jan 1, 1970&#34;, &#34;_password&#34;: &#34;sha256$s7855dwO3jTWgWse$b0034bd36a7dffbce3b19dec79537a34bb9b57b1748db1a51466ac45e974c771&#34;} {&#34;_name&#34;: &#34;Nicholas Tesla&#34;, &#34;_uid&#34;: &#34;nick&#34;, &#34;_dob&#34;: &#34;Jan 1, 1970&#34;, &#34;_password&#34;: &#34;sha256$YjPeUzdfJAH9JUba$fdf86e4d6b8e59942a4292af682882991f01618c71da4744231806648421aaba&#34;} {&#34;_name&#34;: &#34;John Mortensen&#34;, &#34;_uid&#34;: &#34;jm1021&#34;, &#34;_dob&#34;: &#34;Jan 1, 1970&#34;, &#34;_password&#34;: &#34;sha256$Wj46qrSf64xjDHU6$fa6f5cbfe7864959a448d77c3ccff82838039efaaeb3b04051876821e5dcfb23&#34;} {&#34;_name&#34;: &#34;Eli Whitney&#34;, &#34;_uid&#34;: &#34;eli&#34;, &#34;_dob&#34;: &#34;Jan 1, 1970&#34;, &#34;_password&#34;: &#34;sha256$HRfjxkN5a01pHpAP$5b64ed3639954939b455060b81be1c0d00026862ae865aae667e435a01f04d57&#34;} {&#34;_name&#34;: &#34;Hedy Lemarr&#34;, &#34;_uid&#34;: &#34;hedy&#34;, &#34;_dob&#34;: &#34;Jan 1, 1970&#34;, &#34;_password&#34;: &#34;sha256$YgRwinmetnDXwf67$256eff02ec040f925681a472c3cefdc6fa7a55d7cbfd721c3fc5c4ad7449f353&#34;} {&#39;_name&#39;: &#39;Thomas Edison&#39;, &#39;_uid&#39;: &#39;toby&#39;, &#39;_dob&#39;: &#39;Jan 1, 1970&#39;, &#39;_password&#39;: &#39;sha256$s7855dwO3jTWgWse$b0034bd36a7dffbce3b19dec79537a34bb9b57b1748db1a51466ac45e974c771&#39;} Test 4, make a dictionary [{&#34;_name&#34;: &#34;Thomas Edison&#34;, &#34;_uid&#34;: &#34;toby&#34;, &#34;_dob&#34;: &#34;Jan 1, 1970&#34;, &#34;_password&#34;: &#34;sha256$s7855dwO3jTWgWse$b0034bd36a7dffbce3b19dec79537a34bb9b57b1748db1a51466ac45e974c771&#34;}, {&#34;_name&#34;: &#34;Nicholas Tesla&#34;, &#34;_uid&#34;: &#34;nick&#34;, &#34;_dob&#34;: &#34;Jan 1, 1970&#34;, &#34;_password&#34;: &#34;sha256$YjPeUzdfJAH9JUba$fdf86e4d6b8e59942a4292af682882991f01618c71da4744231806648421aaba&#34;}, {&#34;_name&#34;: &#34;John Mortensen&#34;, &#34;_uid&#34;: &#34;jm1021&#34;, &#34;_dob&#34;: &#34;Jan 1, 1970&#34;, &#34;_password&#34;: &#34;sha256$Wj46qrSf64xjDHU6$fa6f5cbfe7864959a448d77c3ccff82838039efaaeb3b04051876821e5dcfb23&#34;}, {&#34;_name&#34;: &#34;Eli Whitney&#34;, &#34;_uid&#34;: &#34;eli&#34;, &#34;_dob&#34;: &#34;Jan 1, 1970&#34;, &#34;_password&#34;: &#34;sha256$HRfjxkN5a01pHpAP$5b64ed3639954939b455060b81be1c0d00026862ae865aae667e435a01f04d57&#34;}, {&#34;_name&#34;: &#34;Hedy Lemarr&#34;, &#34;_uid&#34;: &#34;hedy&#34;, &#34;_dob&#34;: &#34;Jan 1, 1970&#34;, &#34;_password&#34;: &#34;sha256$YgRwinmetnDXwf67$256eff02ec040f925681a472c3cefdc6fa7a55d7cbfd721c3fc5c4ad7449f353&#34;}] . Hacks . Add new attributes/variables to the Class. Make class specific to your CPT work. . Add classOf attribute to define year of graduation Add setter and getter for classOf | . | Add dob attribute to define date of birth This will require investigation into Python datetime objects as shown in example code below | Add setter and getter for dob | . | Add instance variable for age, make sure if dob changes age changes Add getter for age, but don&#39;t add/allow setter for age | . | Update and format tester function to work with changes | . Start a class design for each of your own Full Stack CPT sections of your project . Use new code cell in this notebook | Define init and self attributes | Define setters and getters | Make a tester | . Start Code for Hacks . from datetime import date def calculate_age(born): today = date.today() return today.year - born.year - ((today.month, today.day) &lt; (born.month, born.day)) dob = date(2004, 12, 31) age = calculate_age(dob) print(age) . 18 . Demonstrating that the caucluate age method works . u6 = User(name=&#39;Alex Lu&#39;, uid=&#39;0&#39;, password=&#39;CyberPatriot1!&#39;, bm=11, bd=29, by=2006) print(u6.calculate_age()) . 16 . Start a class design for each of your own Full Stack CPT sections of your project . Use new code cell in this notebook | Define init and self attributes | Define setters and getters | Make a tester | . class Food: def __init__(self, name=&quot;&quot;, ingredients={}, handmade=True, serving_size=1): self._name = name self._ingredients = ingredients self._handmade = handmade self._size = serving_size @property def name(self): return self._name @property def ingredients(self): res = &quot;Ingredients: n&quot; for ing, amn in self._ingredients.items(): res = res + &quot; t {0} : {1} n&quot;.format(ing, amn) return res @property def handmade(self): return self._handmade @property def size(self): return self._size @name.setter def name(self, name): self._name = name @size.setter def size(self, size): self._size = size @handmade.setter def handmade(self, bool): self._handmade = bool # Conventional method to update our dictionary of ingredients def updateIngredients(self, ing, amnt): self._ingredients[ing] = amnt if __name__ == &quot;__main__&quot;: # Instantiating our objects MapoTofu = Food(name=&quot;Mapo Tofu&quot;, ingredients={&quot;Tofu&quot;:&quot;1 carton&quot;, &quot;Chili Peppers&quot;:&quot;4&quot;, &quot;Salt&quot;:&quot;25g&quot;, &quot;ground beef&quot;:&quot;80g&quot;, &quot;Scallions&quot;:&quot;3 stalks&quot;}, handmade=True, serving_size=3) BoiledFish = Food(name=&quot;Boiled Fish&quot;, ingredients={&quot;Fish&quot;:&quot;1&quot;, &quot;Chili Peppers&quot;:&quot;4&quot;, &quot;Salt&quot;:&quot;17g&quot;, &quot;potatoes&quot;:&quot;2, diced&quot;, &quot;Scallions&quot;:&quot;3 stalks&quot;, &quot;MSG&quot;:&quot;70g&quot;, &quot;Sugar&quot;:&quot;10g&quot;, &quot;Water&quot;:&quot;1 L&quot;}, handmade=True, serving_size=6) HerbalJellyDessert = Food(name=&quot;Sweet Herbal Jelly with fruit&quot;, ingredients={&quot;Herbal Jelly&quot;:&quot;2 boxes&quot;, &quot;Watermelon&quot;:&quot;1/4 diced&quot;, &quot;Brown sugar syrup&quot;:&quot;20ml&quot;, &quot;Peanuts&quot;:&quot;5g, crushed&quot;, &quot;Tangerine&quot;:&quot;1&quot;, &quot;Apples&quot;:&quot;1, diced&quot;}, handmade=False, serving_size=1) Meal = [MapoTofu, BoiledFish, HerbalJellyDessert] # Testing setters (names should now be set with their chinese counterparts): MapoTofu.name = &quot;麻婆豆腐&quot; BoiledFish.name = &quot;水跳鱼&quot; HerbalJellyDessert.name = &quot;冰粉&quot; # Testing getters for meal in Meal: print(meal.name + &quot;:&quot;) print(&quot; tSize Serving: &quot;, meal.size) print(&quot; tHomemade? &quot;, meal.handmade) print(&quot; t&quot;+meal.ingredients) . 麻婆豆腐: Size Serving: 3 Homemade? True Ingredients: Tofu : 1 carton Chili Peppers : 4 Salt : 25g ground beef : 80g Scallions : 3 stalks 水跳鱼: Size Serving: 6 Homemade? True Ingredients: Fish : 1 Chili Peppers : 4 Salt : 17g potatoes : 2, diced Scallions : 3 stalks MSG : 70g Sugar : 10g Water : 1 L 冰粉: Size Serving: 1 Homemade? False Ingredients: Herbal Jelly : 2 boxes Watermelon : 1/4 diced Brown sugar syrup : 20ml Peanuts : 5g, crushed Tangerine : 1 Apples : 1, diced .",
            "url": "https://ylu-1258.github.io/YLu-Blog/2023/01/10/PBL-model.html",
            "relUrl": "/2023/01/10/PBL-model.html",
            "date": " • Jan 10, 2023"
        }
        
    
  
    
        ,"post23": {
            "title": "Collegeboard Grading part 1",
            "content": "Cross-grading of submission 1 . The theme of this submission was to create a program that returns the type of a triangle based on it’s side lengths, as well as the various trigonometric ratios of the triangle’s 3 angles. . Analysis: . Row Score Explaintation . Program Function and Purpose | 0/1 | The video response does a good job of showing the input, functionality, and output of the program, and while the written response explains the functionality of the program (What the program does), it does not explina the purpose of the program (Why the progra was created or the problem it intends to solve) | . Data Abstraction | 1/1 | The program and response does include the usage of a list, it&#39;s name, and the purpose it serves, but the list used ultimately does not serve to reduce complexity or serve the functionality of the program. The programmer could&#39;ve just returned literal strings rather than individual list indicies and such. | . Managing Complexity | 0/1 | While the program does include the usage of a list, it does not make sense to store all possible outputs of a program in such a structure. It would be much more simpler had the individual return statements been stored as simple strings and such, making it easier for debugging | . Procedural Abstraction | 1/1 | The student has created a function that takes in 3 parameters, that returns the overall ratio of the triangle&#39;s sides, which is eventually stored within a varaible for future use and printing. This function is shown to be functioning based on the data inputted in the video. | . Algorithm Implementation | 1/1 | The algorithm indicates the proper use of a for-loop to iterate over a range of integers, while selecting specific values from a premade list to output triangle types in the code. The code also follows a clear sequence, where each line is dependent on data or manipulation made in previous lines. | . Testing | 1/1 | The written response clearly shows the inputting of two different input sets, one of integers and one of floats. The written response then goes over the conditional in the ratioCalculate function, which analyzes the the numbers to see if any are factors of others. Finally, the code returns the correct expected outputs, even having the floats converted into an integer ratio. | . Cross-grading of submission 2 . The theme of this submission was to create a program that can simulate conway’s game of life, using the scratch block coding language to achieve their goal. . Analysis: . Row Score Explaintation . Program Function and Purpose | 1/1 | The video response does a good job of showing the input, functionality, and output of the program, which was the user inputed set of pixels, and a set of conditionals to execute the necessary operations, and a final replacement of the previous board as the output of the program. Moreover the function and purpose was clearly stated in the written response, which was to create a program to simulate conway&#39;s game of life for the user&#39;s entertainment | . Data Abstraction | 1/1 | The program and response does include the usage of a list, it&#39;s name, and the purpose it serves. The list is in the form of a grid, and from the looks of it, a two dimentional one given the x and y coordinate inputs used to access each individual element. The written response also shows how the grid is later used in the `cellCheck` function in order to execute the rules of the game of life. Moreover, it seems the list itself is named as `currentGrid` and consists of boolean values of 1 (clicked) and 0 (not clicked) | . Managing Complexity | 1/1 | The written response gives a detailed yet concise explaination as to why the lists used were optimal and even necessary for program function. This is because since Conway&#39;s game of life is a very graphical game, it requires some form or method to be able to store the status of each individual cell in order to run the simulation. In this case, using a list consolidates all the required data of the cells in a single variable name, rather than using thousands of individual variables or some other complex data structures to accomplish (like a graph or something similar) | . Procedural Abstraction | 1/1 | The written reponse very clearly indicates a function that takes in 2 lists as input and updates the current list with the predicted output in order to display the output that the user expects to see. This function is later called within the `Newgen` function satisfying the requirements for this row. | . Algorithm Implementation | 1/1 | The algorithm indicates the proper use of a for-loop to iterate over the range of their gird, while smaking specific selections with their nested if and else conditionals. By doing so, the program could then correctly identify which list to copy to which list, making the overal program functional yet efficient. | . Testing | 1/1 | The written response clearly shows the inputting of two different input sets, one of the beginning inputted grid frame and the currently displayed frame, and the other of the current grid and the next grid. This is clear demonstration of good testing as both inputs give the proper desired results, as the first test represents the beginning of the game as the board is generated and passed along to the current grid, while the second function shows the progression of the game from one frame to the next. | .",
            "url": "https://ylu-1258.github.io/YLu-Blog/markdown/apcsp/tri2/2023/01/05/collegeboard-grading.html",
            "relUrl": "/markdown/apcsp/tri2/2023/01/05/collegeboard-grading.html",
            "date": " • Jan 5, 2023"
        }
        
    
  
    
        ,"post24": {
            "title": "Collegeboard Grading part 1",
            "content": "Cross-grading of submission 1 . The theme of this submission was to create a program that returns the type of a triangle based on it’s side lengths, as well as the various trigonometric ratios of the triangle’s 3 angles. . Analysis: . Row Score Explaintation . Program Function and Purpose | 0/1 | The video response does a good job of showing the input, functionality, and output of the program, and while the written response explains the functionality of the program (What the program does), it does not explina the purpose of the program (Why the progra was created or the problem it intends to solve) | . Data Abstraction | 1/1 | The program and response does include the usage of a list, it&#39;s name, and the purpose it serves, but the list used ultimately does not serve to reduce complexity or serve the functionality of the program. The programmer could&#39;ve just returned literal strings rather than individual list indicies and such. | . Managing Complexity | 0/1 | While the program does include the usage of a list, it does not make sense to store all possible outputs of a program in such a structure. It would be much more simpler had the individual return statements been stored as simple strings and such, making it easier for debugging | . Procedural Abstraction | 1/1 | The student has created a function that takes in 3 parameters, that returns the overall ratio of the triangle&#39;s sides, which is eventually stored within a varaible for future use and printing. This function is shown to be functioning based on the data inputted in the video. | . Algorithm Implementation | 1/1 | The algorithm indicates the proper use of a for-loop to iterate over a range of integers, while selecting specific values from a premade list to output triangle types in the code. The code also follows a clear sequence, where each line is dependent on data or manipulation made in previous lines. | . Testing | 1/1 | The written response clearly shows the inputting of two different input sets, one of integers and one of floats. The written response then goes over the conditional in the ratioCalculate function, which analyzes the the numbers to see if any are factors of others. Finally, the code returns the correct expected outputs, even having the floats converted into an integer ratio. | . Cross-grading of submission 2 . The theme of this submission was to create a program that can simulate conway’s game of life, using the scratch block coding language to achieve their goal. . Analysis: . Row Score Explaintation . Program Function and Purpose | 1/1 | The video response does a good job of showing the input, functionality, and output of the program, which was the user inputed set of pixels, and a set of conditionals to execute the necessary operations, and a final replacement of the previous board as the output of the program. Moreover the function and purpose was clearly stated in the written response, which was to create a program to simulate conway&#39;s game of life for the user&#39;s entertainment | . Data Abstraction | 1/1 | The program and response does include the usage of a list, it&#39;s name, and the purpose it serves. The list is in the form of a grid, and from the looks of it, a two dimentional one given the x and y coordinate inputs used to access each individual element. The written response also shows how the grid is later used in the `cellCheck` function in order to execute the rules of the game of life. Moreover, it seems the list itself is named as `currentGrid` and consists of boolean values of 1 (clicked) and 0 (not clicked) | . Managing Complexity | 1/1 | The written response gives a detailed yet concise explaination as to why the lists used were optimal and even necessary for program function. This is because since Conway&#39;s game of life is a very graphical game, it requires some form or method to be able to store the status of each individual cell in order to run the simulation. In this case, using a list consolidates all the required data of the cells in a single variable name, rather than using thousands of individual variables or some other complex data structures to accomplish (like a graph or something similar) | . Procedural Abstraction | 1/1 | The written reponse very clearly indicates a function that takes in 2 lists as input and updates the current list with the predicted output in order to display the output that the user expects to see. This function is later called within the `Newgen` function satisfying the requirements for this row. | . Algorithm Implementation | 1/1 | The algorithm indicates the proper use of a for-loop to iterate over the range of their gird, while smaking specific selections with their nested if and else conditionals. By doing so, the program could then correctly identify which list to copy to which list, making the overal program functional yet efficient. | . Testing | 1/1 | The written response clearly shows the inputting of two different input sets, one of the beginning inputted grid frame and the currently displayed frame, and the other of the current grid and the next grid. This is clear demonstration of good testing as both inputs give the proper desired results, as the first test represents the beginning of the game as the board is generated and passed along to the current grid, while the second function shows the progression of the game from one frame to the next. | .",
            "url": "https://ylu-1258.github.io/YLu-Blog/markdown/apcsp/tri2/2023/01/04/collegeboard-grading.html",
            "relUrl": "/markdown/apcsp/tri2/2023/01/04/collegeboard-grading.html",
            "date": " • Jan 4, 2023"
        }
        
    
  
    
        ,"post25": {
            "title": "Homework 3.17 - 3.18",
            "content": "3.17 Algorithmic Efficiency . Vocabulary . Problem: A task that could be solved algorithmically or logically Decision Problem: A problem that demands a boolean yes or no answer | Organization Problem: A problem that demands a &quot;best&quot; or &quot;optimal&quot; answer | . | Instance: A problem with a specific input | Efficiency: amount of time, space, or resources it takes to solve a problem. Polynomial Efficiency (Good): More work takes a proportional amount of time and resources to complete. | Exponential Efficiency (Bad): More work takes an exponential amount of time to compelete. | . | Heuristic Approach: When optimal solutions are inefficient, look for an even more optimal solution | Decidable Problem: A decision solution that always gives the right answer | Undecidable Problem: A decision problem with no solution that is guaranteed to give the right answer | . Notes . In the case of searching for an element sequentially, the problem is in polynomial efficiency as the run time increases linearly | . Challenge . Try and fix this ineficcient code! Only change the code between the two commented lines. Fully programmed solution will improve your grade, at a minimum show that you tried. . import time numlist = [1,3,5,7,9,11,13,15,17,19] valuelist = [0,3,6,9,12,15,18,21] def isvalue(value,array): #-- # We use binary Search here :) lo = 0 hi = len(array)-1 while lo &lt;= hi: mid = (lo+hi)//2 if mid == value: return True elif mid &gt; value: hi = mid - 1 else: lo = mid+1 return False #-- starttime = time.time() for i in range(100000): for i in range(len(valuelist)): x = isvalue(valuelist[i],numlist) endtime = time.time() print(endtime-starttime,&#39;seconds&#39;) . 0.20901918411254883 seconds . 3.18 Undecidable Problems . Notes . Undecidable problems can have either multiple answers, no answers or something like the sort. In contrast, decidable problems always has an answer. | Some prolems cannot be solved by a computer. | Contradictory statements often results in undecidable problems | . Homework! . Make an algorithm that finds the fastest route that hits every location once starting and ending at Del Norte. Make sure to show your thinking. If you are strugling, try using a huristic approach. Remember, what matters more than having perfectly functioning code is that you tried your hardest. . dataset = { &#39;DelNorte&#39;:{ &#39;Westview&#39;:15, &#39;MtCarmel&#39;:20, &#39;Poway&#39;:35, &#39;RanchoBernardo&#39;:50 }, &#39;Westview&#39;:{ &#39;DelNorte&#39;:15, &#39;MtCarmel&#39;:35, &#39;Poway&#39;:25, &#39;RanchoBernardo&#39;: 45 }, &#39;MtCarmel&#39;:{ &#39;Westview&#39;:35, &#39;DelNorte&#39;:20, &#39;Poway&#39;:40, &#39;RanchoBernardo&#39;:30 }, &#39;Poway&#39;:{ &#39;Westview&#39;:25, &#39;MtCarmel&#39;:40, &#39;DelNorte&#39;:35, &#39;RanchoBernardo&#39;:15 }, &#39;RanchoBernardo&#39;:{ &#39;Westview&#39;:45, &#39;MtCarmel&#39;:30, &#39;Poway&#39;:15, &#39;DelNorte&#39;:50 } } dataset_graph = [ [0,15,20,35,50], [15,0,35,25,45], [20,35,0,40,30], [35,25,40,0,15], [50,45,30,15,0] ] . Heuristic Approach . This solution takes the most optimal step at each point. Although the overall algorithm is a greedy approach, it is not the most optimal as additional distance could be reduced if certain positions were revisted to travel a short cut to other positions . def fastestroute(start,data): drivetime = 0 order = [start] curr = start while len(order) &lt; 5: min_dist = 60 for key in data[curr]: if data[curr][key] &lt; min_dist and key not in order: next = key min_dist = data[curr][key] order.append(next) curr = next drivetime+=min_dist drivetime+=data[curr][start] order.append(start) #CODE,CODE,CODE return(drivetime,order) start = &#39;DelNorte&#39; print(fastestroute(start, dataset)) # &#39;dataset&#39; is the name of the nested key value pair . (105, [&#39;DelNorte&#39;, &#39;Westview&#39;, &#39;Poway&#39;, &#39;RanchoBernardo&#39;, &#39;MtCarmel&#39;, &#39;DelNorte&#39;]) . Optimal Approach . From one look at the problem, we can deduce that it is likely testing graph theory. The data in the data set dictionary could be converted into an undirected weighted graph, where each position is linked to the other with a weighted path, where the weight is the assigned distance. We can then attempt to find the MST (Mean Spanning Tree) of the graph through a BFS approach . from collections import defaultdict import sys class undirectedWeightedGraph(): def __init__(self, vertices): self.V = vertices self.graph = [[None for i in range(self.V)] for i in range(self.V)] def addVertice(self, s , d, weight): self.graph[s].append((d,weight)) def printGraph(self): print(&quot;Location tEdges&quot;) for i in range(self.V): temp = [str(k) for k in self.graph[i]] print(str(i) + &quot; t t&quot; + &quot; &quot;.join(temp)) def shortestPathLength(graph): if len(graph) == 1: return 0 n = len(graph) ending_mask = (1 &lt;&lt; n) - 1 queue = [(node, 1 &lt;&lt; node) for node in range(n)] seen = set(queue) steps = 0 while queue: next_queue = [] for i in range(len(queue)): node, mask = queue[i] for neighbor in graph[node]: next_mask = mask | (1 &lt;&lt; neighbor) if next_mask == ending_mask: return 1 + steps if (neighbor, next_mask) not in seen: seen.add((neighbor, next_mask)) next_queue.append((neighbor, next_mask)) steps += 1 queue = next_queue locations = undirectedWeightedGraph(5) locations.graph = dataset_graph locations.printGraph() print(shortestPathLength(locations.graph)) . Location Edges 0 0 15 20 35 50 1 15 0 35 25 45 2 20 35 0 40 30 3 35 25 40 0 15 4 50 45 30 15 0 . IndexError Traceback (most recent call last) Cell In [24], line 50 47 locations.graph = dataset_graph 49 locations.printGraph() &gt; 50 print(shortestPathLength(locations.graph)) Cell In [24], line 34, in shortestPathLength(graph) 32 for i in range(len(queue)): 33 node, mask = queue[i] &gt; 34 for neighbor in graph[node]: 35 next_mask = mask | (1 &lt;&lt; neighbor) 36 if next_mask == ending_mask: IndexError: list index out of range . Now that we have created a graph for our schools, we can move on to using djikstra&#39;s algorithm to find the most optimal path . Grading: . Challenge Homework . .15 pts for attempt | .65 for attempt | . .20 pts for complete | .70 for complete | . .25 pts for above and beyond | .75 pts for above and beyond | .",
            "url": "https://ylu-1258.github.io/YLu-Blog/homework1718",
            "relUrl": "/homework1718",
            "date": " • Dec 14, 2022"
        }
        
    
  
    
        ,"post26": {
            "title": "Guest Speaker on 12/13/2022",
            "content": "Kathyleen Beveridge . Initially had a finance degree, studied abroad in spain, not initially rooted in high tech. | Mission Statement: “My mission in life is not merely to survive, but to thrive; and to do so with some passion, some compassion, some humor, and some style” | Worked at HP, Qualcomm and now works in biotech. | Company Mission Statements: HP Inc: Create technology that makes life better for everyone, every where. | Qualcomm: Inventing the tech the world loves. | Thermo Fisher Scientific: Enable our customers to make the world healthier, cleaner, and safer. | . | These tech companies all have a macro and global impact on the world due to their size. | At one point, qualcomm chips were in 99% of the world’s cell phones, and that a billion people have their lives impacted by qualcomm | Thermo Fisher stats: more than 100,000 colleagues | 7,000 enginners and scientists | $1.5 Billion invested in research | more than 40 Billion$ in revenue | . | Mrs. Beveridge is a senior director of marketing and sales at thermo fisher, lots of people work underneathe her. | They would often give their opinions on a product before it’s release or commercialization. Work closely with a scrum master to commercialize the product. | Similar team structure to what we have in APCSP | . | . Kris Porter . Software Engineer, Senior engineer, DevOps | Livermore HS alum liked STEM subjects | . | Double e at UCLA initially failed at computer science but now does it for a living | Communication systems and computer networks. | . | Research, now the research is called the world wide web. Topic on Networked Infomechanical Systems (NIMS) | Use in the Linux Operating System, Software Development Life Cycle | Learned these marketable skill | . | Career history stock market crashed when graduated. | system engineer at GPM, sites are secure, worked with encryption and cybersecurity. | NBC Streaming media. | Devops at Qualcomm in 2014. | Working at Twitter in 2021. | Worked a lot with servers, data collection and analytics | . | Continuous Learning CCNA training | Machine Learning and Deep Learning (Qualcomm) | Python (Udemy) | Data Structures, Algorithms and System Design for Interviews | Twitter University | . | Projects Streaming Media Infrastructure for 2012 Olympic Games | Analytics Infrastructure Projects (LocationSmart, Qualcomm) | Twitter Projects New Datacenter deployment | Migrating HomeTimeline to GraphQL Twitter moving away from restAPI and to GraphQL | . | . | . | . Q and A: . Agile Methodology? . Different companies have different types of agile. workers at qualcomm would plan everything for each quarter in big meetings spanning 2-3 days. Good way to coordinate with other people working on similar projects as you. Always doing short sprints, short times in 2 or 1 months. People can actually even get certification in doing sprints. . Twitter Layoffs? . You don’t wait for companies to shuffle, you have to look ahead and start planning. . Machine Learning Frameworks? . learning how to use keras and tensorflow frameworks, in conjunction with gitlab as a tool to be able to predict to see when to archive and delete repositories. It would help to save qualcomm a lot of money for hosting. Data Scientists, join massive database fragments to predict consumer trends and other such information. Data analytics could also be used in non technical roles for different uses. . Most important skill in tech industry? . Continuous Learning is paramount for this field, different companies have different ways of doing things, different languages and conventions. Also don’t be too concerned about things on the news, programming is supposed to be a self ful-filling job. The best technical people listen to the requirements and can translate it into how technology could fulfill the requirements. . Biggest challenge in tech industry . Interviews are pretty intense now, especially for silicon valley tech companies. Especially stressful as they give you a limited amount of time to pass a certain problem. . Reflections: . Through this experience, I realized that there was much more to the tech industry than just learning how to code. Because technology changes rapidly over time, I will need to be able to continuously learn from my experiences and also from those around me in order to keep up with the trends. It is only through a skillset such as this that I may remain of good use to the company and thrive in the corporate scenario. It was also intruiging to learn about the many challenges and experiences of these tech workers, and I particularly enjoyed the part where they introduced how the agile methodology was applied in their workplace as well as their interviewing experiences. Finally, it was also enlightening to learn about the violatile job market, in order to remain employed, I must be able to look to the future and predict when would be a good time to prepare to change jobs. Overall, this was a very enlightening experience and I would love to discuss this topic with others! .",
            "url": "https://ylu-1258.github.io/YLu-Blog/markdown/apcsp/tri2/2022/12/13/guest-speaker.html",
            "relUrl": "/markdown/apcsp/tri2/2022/12/13/guest-speaker.html",
            "date": " • Dec 13, 2022"
        }
        
    
  
    
        ,"post27": {
            "title": "Unit 3 Lesson 16, Student Copy",
            "content": "Hack #1 - Class Notes . Write any extra notes you have here 1) We run simulations all the time to test projected results in real life 2) Simulations could help us represent various real life scenarios with many of their random values and nature with spinners and rngs 3) Many different examples can be simulated 1) Air traffic 2) Chemical Reactions 3) Missile launches 4) We can use the python random module to run various simulations in our programs. For instance, random number generators could be used to simulate different probabilities and such. . Hack #2 - Functions Classwork . from random import * lo = int(input(&quot;What is your desired lower range?&quot;)) hi = int(input(&quot;What is your desired upper range?&quot;)) x = randint(lo,hi) print(&quot;Your random number generated on the interval of {0} to {1} was {2}.&quot;.format(lo, hi, x)) closet = [&quot;Shoes&quot;, &quot;T-Shirts&quot;, &quot;pants&quot;, &quot;boxers&quot;, &quot;socks&quot;, &quot;jacket&quot;] new_closet = [&quot;Hoodie&quot;, &quot;boots&quot;, &quot;umbrella&quot;] def randomTrash(): x = choice(closet) closet.remove(x) print(&quot;Removed random piece of clothing &#39;{0}&#39; from closet&quot;.format(x)) def randomAdd(): y = choice(new_closet) closet.append(y) new_closet.remove(y) print(&quot;Moved a random piece of clothing &#39;{}&#39; from another closet to the main closet&quot;.format(y)) def coinflip(): if randint(1, 3) &gt; 1: print(&quot;Heads&quot;) else: print(&quot;Tails&quot;) randomTrash() randomAdd() print(closet) #Tossing the coin 5 times: coinflip() coinflip() coinflip() coinflip() coinflip() . Your random number generated on the interval of 10 to 100 was 92. Removed random piece of clothing &#39;T-Shirts&#39; from closet Moved a random piece of clothing &#39;Hoodie&#39; from another closet to the main closet [&#39;Shoes&#39;, &#39;pants&#39;, &#39;boxers&#39;, &#39;socks&#39;, &#39;jacket&#39;, &#39;Hoodie&#39;] Heads Heads Heads Tails Heads . coinflip example . Hack #3 - Binary Simulation Problem . from random import randint def randomnum(): # function for generating random int return randint(0,255) def converttobin(n): # function for converting decimal to binary x = n N = 7 res = [] while N &gt;= 0: if (x - 2 ** N) &gt;= 0: res.append(1) x-=2 ** N N-=1 else: res.append(0) N-=1 return res def survivors(): # function to assign position survivorstatus = [&quot;Jiya&quot;, &quot;Shruthi&quot;, &quot;Noor&quot;, &quot;Ananya&quot; , &quot;Peter Parker&quot;, &quot;Andrew Garfield&quot;, &quot;Tom Holland&quot;, &quot;Tobey Maguire&quot;] chances = converttobin(randomnum()) return [survivorstatus[i] for i in range(8) if chances[i] == 1] def printsurvivors(survivor_list): print(&quot;The following people survived the apocalypse: &quot;) for i in survivor_list: print(&quot; t - {}&quot;.format(i)) # replace the names above with your choice of people in the house survivor_list = survivors() printsurvivors(survivor_list) . The following people survived the apocalypse: - Jiya - Shruthi - Ananya - Tom Holland . for i in range(8): print(i) . 0 1 2 3 4 5 6 7 . Hack #4 - Thinking through a problem . create your own simulation involving a dice roll | should include randomization and a function for rolling + multiple trials | . def snakeeyes(): x = randint(1,6) y = randint(1,6) print(&quot;Dice1: &quot;, x) print(&quot;Dice2: &quot;, y) if x == y == 1: print(&quot;Snake Eyes!&quot;) return print(&quot;Nothing Special :(&quot;) def runtrials(): trials = 10 counter = 0 while counter &lt; trials: print(&quot;Trial {0}&quot;.format(counter+1)) snakeeyes() counter+=1 runtrials() . Trial 1 Dice1: 2 Dice2: 3 Nothing Special :( Trial 2 Dice1: 5 Dice2: 4 Nothing Special :( Trial 3 Dice1: 1 Dice2: 1 Snake Eyes! Trial 4 Dice1: 3 Dice2: 1 Nothing Special :( Trial 5 Dice1: 3 Dice2: 4 Nothing Special :( Trial 6 Dice1: 3 Dice2: 3 Nothing Special :( Trial 7 Dice1: 2 Dice2: 3 Nothing Special :( Trial 8 Dice1: 3 Dice2: 5 Nothing Special :( Trial 9 Dice1: 1 Dice2: 5 Nothing Special :( Trial 10 Dice1: 1 Dice2: 6 Nothing Special :( . Hack 5 - Applying your knowledge to situation based problems . Using the questions bank below, create a quiz that presents the user a random question and calculates the user&#39;s score. You can use the template below or make your own. Making your own using a loop can give you extra points. . A researcher gathers data about the effect of Advanced Placement®︎ classes on students&#39; success in college and career, and develops a simulation to show how a sequence of AP classes affect a hypothetical student&#39;s pathway.Several school administrators are concerned that the simulation contains bias favoring high-income students, however. answer options: The simulation is an abstraction and therefore cannot contain any bias | The simulation may accidentally contain bias due to the exclusion of details. | If the simulation is found to contain bias, then it is not possible to remove the bias from the simulation. | The only way for the simulation to be biased is if the researcher intentionally used data that favored their desired output. | | . | Jack is trying to plan his financial future using an online tool. The tool starts off by asking him to input details about his current finances and career. It then lets him choose different future scenarios, such as having children. For each scenario chosen, the tool does some calculations and outputs his projected savings at the ages of 35, 45, and 55.Would that be considered a simulation and why? answer options No, it&#39;s not a simulation because it does not include a visualization of the results. | No, it&#39;s not a simulation because it does not include all the details of his life history and the future financial environment. | Yes, it&#39;s a simulation because it runs on a computer and includes both user input and computed output. | Yes, it&#39;s a simulation because it is an abstraction of a real world scenario that enables the drawing of inferences. | | . | Sylvia is an industrial engineer working for a sporting goods company. She is developing a baseball bat that can hit balls with higher accuracy and asks their software engineering team to develop a simulation to verify the design.Which of the following details is most important to include in this simulation? answer options Realistic sound effects based on the material of the baseball bat and the velocity of the hit | A depiction of an audience in the stands with lifelike behavior in response to hit accuracy | Accurate accounting for the effects of wind conditions on the movement of the ball | A baseball field that is textured to differentiate between the grass and the dirt | | . | Ashlynn is an industrial engineer who is trying to design a safer parachute. She creates a computer simulation of the parachute opening at different heights and in different environmental conditions.What are advantages of running the simulation versus an actual experiment? answer options The simulation will not contain any bias that favors one body type over another, while an experiment will be biased. | The simulation can be run more safely than an actual experiment | The simulation will accurately predict the parachute&#39;s safety level, while an experiment may be inaccurate due to faulty experimental design. | The simulation can test the parachute design in a wide range of environmental conditions that may be difficult to reliably reproduce in an experiment. | | this question has 2 correct answers | . | YOUR OWN QUESTION; can be situational, pseudo code based, or vocab/concept based | YOUR OWN QUESTION; can be situational, pseudo code based, or vocab/concept based | from random import shuffle questions = [ (&quot;A researcher gathers data about the effect of Advanced Placement®︎ classes on students&#39; success in college and career, and develops a simulation to show how a sequence of AP classes affect a hypothetical student&#39;s pathway.Several school administrators are concerned that the simulation contains bias favoring high-income students, however.&quot;, &quot;A) The simulation is an abstraction and therefore cannot contain any bias.&quot;, &quot;B) The simulation may accidentally contain bias due to the exclusion of details.&quot;, &quot;C) If the simulation is found to contain bias, then it is not possible to remove the bias from the simulation.&quot;, &quot;D) The only way for the simulation to be biased is if the researcher intentionally used data that favored their desired output.&quot;, [&quot;D&quot;]), (&quot;Jack is trying to plan his financial future using an online tool. The tool starts off by asking him to input details about his current finances and career. It then lets him choose different future scenarios, such as having children. For each scenario chosen, the tool does some calculations and outputs his projected savings at the ages of 35, 45, and 55.Would that be considered a simulation and why?&quot;, &quot;A) No, it&#39;s not a simulation because it does not include a visualization of the results.&quot;, &quot;B) No, it&#39;s not a simulation because it does not include all the details of his life history and the future financial environment.&quot;, &quot;C) Yes, it&#39;s a simulation because it runs on a computer and includes both user input and computed output.&quot;, &quot;D) Yes, it&#39;s a simulation because it is an abstraction of a real world scenario that enables the drawing of inferences.&quot;, [&quot;D&quot;]), (&quot;Sylvia is an industrial engineer working for a sporting goods company. She is developing a baseball bat that can hit balls with higher accuracy and asks their software engineering team to develop a simulation to verify the design.Which of the following details is most important to include in this simulation?&quot;, &quot;A) Realistic sound effects based on the material of the baseball bat and the velocity of the hit.&quot;, &quot;B) A depiction of an audience in the stands with lifelike behavior in response to hit accuracy.&quot;, &quot;C) Accurate accounting for the effects of wind conditions on the movement of the ball.&quot;, &quot;D) A baseball field that is textured to differentiate between the grass and the dirt.&quot;, &quot;C&quot;), (&quot;Ashlynn is an industrial engineer who is trying to design a safer parachute. She creates a computer simulation of the parachute opening at different heights and in different environmental conditions.What are advantages of running the simulation versus an actual experiment?&quot;, &quot;A) The simulation will not contain any bias that favors one body type over another, while an experiment will be biased.&quot;, &quot;B) The simulation can be run more safely than an actual experiment.&quot;, &quot;C) The simulation will accurately predict the parachute&#39;s safety level, while an experiment may be inaccurate due to faulty experimental design.&quot;, &quot;D) The simulation can test the parachute design in a wide range of environmental conditions that may be difficult to reliably reproduce in an experiment.&quot;, [&quot;B&quot;, &quot;D&quot;]), (&quot;What module in python could be used to generate random values in simulation experiments?&quot;, &quot;A) random&quot;, &quot;B) randint&quot;, &quot;C) turtle&quot;, &quot;D) tensorflow&quot;, [&quot;A&quot;]), (&quot;Which of the following ways can we best simulate a spinner?&quot;, &quot;A) Having the user randomly pick a number.&quot;, &quot;B) Using the randint method from the random package over a range of values to generate a value.&quot;, &quot;C) Hard code a random sequence in the source code.&quot;, &quot;D) Use some pattern in nature to return a random value&quot;, [&quot;B&quot;]) ] correct = 0 total = 6 shuffle(questions) def printQuestion(tup): print(tup[0]) print(tup[1]) print(tup[2]) print(tup[3]) print(tup[4]) def questionloop(): correct = 0 for i in range(len(questions)): printQuestion(questions[i]) ans = input(&quot;What is your answer? &quot;) if ans in questions[i][5]: print(&quot;Correct! Nice Job!&quot;) correct+=1 else: print(&quot;Try again next time&quot;) return correct correct = questionloop() print(&quot;You got a total of {0} out of {1} problems right!&quot;.format(correct, total)) . Sylvia is an industrial engineer working for a sporting goods company. She is developing a baseball bat that can hit balls with higher accuracy and asks their software engineering team to develop a simulation to verify the design.Which of the following details is most important to include in this simulation? A) Realistic sound effects based on the material of the baseball bat and the velocity of the hit. B) A depiction of an audience in the stands with lifelike behavior in response to hit accuracy. C) Accurate accounting for the effects of wind conditions on the movement of the ball. D) A baseball field that is textured to differentiate between the grass and the dirt. Correct! Nice Job! Ashlynn is an industrial engineer who is trying to design a safer parachute. She creates a computer simulation of the parachute opening at different heights and in different environmental conditions.What are advantages of running the simulation versus an actual experiment? A) The simulation will not contain any bias that favors one body type over another, while an experiment will be biased. B) The simulation can be run more safely than an actual experiment. C) The simulation will accurately predict the parachute&#39;s safety level, while an experiment may be inaccurate due to faulty experimental design. D) The simulation can test the parachute design in a wide range of environmental conditions that may be difficult to reliably reproduce in an experiment. Try again next time A researcher gathers data about the effect of Advanced Placement®︎ classes on students&#39; success in college and career, and develops a simulation to show how a sequence of AP classes affect a hypothetical student&#39;s pathway.Several school administrators are concerned that the simulation contains bias favoring high-income students, however. A) The simulation is an abstraction and therefore cannot contain any bias. B) The simulation may accidentally contain bias due to the exclusion of details. C) If the simulation is found to contain bias, then it is not possible to remove the bias from the simulation. D) The only way for the simulation to be biased is if the researcher intentionally used data that favored their desired output. Correct! Nice Job! Which of the following ways can we best simulate a spinner? A) Having the user randomly pick a number. B) Using the randint method from the random package over a range of values to generate a value. C) Hard code a random sequence in the source code. D) Use some pattern in nature to return a random value Correct! Nice Job! What module in python could be used to generate random values in simulation experiments? A) random B) randint C) turtle D) tensorflow Correct! Nice Job! Jack is trying to plan his financial future using an online tool. The tool starts off by asking him to input details about his current finances and career. It then lets him choose different future scenarios, such as having children. For each scenario chosen, the tool does some calculations and outputs his projected savings at the ages of 35, 45, and 55.Would that be considered a simulation and why? A) No, it&#39;s not a simulation because it does not include a visualization of the results. B) No, it&#39;s not a simulation because it does not include all the details of his life history and the future financial environment. C) Yes, it&#39;s a simulation because it runs on a computer and includes both user input and computed output. D) Yes, it&#39;s a simulation because it is an abstraction of a real world scenario that enables the drawing of inferences. Try again next time You got a total of 4 out of 6 problems right! . Hack #6 / Challenge - Taking real life problems and implementing them into code . Create your own simulation based on your experiences/knowledge! Be creative! Think about instances in your own life, science, puzzles that can be made into simulations . Some ideas to get your brain running: A simulation that breeds two plants and tells you phenotypes of offspring, an adventure simulation... . import math, random def pendulum(length, angle): h = length - length*math.sin(angle) v = math.sqrt(2*9.8 * h) return v for i in range(5): l = (random.randint(1,10)/10) theta = random.randint(0,179) print(&quot;The speed of a pendulum with length {0} and angle {1} is around {2} m/s&quot;.format(l, theta, pendulum(l, theta))) . The speed of a pendulum with length 0.2 and angle 84 is around 1.0226895644880192 m/s The speed of a pendulum with length 0.9 and angle 70 is around 1.9971400495004945 m/s The speed of a pendulum with length 0.8 and angle 52 is around 0.4579075854418543 m/s The speed of a pendulum with length 0.5 and angle 143 is around 4.4253569953806435 m/s The speed of a pendulum with length 0.2 and angle 70 is around 0.9414608479873579 m/s .",
            "url": "https://ylu-1258.github.io/YLu-Blog/2022/12/12/hw.html",
            "relUrl": "/2022/12/12/hw.html",
            "date": " • Dec 12, 2022"
        }
        
    
  
    
        ,"post28": {
            "title": "Unit 3.14 Libraries Lesson",
            "content": "Purpose: Help students streamline and make their coding experience easier through built in packages and methods from a library Objective: By the end of the lesson, students should be able to fluently use methods from the turtle and math packages, and be able to look up documentation for any python package and us it. . fill in the blanks! . Libraries . Okay, so we&#39;ve learned a lot of code, and all of you now can boast that you can code at least some basic programs in python. But, what about more advanced stuff? What if there&#39;s a more advanced program you don&#39;t know how to make? Do you need to make it yourself? Well, not always. . You&#39;ve already learned about functions that you can write to reuse in your code in previous lessons. But,there are many others who code in python just like you. So why would you do again what someone has already done, and is available for any python user? . Packages allow a python user to import methods from a library, and use the methods in their code. Most libraries come with documentation on the different methods they entail and how to use them, and they can be found with a quick google search methods are used with the following: . Note: a method from a package can only be used after the import statement. . Some libraries are always installed, such as those with the list methods which we have previously discussed. But others require a special python keyword called import. We will learn different ways to import in Challenge 1. . Sometimes we only need to import a single method from the package. We can do that with the word from, followed by the package name, then the word import, then the method. This will alllow you to use the method without mentioning the package&#39;s name, unlike what we did before, however other methods from that package cannot be used. To get the best of both worlds you can use *. . To import a method as an easier name, just do what we did first, add the word as, and write the name you would like to use that package as. . Personal Notes . A star (*) is also known as a wildcard character. It will automatically grab everything under a certain subset. | There are many different python modules for many different tasks, there is no need in reinventing the wheel. | Built-in methods and packages exist within python, and we can directly use those without importing | There are different ways to import a package, which methods such as the normal import statement, from package statement, even importing a method or module underneath a different name. | . Challenge 1: Basic Libraries . Find a python package on the internet and import it | Choose a method from the package and import only the method | import the package as a more convenient name. | from math import sin as s pi = 3.1415926535 print(s(pi/6)) . 0.4999999999870395 . Challenge 2: Turtle . Turtle is a python drawing library which allows you to draw all kinds of different shapes. It&#39;s ofter used to teach beginning python learners, but is really cool to use anywhere. Turtle employs a graphics package to display what you&#39;ve done, but unfortunately it&#39;s kind of annoying to make work with vscode. Use: repl.it Click &quot;+ Create&quot;, and for language, select &quot;Python (with Turtle)&quot; Documentation Task: Have fun with turtle! Create something that uses at least 2 lines of different lengths and 2 turns with different angles, and changes at least one setting about either the pen or canvas. Also use one command that isn&#39;t mentioned on the table below(there are a lot). Paste a screenshot of the code and the drawing from repl.it . Commands . forward(pixels) | . right(degrees) | . left(degrees) | . setpos(x,y) | . speed(speed) | . pensize(size) | . pencolor(color) | . Note: Color should be within quotes, like &quot;brown&quot;, or &quot;red&quot; . Challenge 3: Math . The math package allows for some really cool mathematical methods! . methods Action . ceil(x) | Rounds to the nearest integer greater than a number | . floor(x) | rounds to largest intefer less than or equal to x | . factorial(x) | Returns the factorial, or sequence of products of numbers before the input | . gcd(x) | returns the greatest common denominator of x and y | . lcm(x,y) | returns the least common denominator of x and y | . Challenge: Create a program which asks for a user input of two numbers, and returns the following: . each number rounded up | each number rounded down | the lcm of the rounded down numbers | the gcf of the rounded up numbers | the factorial of each number | something else using the math package! Documentation | . from math import * inpt = input(&quot;Enter your two decimal numbers: &quot;).split(&quot; &quot;) num1, num2 = float(inpt[0]), float(inpt[1]) print(&quot;Ceiling of two numbers:&quot;, ceil(num1), ceil(num2)) print(&quot;Floor of two numbers:&quot;, floor(num1), floor(num2)) print(&quot;Least Common Multiple of two numbers rounded down:&quot;, lcm(floor(num1),floor(num2))) print(&quot;Greatest Common Divisor of two numbers rounded up:&quot;, gcd(ceil(num1),ceil(num2))) print(&quot;Factorial of two numbers rounded down:&quot;, factorial(floor(num1)), factorial(floor(num2))) print(&quot;Sine of the sum two numbers:&quot;, sin(num1 + num2)) . Ceiling of two numbers: 18 20 Floor of two numbers: 17 19 Least Common Multiple of two numbers rounded down: 323 Greatest Common Divisor of two numbers rounded up: 2 Factorial of two numbers rounded down: 355687428096000 121645100408832000 Sine of the sum two numbers: -0.2946716015002508 . Homework: Putting it all together(complete only after the random values lesson) . Option 1: Create a python program which generates a random number between 1 and 10, and use turtle to draw a regular polygon with that many sides. As a hint, remember that the total sum of all the angles in a polygon is (the number of sides - 2) * 180. Note: a regular polygon has all sides and angles the same size. Paste a screenshot of the code and the drawing from repl.it . Option 2: use the &quot;datetime&quot; package, and looking up documentation, create a program to generate 2 random dates and find the number of days between . Extra ideas: customize the settings, draw a picture, or something else! . Item #1 . Program to find the difference between two dates . import datetime import random days_dictionary = { 1: 31, 2: 28, 3: 31, 4: 30, 5: 31, 6: 30, 7: 31, 8: 31, 9: 30, 10: 31, 11: 30, 12: 31, } usr_date = input(&quot;Enter a date MM/DD/YYYY&quot;).split(&quot;/&quot;) usr_day, usr_month, usr_year = int(usr_date[0]), int(usr_date[1]), int(usr_date[2]) def generateRandomDate(year): start = datetime.datetime(year, 1, 1) return start + datetime.timedelta(days=random.randint(1,364)) def calculateDistance(m1,d1,m2, d2): days = 0 if m1 == m2: return d2-d1 days += days_dictionary[m1] - d1 month = m1 + 1 while month != m2: days += days_dictionary[month] month += 1 days += d2 return days random_date = generateRandomDate(usr_year) random_month=random_date.month random_day=random_date.day if (random_month &gt; usr_month or (random_month == usr_month and random_day &gt; usr_day)): print(&quot;There are {0} days between your date of {1}/{2} and the random date of {3}/{4}&quot;.format(calculateDistance(usr_month, usr_day, random_month, random_day), usr_month, usr_day, random_month, random_day)) else: print(&quot;There are {0} days between your date of {1}/{2} and the random date of {3}/{4}&quot;.format(calculateDistance(random_month, random_day, usr_month, usr_day), usr_month, usr_day, random_month, random_day)) . There are 288 days between your date of 13/12 and the random date of 3/30 . Item 2 . Program to convert a random number into a binary and then into hexadecimal . def DecimalToHex(num): hex_correlation = { 0:&quot;0&quot;, 1:&quot;1&quot;, 2:&quot;2&quot;, 3:&quot;3&quot;, 4:&quot;4&quot;, 5:&quot;5&quot;, 6:&quot;6&quot;, 7:&quot;7&quot;, 8:&quot;8&quot;, 9:&quot;9&quot;, 10:&quot;A&quot;, 11:&quot;B&quot;, 12:&quot;C&quot;, 13:&quot;D&quot;, 14:&quot;E&quot;, 15:&quot;F&quot; } res = &quot;&quot; digits = 7 while digits &gt;= 0: if (num - 16 ** digits) &gt;= 0: sub_res = 0 while (num - 16 ** digits) &gt;= 0: sub_res+=1 num-=16 ** digits res = res + str(hex_correlation[sub_res]) else: res = res + &quot;0&quot; digits-=1 return res num = random.randint(0,1000000000) print(&quot;Random Number:&quot;, num) print(&quot;Binary representation of random number:&quot;, bin(num)[2:]) print(&quot;Hexadecimal representation of random number:&quot;, DecimalToHex(num)) . Random Number: 934222060 Binary representation of random number: 110111101011110001100011101100 Hexadecimal representation of random number: 37AF18EC . Extra . Drawing a polygon with random number of sides with turtle .",
            "url": "https://ylu-1258.github.io/YLu-Blog/week-13/big-idea-3/2022/12/10/3.14-Libraries.html",
            "relUrl": "/week-13/big-idea-3/2022/12/10/3.14-Libraries.html",
            "date": " • Dec 10, 2022"
        }
        
    
  
    
        ,"post29": {
            "title": "Vocabulary",
            "content": "Unit 2 Binary/Data Terms . Bits: The smallest unit of computational storage. Stores a boolean value and can be either on or off, 1 or 0. . Bytes: A collection of 8 bits, typically, characters are encoded with a binary byte, such as the ascii encoding system . Nibble: A collection of 4 bits, can store a value between 0 and 15. Due to this nature, a nibble could also be stored within one hexadecimal digit. | . Hexadecimal: The base 16 numbering system. Follows the standard base 10 digits from 0 - 9, and then branches off. In hex, 10 is represented as A, 11 is represented as B and so on until F, which represents 15. . Binary Numbers: . The binary representation of a number. Stored in bits of powers of two. If a bit&#39;s value is 1, that means that that power of two &quot;fits within the original number. If the bit is 0, that means that that number does not fit within the original number. . Unsigned Integer:An integer value without a specified sign. In other words, these integers are always positive or 0. An unsigned int has 32 bits to store a value that can range from 0 to 4,294,967,295 (2^32 - 1) Signed Integer: An integer value with a specified sign. In other words, these integers can be either positive or negative. A signed int also has 32 bits to store a value that can range from -2,147,483,647 (-2^31 + 1) to 2,147,483,647 (2^31 - 1) . Floating Point: A floating point number is a number that contains a decimal point, or some decimal value attached to the number itself. A floating point number typically also consists of 32 bits. In memory, the floating point number is stored with 1 bit for the sign of the number, 8 bits for the exponent, and 23 bits fo the fraction. This type of floating point number is called &quot;Single precision&quot;. . Binary Data Abstraction . The act of taking higher level data and simplifying it through a binary representation of it. EG, color and sound could be broken into numerical values, then decimal representations. . Boolean:A boolean value is one that could be either on or off. Typically, boolean values are represented in programing as either True or False. . print(&quot;A quick demonstration on the behaviors of booleans and their operations&quot;) t = True f = False print(t == f) print(t != f) print(not t) print(t) print(f) . A quick demonstration on the behaviors of booleans and their operations False True False True False . ASCII: A character encoding system based on binary that coordinates letters, special characters, and certain control characters. . Unicode: Another character encoding system that uses 16 bits instead of 8 bits to encode characters. Thus, unlike ASCII, Unicode is able to store much more characters on a broader scale. . RGB: A color model in which the colors red, blue and green are mixed together in order to form different blends and hues of colors. . Data Compression . The act of of restructuring or encoding text in an attempt to reduce the overall size of a particular file or string. . Lossy:An irreversible form of data compression that uses imprecise rounding and approximations to reduce data size. Some examples are JPEG image, MPEG video and MP3 audio formats. Lossless: In the name, when re-extracting or recovering, the original data can almost be perfectly restored from the compression, hence lossless. This type of compression is possible because of various statistical redundancies that occur in files and data. . Unit 3: Algorithms and Programming Terms . Variables: Variables can serve as a reference name for a container of data. Variables can have different data types that correlate to the type of data they store. For instance, integers, chars, or booleans. . var_int = 1337 var_bool = True var_string = &quot;I love coding&quot; print(&quot;Integer Variable: &quot;, var_int) print(&quot;Boolean Variable: &quot;, var_bool) print(&quot;String Variable: &quot;, var_string) . Integer Variable: 1337 Boolean Variable: True String Variable: I love coding . Data Types: A specific family or group of similar data that grant certain interactions or interpretations of it&#39;s data. Common examples are: . Integer | Float | Character | Boolean | String | Array | Enums | . Assignment Operators: Assignment operators are operators that can perform a certain arithmetic operation, while assigning a value to a certain variable. . x = 10 print(&quot;x = &quot;, x) x += 40 print(&quot;x = &quot;, x) x *= 60 print(&quot;x = &quot;, x) x /= 3 print(&quot;x = &quot;, x) x -= 25 print(&quot;x = &quot;, x) x %= 23 print(&quot;x = &quot;, x) . x = 10 x = 50 x = 3000 x = 1000.0 x = 975.0 x = 9.0 . Managing Complexity with Variables . Variables can be used to create references to values and complex structures in our code . Lists:A python list is a data structure that allows for the storage of multiple values sequentially within a single variable. A list is also one of python&#39;s iterables, which are objects that could be traversed via iteration. 2D-Lists: 2-Dimensional lists are essentially lists that have nested lists inside them, hence, two dimensional. in this case, the nested list would count as one whole element of the larger list, and can have it&#39;s own elements to be accessed. . Dictionaries: Dictionaries in python are similar to hashmaps in C++ and other languages. Dictionaries serve to store key-value pairs, which is convenient for matching one value to another. . Class: A class is one of the fundamental building blocks for Object Oriented Programming. A class serves as a blueprint for an Object, which can contain various attributes and methods. . arr1 = [1,2,3,4,5,6,7,8,9,10] # List operations x = arr1[1] # retrieving the second element y = arr1.pop() # popping and storing the last element of hte list within variable y arr1.insert(3,4) # inserting value 4 at index 3 of the list arr1.append(11) # Adding 11 at the end of the list print(&quot;A python list: &quot;, arr1) # Python&#39;s list comprehension syntax arr2 = [i+i-1 for i in range(1,11)] # Creating a list of the first 10 odd numbers print(&quot;First 10 odd numbers: &quot;, arr2) # Dictionaries names = { &quot;Alex&quot;:16, &quot;Evan&quot;:17, &quot;Hasseb&quot;:15, &quot;Nathan&quot;:17, &quot;Lily&quot;:16 } print(names[&quot;Alex&quot;]) # Accessing the value of Alex for key in names.keys(): # Accessing each element in the dictionary print(key + &quot; : &quot; + str(names[key]) + &quot; years old&quot;) # Classes class Vehicle(): def __init__(self, color=&quot;white&quot;, speed=0.2): # Class Constructor self.color = color # Class Attributes self.speed = speed self.pos = 0 self.time = 60 def goForward(self): # Class Methods self.pos + self.speed * 60 def goBackwards(self): self.pos - self.speed * 60 class Car(Vehicle): # Child class inheriting from the vehicle class def __init__(self, color = &quot;red&quot;, speed = &quot;0.8&quot;): self.fuel = 45 def getSpeed(self): print(self.speed) def getFuel(self): print(self.fuel) car1 = Car() car1.getSpeed() . A python list: [1, 2, 3, 4, 4, 5, 6, 7, 8, 9, 11] First 10 odd numbers: [1, 3, 5, 7, 9, 11, 13, 15, 17, 19] 16 Alex : 16 years old Evan : 17 years old Hasseb : 15 years old Nathan : 17 years old Lily : 16 years old 0.2 . Expressions: A combination of operators and terms that form a mathematical relationship. . Comparison Operators: Compares two values and returns a boolean result. . Booleans Expressions and Selection: An expression that would return a boolean value, either True or False. If True, the program will select that value and do something with it, if not, the program will ignore that value . Booleans Expressions and Iteration: Similar to &quot;Boolean Expressions and Selection&quot; (see above), except boolean expressions could be used as conditionals within loop and iterations to create logical progressions. . Truth Tables: A set of rules for binary operations that are always strictly true: . res = 1+5*9/2+6 print(res) # Comparison Operators print(6&gt;1) print(6&lt;1) print(42&gt;=3) print(2349&lt;=34) print(18==18) print(81!=81) # Boolean Expressions and selection arr1 = [123,13,2,192,84,20,153,47,164] arr2 = [i for i in arr1 if i &gt; 100] # Checking each element in arr1 to see if they are greater than 100, if so, add to arr2 print(arr2) # Boolean Expressions and iteration i = 0 while i &lt;= 10: # Checking for if the counter variable is smaller than or equal to 10. print(i) i+=1 . 29.5 True False True False True False [123, 192, 153, 164] 0 1 2 3 4 5 6 7 8 9 10 . Characters: A singular symbol or control character. . Strings: A collection of characters &quot;stringed&quot; together. . Length: The overall number of elements a particular structure has. Elements for a list, and characters for a string . Concatenation: The act of combining two strings together into a larger string . Upper and Lower: String operations in python to convert a string into either all caps or lower case. Useful for controlling case-insensitive user information. . Traversing strings: Strings can be traversed similar to lists, where we can visit each character and perform an action on it . a = &quot;A&quot; # String: b = &quot;Hello World&quot; # Length print(len(b)) # Length of the b string variable # Upper and Lower print(b.upper()) print(b.lower()) # Traversing a string for i in b: print(ord(i)) . 11 HELLO WORLD hello world 72 101 108 108 111 32 87 111 114 108 100 . Conditional Statements . Python If, Elif, Else conditionals: The if statement in python consists of the if keyword and a boolean condition. If the condition evaluates to be true, then the code underneath the if block would be executed. . The elif statement in python consists of the elif keyword and another boolean condition. If the condition for the first if statement evaluates to be false, python will then check for the elif conditions in order. If any one of the conditions meet, python will execute the first True condition and stop the check there. . The else statement in python consists of the else keyword and no conditions. The code under this block is ran if none of the if or elif statements are satisfied. Thus, the else statement could be thought of as a default output. . Nested Selection Statements: Similar to loops, if elif, and else statements could be nested within one another to create more specific and complex checks. . temp = 75 if temp &lt;= 80: if temp &gt;= 60: print(&quot;It is warm outside, Go play!&quot;) elif (temp &lt; 60 and temp &gt; 35): print(&quot;It is slightly chilly, go wear a jacket!&quot;) else: print(&quot;Prepare for snow!&quot;) elif (temp &gt; 80 and temp &lt; 100): print(&quot;It is pretty hot outside, bring some water&quot;) else: print(&quot;It is BURNING!!!!&quot;) . It is warm outside, Go play! . Python Iteration . Python For: A for loop is a loop used for iterating over an array of values or a range of numbers. Typically, a force loop could be used for traversing a list to perform actions on each element, or using numerical values to represent list indices. . While loops with Range with List: A while loop could be used in conjuncture with a range to specify a certain set of numbers to analyze. Typically, this set up could also be used with a list to traverse the list by indexes. A powerful aspect of this approach is that we can set different rules about how our range behaves to have traversing behaviors outside of the standard sequence. . Combining loops with conditionals to Break: An if-elif-else conditional could also be added within for and while loops to control code execution. A condition can be set such that if met, the program will call the break keyword, which will automatically terminal the current iteration and end the loop cycle, continuing with the rest of the program. . Continue: The continue keyword can be used within a loop to skip all remaining code in that cycle and continue on to the next iteration. . arr = [&quot;Bob&quot;, &quot;John&quot;, &quot;Alex&quot;, &quot;Steve&quot;, &quot;Michael&quot;] for i in range(5): print(arr[i]) print(&quot;-&quot;) # While loops with range with list i = 0 while i in range(5): print(arr[i]) i+=1 print(&quot;-&quot;) # Combining loops with conditionals to Break: for i in range(5): if arr[i] == &quot;Alex&quot;: break print(arr[i]) print(&quot;-&quot;) # Continue keyword (We can skip an element) for i in range(5): if arr[i] == &quot;Alex&quot;: continue print(arr[i]) . Bob John Alex Steve Michael - Bob John Alex Steve Michael - Bob John - Bob John Steve Michael . Procedural Abstraction: Complex operations and computing logic could be stored within smaller procedures that when combined, make up the whole program and logic. These smaller bits of code allows for a simpler view of the code and better code structure. . Python Def procedures: Procedures (functions) could be created in python using the def keyword . Parameters: Parameters are the input values for a given procedure or function . Return Values: Return values are the result of each procedure or the desired value that the given procedure passes on to the rest of the program . def fibo(n): # Procedural Abstraction by abstracting the fibonacci sequence with the python def function if n &lt;= 2: # n is our argument return 1 return fibo(n - 1) + fibo(n - 2) # Our return values are either a) 1 if the number is equal to or smaller than 2 # or b) the sum of the 2 previous fibonacci numbers print(fibo(10)) . 55 .",
            "url": "https://ylu-1258.github.io/YLu-Blog/week%2013/2022/12/08/Vocab.html",
            "relUrl": "/week%2013/2022/12/08/Vocab.html",
            "date": " • Dec 8, 2022"
        }
        
    
  
    
        ,"post30": {
            "title": "Unit 3 Sections 14-15 Homework",
            "content": "Create a program that asks the user for a day and then gives them a distance in days between that day and another random day in the year. We have provided you with a possible starter, but you are welcome to change it up if you would like. . import datetime import random days_dictionary = { 1: 31, 2: 28, 3: 31, 4: 30, 5: 31, 6: 30, 7: 31, 8: 31, 9: 30, 10: 31, 11: 30, 12: 31, } usr_date = input(&quot;Enter a date MM/DD/YYYY&quot;).split(&quot;/&quot;) usr_day, usr_month, usr_year = int(usr_date[0]), int(usr_date[1]), int(usr_date[2]) def generateRandomDate(year): start = datetime.datetime(year, 1, 1) return start + datetime.timedelta(days=random.randint(1,364)) def calculateDistance(m1,d1,m2, d2): days = 0 if m1 == m2: return d2-d1 days += days_dictionary[m1] - d1 month = m1 + 1 while month != m2: days += days_dictionary[month] month += 1 days += d2 return days random_date = generateRandomDate(usr_year) random_month=random_date.month random_day=random_date.day if (random_month &gt; usr_month or (random_month == usr_month and random_day &gt; usr_day)): print(&quot;There are {0} days between your date of {1}/{2} and the random date of {3}/{4}&quot;.format(calculateDistance(usr_month, usr_day, random_month, random_day), usr_month, usr_day, random_month, random_day)) else: print(&quot;There are {0} days between your date of {1}/{2} and the random date of {3}/{4}&quot;.format(calculateDistance(random_month, random_day, usr_month, usr_day), usr_month, usr_day, random_month, random_day)) . There are 92 days between your date of 12/12 and the random date of 9/11 . Homework . Given a random decimal number convert it into binary as Extra convert it to hexidecimal as well. . def DecimalToHex(num): hex_correlation = { 0:&quot;0&quot;, 1:&quot;1&quot;, 2:&quot;2&quot;, 3:&quot;3&quot;, 4:&quot;4&quot;, 5:&quot;5&quot;, 6:&quot;6&quot;, 7:&quot;7&quot;, 8:&quot;8&quot;, 9:&quot;9&quot;, 10:&quot;A&quot;, 11:&quot;B&quot;, 12:&quot;C&quot;, 13:&quot;D&quot;, 14:&quot;E&quot;, 15:&quot;F&quot; } res = &quot;&quot; digits = 7 while digits &gt;= 0: if (num - 16 ** digits) &gt;= 0: sub_res = 0 while (num - 16 ** digits) &gt;= 0: sub_res+=1 num-=16 ** digits res = res + str(hex_correlation[sub_res]) else: res = res + &quot;0&quot; digits-=1 return res num = random.randint(0,1000000000) print(&quot;Random Number:&quot;, num) print(&quot;Binary representation of random number:&quot;, bin(num)[2:]) print(&quot;Hexadecimal representation of random number:&quot;, DecimalToHex(num)) . Random Number: 821613606 Binary representation of random number: 110000111110001101010000100110 Hexadecimal representation of random number: 30F8D426 . Extra: . Polygon drawing with turtle .",
            "url": "https://ylu-1258.github.io/YLu-Blog/week-13/big-idea-3/2022/12/03/libraries-hw.html",
            "relUrl": "/week-13/big-idea-3/2022/12/03/libraries-hw.html",
            "date": " • Dec 3, 2022"
        }
        
    
  
    
        ,"post31": {
            "title": "Note Template",
            "content": "Take some additional notes that you would like here for 3.12 and 3.13. We will be looking for additional notes from the presentation. . What are procedures? . Fill in the blanks please: . Procedure: A named group of programming instructions that have parameters and return values. A procedure call will interrupt a series of statements and instead execute the code within the procedure. It will return to the original program call after the procedure hs been fully executed. . Parameters: input values of a procedure . Arguments: the values of the parameters when a procedure is called. . Modularity: Separating a program&#39;s function into independent pieces or blocks while containing all o the parts needed to execute a single aspect. . Procedural Abstraction: Provides a name for a process that allows a procedure to be used. . What are some other names for procedures?: functions . Why are procedures effective?: Procedures are effective in that they allow for simplification and reusability of code. Each piece of code in the program is now more meaningful as some higher level process are simplified or abstracted down into simple procedures. . Challenge 1 below: Add the command that will call the procedure. . decimal = 7 def convertToBinary(decimal): N = 7 res = &quot;&quot; while N &gt;= 0: if (decimal - 2**N) &gt;= 0: res = res +&quot;1&quot; decimal -= 2**N N-=1 else: N-=1 res = res + &quot;0&quot; return res print(convertToBinary(decimal)) . 00000111 . Challenge 2 below: Complete the Min and Max procedure in either JavaScript and Python using the instructions from the JavaScript page. (JavaScript will get you a extra 0.1) . function findMax (numA, numB) { return numA &gt; numB ? numA : numB; } function findMin (numA, numB) { return numA &lt; numB ? numA : numB; } . Above is the code in javascript, but the code wouldn&#39;t run in the notebook, I&#39;ve attached the result in an online debugger and compiler below. . Homework/Hacks: For the hw, you have two options, easy or hard. The easy hack is for a 2.7 + extra work for the full 3. The easy hack is simply creating your own procedure with your own creativity. Since there is a lot of leeway for this one, you must do additional work to get a 3. For the hard hack, below is the start to a character to binary convertor. This is just a template, but the goal is to translate &quot;APCSP&quot; into binary. You can delete the existing code if you want. The only contraint is that you must use a procedure. Doing this will get you a 3. . def stringToBinary(x): res = [convertToBinary(ord(i)) for i in x] return res print(&quot;Binary representation of APCSP&quot;) print(stringToBinary(&quot;APCSP&quot;)) . Binary representation of APCSP [&#39;01000001&#39;, &#39;01010000&#39;, &#39;01000011&#39;, &#39;01010011&#39;, &#39;01010000&#39;] .",
            "url": "https://ylu-1258.github.io/YLu-Blog/2022/12/01/notetemplate.html",
            "relUrl": "/2022/12/01/notetemplate.html",
            "date": " • Dec 1, 2022"
        }
        
    
  
    
        ,"post32": {
            "title": "Boolean expression",
            "content": "Introduction to the Lesson . Booleans are like binary, having 2 possible values, but in Boolean, True is 1 and False is 0. By using boolean operators, a boolean value can be produced. . Important Vocabulary . Boolean | Relational Operator | Logical Operator | . Boolean Operators . Boolean operators produces booleans after it is used between two values . Relational Operators . Relational Operators can work between any two values of the same type known as operands, provided that the type supports such types of operators They consist of the operators ==, !=, &gt;, &lt;, &gt;=, &lt;=, each one working the same way that they work as one would expect, with == being the replacement for equal to, since = is reserved for value assignments. . They also work on other types, such as string or list, and values at each index is compared in order to determine which one is greater for example: &quot;abg&quot;&lt;&quot;acd&quot; returns True, since the computer first goes to a and a, after they are the same, they go to b and c, and since c is &quot;greater&quot; than b, &quot;acd&quot; is greater than &quot;abg&quot; . print(&quot;True:&quot;,4 &gt;= 4) print(&quot;True:&quot;,1 != 0) print(&quot;False:&quot;,7 &lt; 3) print(&quot;True:&quot;,5 &lt;= 6) print(&quot;False:&quot;,7 &gt; 8) print(&quot;True:&quot;,3 == 3) print(&#39;&#39;) # Same as above, but now for other values other than int print(&#39;True:&#39;,&quot;as&quot; &lt;= &quot;as&quot;) print(&quot;False&quot;,True == False) print(&quot;False:&quot;,[2,3,1] &gt; [2,3,1]) print(&quot;True:&quot;,&#39;af&#39; &lt; &#39;bc&#39;) print(&quot;False:&quot;,&#39;ce&#39; &gt;= &#39;cf&#39;) print(&quot;True:&quot;,[1,&#39;b&#39;] != [1,&#39;a&#39;]) print(&#39;&#39;) . True: True True: True False: False True: True False: False True: True True: True False False False: False True: True False: False True: True . Logical Operators . These are operators that works on operand(s) to produce a single boolean result. Examples include and, or, not. . and returns true when both operands around it is true. . or returns true when at least one of its operands is true. . not returns true when the boolean after it is false. . They work the same way in binary, returning a specific value after specific inputs that is shown in a Truth Table . print(&quot;True:&quot;, True or False) print(&quot;False:&quot;, not True) print(&quot;True:&quot;, True or True) print(&quot;False:&quot;, not True) print(&quot;False:&quot;, True and False) print(&quot;True:&quot;, not False) . True: True False: False True: True False: False False: False True: True . Combination of Relational and Logical Operators and the Procedure of Operations . Relational and Logical Operators can work by a set of precedece, a set of order that the computer understands the operations on. . Relational Operators goes first, and then Logical operators work in order of not, and, or . as usual, things in the parenthesis takes precedence over others . print( 3 == 3 and 4 != 6 or 5 &lt; 7 ) . True .",
            "url": "https://ylu-1258.github.io/YLu-Blog/2022/12/01/boolean.html",
            "relUrl": "/2022/12/01/boolean.html",
            "date": " • Dec 1, 2022"
        }
        
    
  
    
        ,"post33": {
            "title": "Unit 3.15 Random Values Student Copy",
            "content": "Purpose/Objectives: Teach student how to implement randomness into their code to make their code simulate real life situations. . In this lesson students will learn: . How to import random to python | How to use random with a list or number range | How to code randomness in everyday scenarios | . ADD YOUR ADDITIONAL NOTES HERE: 1) random values are incredibly useful as can be seen in the simulations lecture 2) we can use random values to simulate events such as coinflips or card games. 3) Another way that they are powerful is that they allow us to explore different combinations and probabilities. . What are Random Values? . Random Values are a number generated using a large set of numbers and a mathematical algorithm which gives equal probability to all number occuring . Each Result from randomization is equally likely to occur Using random number generation in a program means each execution may produce a different result . What are Examples of Random outputs in the world? Add a few you can think of. . Ex: Marbles | Cryptography | Games | Statistical Sampling | . Why do we need Random Values for code? . Random values can be used in coding: . import random random_number = random.randint(1,100) print(random_number) . 84 . def randomlist(): list = [&quot;apple&quot;, &quot;banana&quot;, &quot;cherry&quot;, &quot;blueberry&quot;] element = random.choice(list) print(element) randomlist() . blueberry . Real Life Examples: Dice Roll . import random for i in range(3): roll = random.randint(1,6) print(&quot;Roll &quot; + str(i + 1) + &quot;:&quot; + str(roll)) . Roll 1:2 Roll 2:2 Roll 3:5 . Challenge #1 . Write a function that will a simulate a coinflip and print the output . def coinflip(): return &quot;Heads&quot; if random.randint(1,2) == 1 else &quot;Tails&quot; print(coinflip()) . Tails . EXTRA: Create a function that will randomly select 5 playing Cards and check if the 5 cards are a Royal Flush . cards = [i for i in range(52)] def selectCard(cards): res = random.choice(cards) cards.remove(res) return res deck = [selectCard(cards) for i in range(5)] if set(deck) == set([0,1,2,3,4]): print(&quot;Royal Flush Detected&quot;) elif set(deck) == set([5,6,7,8,9]): print(&quot;Royal Flush Detected&quot;) elif set(deck) == set([10,11,12,13,14]): print(&quot;Royal Flush Detected&quot;) elif set(deck) == set([15,16,17,18,19]): print(&quot;Royal Flush Detected&quot;) else: print(&quot;No Royal Flush&quot;) . Royal Flush Detected . Homework . Given a random decimal number convert it into binary as Extra convert it to hexidecimal as well. . def DecimalToHex(num): hex_correlation = { 0:&quot;0&quot;, 1:&quot;1&quot;, 2:&quot;2&quot;, 3:&quot;3&quot;, 4:&quot;4&quot;, 5:&quot;5&quot;, 6:&quot;6&quot;, 7:&quot;7&quot;, 8:&quot;8&quot;, 9:&quot;9&quot;, 10:&quot;A&quot;, 11:&quot;B&quot;, 12:&quot;C&quot;, 13:&quot;D&quot;, 14:&quot;E&quot;, 15:&quot;F&quot; } res = &quot;&quot; digits = 7 while digits &gt;= 0: if (num - 16 ** digits) &gt;= 0: sub_res = 0 while (num - 16 ** digits) &gt;= 0: sub_res+=1 num -= 16 ** digits res = res + str(hex_correlation[sub_res]) else: res = res + &quot;0&quot; digits-=1 return res num = random.randint(0,1000000000) print(&quot;Random Number:&quot;, num) print(&quot;Binary representation of random number:&quot;, bin(num)[2:]) print(&quot;Hexadecimal representation of random number:&quot;, DecimalToHex(num)) . Random Number: 345791430 Binary representation of random number: 10100100111000101101111000110 Hexadecimal representation of random number: 149C5BC6 .",
            "url": "https://ylu-1258.github.io/YLu-Blog/week-13/big-idea-3/2022/12/01/StudentCopyRandomValues.html",
            "relUrl": "/week-13/big-idea-3/2022/12/01/StudentCopyRandomValues.html",
            "date": " • Dec 1, 2022"
        }
        
    
  
    
        ,"post34": {
            "title": "Homework",
            "content": "Homework/Hacks . our homework we have decided for a decimal number to binary converter. You must use conditional statements within your code and have a input box for where the decimal number will go. This will give you a 2.7 out of 3 and you may add anything else to the code to get above a 2.7. . Below is an example of decimal number to binary converter which you can use as a starting template. . def DecimalToBinary(num): strs = &quot;&quot; while num: # if (num &amp; 1) = 1 if (num &amp; 1): strs += &quot;1&quot; # if (num &amp; 1) = 0 else: strs += &quot;0&quot; # right shift by 1 num &gt;&gt;= 1 return strs ## making a function to convert a given character into it&#39;s ascii binary code, using the if conditional statement and ord() function def CharToBinary(char): return DecimalToBinary(ord(char)) if len(char) == 1 else False def DecimalToOctal(num): res = &quot;&quot; digits = 7 while digits &gt;= 0: if (num - 8 ** digits) &gt;= 0: sub_res = 0 while (num - 8 ** digits) &gt;= 0: sub_res+=1 num -= 8 ** digits res = res + str(sub_res) else: res = res + &quot;0&quot; digits-=1 return res def DecimalToHex(num): hex_correlation = { 0:&quot;0&quot;, 1:&quot;1&quot;, 2:&quot;2&quot;, 3:&quot;3&quot;, 4:&quot;4&quot;, 5:&quot;5&quot;, 6:&quot;6&quot;, 7:&quot;7&quot;, 8:&quot;8&quot;, 9:&quot;9&quot;, 10:&quot;A&quot;, 11:&quot;B&quot;, 12:&quot;C&quot;, 13:&quot;D&quot;, 14:&quot;E&quot;, 15:&quot;F&quot; } res = &quot;&quot; digits = 7 while digits &gt;= 0: if (num - 16 ** digits) &gt;= 0: sub_res = 0 while (num - 16 ** digits) &gt;= 0: sub_res+=1 num -= 16 ** digits res = res + str(hex_correlation[sub_res]) else: res = res + &quot;0&quot; digits-=1 return res # function to reverse the string def reverse(strs): print(strs[::-1]) # Driver Code num = int(input(&quot;What is your desired number? &quot;)) print(&quot;Binary of num {0} is: {1}&quot;.format(num, DecimalToBinary(num))) char = input(&quot;What is your desired character? &quot;) print(&quot;Binary of character {0} is: {1}&quot;.format(char, CharToBinary(char))) O_num = int(input(&quot;what is your desired number? &quot;)) print(&quot;Octal representation of number {0} is: {1}&quot;.format(O_num, DecimalToOctal(O_num))) H_num = int(input(&quot;what is your desired number? &quot;)) print(&quot;Hexadecimal representation of number {0} is: {1}&quot;.format(H_num, DecimalToHex(H_num))) . Binary of num 198 is: 01100011 Binary of character B is: 0100001 Octal representation of number 10394 is: 00024232 Hexadecimal representation of number 123908378 is: 0762B11A .",
            "url": "https://ylu-1258.github.io/YLu-Blog/week%2013/2022/11/30/homework.html",
            "relUrl": "/week%2013/2022/11/30/homework.html",
            "date": " • Nov 30, 2022"
        }
        
    
  
    
        ,"post35": {
            "title": "BVariables Assignments, and Data Abstraction",
            "content": "Unit 3.1: Variables and Assignments . Variables are abstractions within programs which represent a value | Values can be individual data points or a list/collection that contains many data values | Types of data: numbers, Booleans (T/F), lists, and strings | . Python . To define variables, just establish a variable name, and then use the equal sign to set it equal to a value, wheter it be a string, character, list, or boolean. . Javascript . global variables can be defined with the var keyword . Constant variables could be defined with the const keyword . Local variables could be defined with the let keyword. . Unit 3.2: Data Abstraction . A list is made up of elements organized in a specific order | An element is a unique, individual value in a list | A string differs from a list as it is a sequence of characters rather than elements | Data abstraction uses a list by taking specific data elements from a list and organizing into a whole, less complex representation of the values such as a table | . Python . In python, lists could be used to organize similar data structures. . Lists could be split with the .split() method. The deliminator for the splitting operation could be set within the parenthesis. By default, the deliminator are whitespace characters. . Lists could also be joined into a variable with the .join() method. A deliminator could also be set for such a list. . ### .",
            "url": "https://ylu-1258.github.io/YLu-Blog/markdown/apcsp/tri2/2022/11/28/Variables-Assignments-and-Data-Abstractions-copy.html",
            "relUrl": "/markdown/apcsp/tri2/2022/11/28/Variables-Assignments-and-Data-Abstractions-copy.html",
            "date": " • Nov 28, 2022"
        }
        
    
  
    
        ,"post36": {
            "title": "Challenges and Homework",
            "content": "data = [104, 101, 4, 105, 308, 103, 5, 107, 100, 306, 106, 102, 108] # list of the different numerical values min_valid = 100 # minimum value max_valid = 200 # maximum value for i in range(len(data)): if data[i] &lt; min_valid: print(data[i], i) elif data[i] &gt; max_valid: print(data[i], i) . 4 2 308 4 5 6 306 9 . albums = [ (&quot;Welcome to my Nightmare&quot;, &quot;Alice Cooper&quot;, 1975, # First album list [ (1, &quot;Welcome to my Nightmare&quot;), (2, &quot;Devil&#39;s Food&quot;), (3, &quot;The Black Widow&quot;), (4, &quot;Some Folks&quot;), (5, &quot;Only Women Bleed&quot;), ] ), (&quot;Bad Company&quot;, &quot;Bad Company&quot;, 1974, # Second album list [ (1, &quot;Can&#39;t Get Enough&quot;), (2, &quot;Rock Steady&quot;), (3, &quot;Ready for Love&quot;), (4, &quot;Don&#39;t Let Me Down&quot;), (5, &quot;Bad Company&quot;), (6, &quot;The Way I Choose&quot;), (7, &quot;Movin&#39; On&quot;), (8, &quot;Seagull&quot;), ] ), (&quot;Nightflight&quot;, &quot;Budgie&quot;, 1981, [ (1, &quot;I Turned to Stone&quot;), (2, &quot;Keeping a Rendezvous&quot;), (3, &quot;Reaper of the Glory&quot;), (4, &quot;She Used Me Up&quot;), ] ), (&quot;More Mayhem&quot;, &quot;Imelda May&quot;, 2011, [ (1, &quot;Pulling the Rug&quot;), (2, &quot;Psycho&quot;), (3, &quot;Mayhem&quot;), (4, &quot;Kentish Town Waltz&quot;), ] ), ] album_id = int(input(&quot;What is your preferred album?&quot;)) song_id = int(input(&quot;What is your preferred song?&quot;)) print(&quot;Playing &quot;{0} &quot; from album &quot;{1} &quot;&quot;.format(albums[album_id-1][3][song_id-1][1],albums[album_id-1][0])) . Playing &#34;She Used Me Up&#34; from album &#34;Nightflight&#34; .",
            "url": "https://ylu-1258.github.io/YLu-Blog/week%2013/2022/11/28/Challenges-and-Homework.html",
            "relUrl": "/week%2013/2022/11/28/Challenges-and-Homework.html",
            "date": " • Nov 28, 2022"
        }
        
    
  
    
        ,"post37": {
            "title": "Lists and Iteration Homework",
            "content": "Overview and Notes: 3.10 - Lists . Make sure you complete the challenge in the challenges section while we present the lesson! | . Add your OWN Notes for 3.10 here: . Lists are a collection of related data and are incredibly useful in storing large amounts of data. | lists have indexes which represents the location of an element in the list. Most programming languages use 0-based indexing, but others could use 1-based indexing. FYI, Collegeboard uses 1-based indexing. | Fill out the empty boxes: . Pseudocode Operation Python Syntax Description . aList[i] | aList[i] | Accesses the element of aList at index i | . x ← aList[i] | x = aList[i] | Assigns the element of aList at index i to a variable &#39;x&#39; | . aList[i] ← x | aList[i] = x | Assigns the value of a variable &#39;x&#39; to the element of a List at index i | . aList[i] ← aList[j] | aList[i] = aList[j] | Assigns value of aList[j] to aList[i] | . INSERT(aList, i, value) | aList.insert(i, value) | value is placed at index i in aList. Any element at an index greater than i will shiftone position to the right. | . APPEND(aList, value) | aList.append(value) | Value is appended as an element to the end of the list. | . REMOVE(aList, i) | aList.pop(i)ORaList.remove(value) | Removes item at index i and any values at indices greater than i shift to the left. Length of aList decreased by 1. | . Overview and Notes: 3.8 - Iteration . Add your OWN Notes for 3.8 here: . Iteration is repetition: repeating a set of actions based on a set of conditions. | Make sure that you don&#39;t have a dead loop, so that the program doesn&#39;t keep repeating | We want to automate certain process with loops, this way we reduce unprofessional, repetitive, and duplicate code. | For loops are an example of an iterative loop that could be used to check values within a list, we can establish a variable in a for loop to pass values to the rest of our code, | We can print multi-dimensional lists: petlist = [(&quot;Dogs&quot;,1),(&quot;Cats&quot;, 2), (&quot;Fish&quot;, 0)] for pet , number in petlist: print(pet + &quot; : &quot; + number) | We can have nested loops inside of a loop. | We can also loop with recursion, if we call a function within itself. | Homework Assignment . Instead of us making a quiz for you to take, we would like YOU to make a quiz about the material we reviewed. . We would like you to input questions into a list, and use some sort of iterative system to print the questions, detect an input, and determine if you answered correctly. There should be at least five questions, each with at least three possible answers. . You may use the template below as a framework for this assignment. . from random import shuffle questions = [ (&quot;What is a list?&quot;, &quot;A) A data structure that could store large amounts of data.&quot;, &quot;B) A FIFO structure that outputs the first value that was inputted.&quot;, &quot;C) Your Grocery list for shopping on saturday night.&quot;, &quot;A&quot;), (&quot;How does a while loop work?&quot;, &quot;A) Recursively executes a function within it&#39;s self.&quot;, &quot;B) Checks for a certain condition and executes a block of code.&quot;, &quot;C) Iterates over each element in a list by assigning each element a variable.&quot;, &quot;B&quot;), (&quot;What is a list index?&quot;, &quot;A) A representation of the location of a given list element.&quot;, &quot;B) The location of the list in the computer memory&quot;, &quot;C) A general term for the elements in a list&quot;, &quot;A&quot;), (&quot;What list indexing convention is typically used for most programming languages?&quot;, &quot;A) 1 based indexing&quot;, &quot;B) keyword indexing&quot;, &quot;C) 0 based indexing&quot;, &quot;C&quot;), (&quot;What list indexing convention is typically used for collegeboard&#39;s lists?&quot;, &quot;A) pseudo indexing&quot;, &quot;B) 1 based indexing&quot;, &quot;C) 0 based indexing&quot;, &quot;B&quot;) ] correct = 0 total = 5 shuffle(questions) def printQuestion(tup): print(tup[0]) print(tup[1]) print(tup[2]) print(tup[3]) def questionloop(): correct = 0 for i in range(len(questions)): printQuestion(questions[i]) ans = input(&quot;What is your answer? &quot;) if ans == questions[i][4]: print(&quot;Correct! Nice Job!&quot;) correct+=1 else: print(&quot;Try again next time&quot;) return correct correct = questionloop() print(&quot;You got a total of {0} out of {1} problems right!&quot;.format(correct, total)) . What list indexing convention is typically used for most programming languages? A) 1 based indexing B) keyword indexing C) 0 based indexing Correct! Nice Job! How does a while loop work? A) Recursively executes a function within it&#39;s self. B) Checks for a certain condition and executes a block of code. C) Iterates over each element in a list by assigning each element a variable. Correct! Nice Job! What is a list? A) A data structure that could store large amounts of data. B) A FIFO structure that outputs the first value that was inputted. C) Your Grocery list for shopping on saturday night. Correct! Nice Job! What list indexing convention is typically used for collegeboard&#39;s lists? A) pseudo indexing B) 1 based indexing C) 0 based indexing Try again next time What is a list index? A) A representation of the location of a given list element. B) The location of the list in the computer memory C) A general term for the elements in a list Try again next time You got a total of 3 out of 5 problems right! . Extra things I did . I randomized the quiz questions so that the order is different each time | I displayed the user&#39;s score out of 5 at the very end. | Hacks . Here are some ideas of things you can do to make your program even cooler. Doing these will raise your grade if done correctly. . Add more than five questions with more than three answer choices | Randomize the order in which questions/answers are output | At the end, display the user&#39;s score and determine whether or not they passed | . Challenges . Important! You don&#39;t have to complete these challenges completely perfectly, but you will be marked down if you don&#39;t show evidence of at least having tried these challenges in the time we gave during the lesson. . 3.10 Challenge . Follow the instructions in the code comments. . grocery_list = [&#39;apples&#39;, &#39;milk&#39;, &#39;oranges&#39;, &#39;carrots&#39;, &#39;cucumbers&#39;] # Print the fourth item in the list print(grocery_list[3]) # Now, assign the fourth item in the list to a variable, x and then print the variable x = grocery_list[3] print(x) # Add these two items at the end of the list : umbrellas and artichokes grocery_list.append(&quot;umbrellas&quot;) grocery_list.append(&quot;artichokes&quot;) # Insert the item eggs as the third item of the list grocery_list.insert(2, &quot;eggs&quot;) # Remove milk from the list grocery_list.remove(&quot;milk&quot;) # Assign the element at the end of the list to index 2. Print index 2 to check grocery_list[2] = grocery_list[-1] print(grocery_list[2]) # Print the entire list, does it match ours ? print(grocery_list) # Expected output # carrots # carrots # artichokes # [&#39;apples&#39;, &#39;eggs&#39;, &#39;artichokes&#39;, &#39;carrots&#39;, &#39;cucumbers&#39;, &#39;umbrellas&#39;, &#39;artichokes&#39;] . carrots carrots artichokes [&#39;apples&#39;, &#39;eggs&#39;, &#39;artichokes&#39;, &#39;carrots&#39;, &#39;cucumbers&#39;, &#39;umbrellas&#39;, &#39;artichokes&#39;] . 3.8 Challenge . Create a loop that converts 8-bit binary values from the provided list into decimal numbers. Then, after the value is determined, remove all the values greater than 100 from the list using a list-related function you&#39;ve been taught before. Print the new list when done. . Once you&#39;ve done this with one of the types of loops discussed in this lesson, create a function that does the same thing with a different type of loop. . binarylist = [&quot;01001001&quot;, &quot;10101010&quot;, &quot;10010110&quot;, &quot;00110111&quot;, &quot;11101100&quot;, &quot;11010001&quot;, &quot;10000001&quot;] def binary_convert(binary): N = 7 res = 0 for bit in binary: if bit == &quot;1&quot;: res+=2**int(N) N-=1 return res #use this function to convert every binary value in binarylist to decimal #afterward, get rid of the values that are greater than 100 in decimal #when done, print the results binarylist = [binary_convert(i) for i in binarylist if binary_convert(i) &lt; 100] binarylist . [73, 55] .",
            "url": "https://ylu-1258.github.io/YLu-Blog/2022/11/26/listanditerationhomework.html",
            "relUrl": "/2022/11/26/listanditerationhomework.html",
            "date": " • Nov 26, 2022"
        }
        
    
  
    
        ,"post38": {
            "title": "Big idea 3!",
            "content": "3.3 Expressions(Show video 1 and 3) . Vocab: fill in the blanks . the symbol for exponent is ** the symbol for addition is + the symbol for subtraction is - the symbol for multiplication is * the symbol for division is / the symbol for modulus is % an algorithm is A logical set of instructions for the computer to interpret and execute to produce a desired output from given data . Sequencing Practice: the code below does not follow the intended steps below. change the code so that it does so. . divide value1 by 10(value1 = 5) | multiply 2 from the result of the step 1 | subtract 4 from the result of the step 2 | print the result of step 3 | value1 = 5 value2 = value1/10 #step 1 value3 = value2*2 #step 2 value4 = value3-4 #step 3 print(value4) . -3.0 . Selection/Iteration Practice: Create a function to print ONLY the numbers of numlist that are divisble by 3. Hint: use the MOD operator (a % b) to find the remainder when a is divided by b. . numlist = [&quot;3&quot;,&quot;4&quot;,&quot;9&quot;,&quot;76&quot;,&quot;891&quot;] for i in range(len(numlist)): if int(numlist[i])%3 == 0: print( numlist[i] + &quot; is divisible by 3&quot;) else: continue . 3 is divisible by 3 9 is divisible by 3 891 is divisible by 3 . Homework/Binary Adaptation: Create a python function that will convert a decimal number 1-255 to binary using mathematical operations and powers of 2. Challenge: add frontend with javascript or html. . def convert(num): N = 7 res = &quot;&quot; while N &gt;= 0: if (num-2**N)&gt;=0: res=res+&quot;1&quot; num-=2**N else: res=res+&quot;0&quot; N-=1 return res print(convert(243)) . 11110011 . 3.4 Strings(Show video 1) . Vocab: fill in the blanks using the video . Index is a number representing a position, like a character&#39;s position in a string or a string&#39;s position in a list. Concatenation is The act of combining or adding strings Length is How long a given object is. Characters in python, bytes in cpp, etc. A substring is A smaller string of a larger parent string . What is psuedocode? . Pseudocode is writing out a program in plain language with keywords that are used to refer to common coding concepts. . Can you think of some benefits of using pseudocode prior to writing out the actual code? . Choose an everyday activity | Imagine that you are providing instructions for this activity to a person who has never done it before | Challenge someone to do the steps you wrote out | Ex. Brushing Teeth . Pick up your toothbrush | Rinse toothbrush | Pick up toothpaste | Place toothpaste on the toothbrush | Rinse toothbrush again | Brush teeth in a circular motion | Spit | Wash mouth | Rinse toothbrush | You have brushed your teeth! | Substring/Length Practice: change the print functions to print &quot;hello&quot;, &quot;bye&quot;, and the string length . #the substring will have the characters including the index &quot;start&quot; to the character BEFORE the index &quot;end&quot; #len(string) will print the length of string string = &quot;hellobye&quot; print(string[0:5]) print(string[5:]) print(len(string)) . hello bye 8 . Concatenation Practice: combine string1 and string2 to make string3, then print string3. . string1 = &quot;computer&quot; string2 = &quot;science&quot; string3 = string1 + string2 print(string3) . computerscience . Homework/List Adaptation: create a function that prints the name of each string in the list and the string&#39;s length. Challenge: add frontend with javascript or html. . names = [&quot;jaden&quot;,&quot;max&quot;,&quot;dylan&quot;,&quot;orlando&quot;] def length(list): for i in names: print(i,len(i)) length(names) . jaden 5 max 3 dylan 5 orlando 7 . Stuck? . Check out what we did. | . Have any questions? . Ask us if you have any questions! | .",
            "url": "https://ylu-1258.github.io/YLu-Blog/week-13/big-idea-3/2022/11/18/expressions-and-strings.html",
            "relUrl": "/week-13/big-idea-3/2022/11/18/expressions-and-strings.html",
            "date": " • Nov 18, 2022"
        }
        
    
  
    
        ,"post39": {
            "title": "Binary Calculator",
            "content": "Binary Math with Conversions . Plus Binary Octal Hexadecimal Decimal Minus . +1 | 00000000 | 0 | 0 | 0 | -1 | . Turn on | Turn on | Turn on | Turn on | Turn on | Turn on | Turn on | Turn on | . | | | | | | | | .",
            "url": "https://ylu-1258.github.io/YLu-Blog/markdown/apcsp/2022/11/16/XOR-calculator.html",
            "relUrl": "/markdown/apcsp/2022/11/16/XOR-calculator.html",
            "date": " • Nov 16, 2022"
        }
        
    
  
    
        ,"post40": {
            "title": "Binary Math",
            "content": "Binary Math with Conversions . Plus Binary Octal Hexadecimal Decimal Character Minus . +1 | 00000000 | 0 | 0 | 0 | NULL | -1 | . Turn on | Turn on | Turn on | Turn on | Turn on | Turn on | Turn on | Turn on | . | | | | | | | | . Plus Binary Hexadecimal RGB Minus . +1 | 00000000 | 00 | 0 | -1 | . +1 | 00000000 | 00 | 0 | -1 | . +1 | 00000000 | 00 | 0 | -1 | . Turn on | Turn on | Turn on | Turn on | Turn on | Turn on | Turn on | Turn on | . | | | | | | | | . Turn on | Turn on | Turn on | Turn on | Turn on | Turn on | Turn on | Turn on | . | | | | | | | | . Turn on | Turn on | Turn on | Turn on | Turn on | Turn on | Turn on | Turn on | . | | | | | | | | . Binary Search Demonstration . Input your array: Input your target value: Search!",
            "url": "https://ylu-1258.github.io/YLu-Blog/markdown/apcsp/2022/11/15/Binary.html",
            "relUrl": "/markdown/apcsp/2022/11/15/Binary.html",
            "date": " • Nov 15, 2022"
        }
        
    
  
    
        ,"post41": {
            "title": "APCSP Final MC Analysis",
            "content": "Final MC results . Overall, the final was pretty easy and smooth. However, I think that there are definitely areas of improvement for me to not only increase my acccuracy but also my efficiency at these problems. . Incorrect Answers and Writeup . Writeup: Originally, I answered that the only strings of characters could be represented by digital binary sequences (i.e. the ASCII text encoding), and neglected the possibility of storing colors or audio recordings as binary. . However, after consulting some online resources and review the collegeboard 2.1 videos, I realized that almost everything in our world, weather it be abstract ideas or tangible objects, could be represented and abstracted into simple binary sequences. For instance, color may be encoded as a hex value or RGB code as we’ve experimented in our CSS style sheets for our project. In other cases, audio recordings may also be stored as binary sequence. For example, we can take the sound waves of a recording and simplify it into a numerical sequence. We could then parse that sequence through a computer, by transforming this numerical wave into sequences of binary bits instead. In fact, this is how most Machine Learning algorithms process sound for speech recognition and voice commands. . Overall conclusion and analysis . Overall, I feel like I have gained a strong grasp of the contents taught in this trimester about error checking, programming collaboration, computer networking, and etc. However I do think I could review some other topics on collegeboard such as data abstraction (binary representation), and some other topics to truly master my computer science knowledge and skills for trimester 2 of this course. .",
            "url": "https://ylu-1258.github.io/YLu-Blog/markdown/apcsp/2022/11/08/Final-MC.html",
            "relUrl": "/markdown/apcsp/2022/11/08/Final-MC.html",
            "date": " • Nov 8, 2022"
        }
        
    
  
    
        ,"post42": {
            "title": "APCSP N@tM",
            "content": "Goals . The Night at the Museum was an event coordinated by the visual arts and computer science departments to create an opportunity for students to observe and learn from the projects of others while benefiting from their gains. The projects displayed by the Tri 1 APCSP classes utilized two web applications with one designated for Frontend display and one for Backend processing. The two endpoints were linked up via an API developed on the backend, which allows for communication between backend and frontend and ultimately interactive webpages. . Our Project . We created a minesweeper game that boasts an ability that allows for individual users to log in into an account that they can create on the home page. This system is created by our backend API which stores users and their credentials and information in a 2 dimensional python dictionary. Ultimately, this allows for our user to play the minesweeper game while allowing them to freely store their high scores so long as the website is up. . Others . Alan, Steven, Ederick, Noor, Liav - Created a calendar that displays date and weather information. Allows users to input and store events on any date and incorporated city-wide weather data via an API. . Luke - Created a Quiz program that compiles random questions from a collection of geography, math and SAT prompts. Has a well-designed about page that links to everyone’s fastpage blogs. . Paaras, Samarth, AJ, Haoxuan - Developed an api that grabs some motivational qoutes from the internet and allows users to vote on their favorite quotes through number counter. . Peacock Justin, James, Shruti, Joslyn- Created a tetris game and used an astronomy api to create a table based on city names to provide location details such as longitude latitude and moon traits. . Advay, Krishiv, Shivansh, Dhruva, Prasith - Another motivational quotes generator with added motivational features for fitness and atheletic goals. . Jonathan, Martin, Abdulla, Leonard - Created a program to return basketball stats of NBA teams, and used an API to get the stats for each team. . Lyntax Aniket, Soham, Ryan, Lucas - One api with a dictionary hosted on a flask server that has words and definitions so users can imput it against a word. Which serves to play a hangman game. Which also eliminates letters and dynamically renders the charector. . Ananya, Sreeja, Aliya, Clair - Created a Wordle game that uses a custom api and randomly generates a game. Also checks if words are valid are not and informs the user if they are incorrect or not. . Raunak, Tanay, Yuri, Sachit, Harsha - Created a clock alarm and stopwatch, using a custom api to request data from a variety of time of timezones to display times. . Lily, Ekam, Ishi, Shreyas - Uses a custom dictionary api of different words and their definitions to allow users to find them and a word of the day that changes. Along with a feedback to adapt the user experience. . Aiden, ahad, dash, sabine - Innovative api that has games such as tetris, blackjack, cookie clicker, and a pokemon game with custom to log losses wins, correct answers wrong and other game data. . Keira, Zeen, Ellie, Giannina - Made a custom custmoer survey along with a quiz that has three different quizzes for calculus, physics, and satstics, with solutions. Display is randomized with a backend database along with a quiz summary and a customer service api that prompts users with questions to save them. . Safin, Navan, Alex, Kalani - Made a motivational quotes website that allows users to like and dislike posts and also giving them the ability to comment on each individual quote .",
            "url": "https://ylu-1258.github.io/YLu-Blog/markdown/apcsp/2022/11/07/N@tM.html",
            "relUrl": "/markdown/apcsp/2022/11/07/N@tM.html",
            "date": " • Nov 7, 2022"
        }
        
    
  
    
        ,"post43": {
            "title": "Python Web API Endpoints using Jokes",
            "content": "Model for API . We will begin our journey into APIs by creating and thinking about data. We have learned about Python Lists and dictionaries. In this data example, we are going to make &quot;the best computer jokes ever ;)&quot; and serve them over the Internet. The ultimate objective is to allow our viewers to provide a like or dislike on each of our jokes. . This code planning begins by coming up with some jokes and defining a data &quot;model&quot; to keep and manage the jokes. . jokes_data contains a list of dictionary records containing joke and reactions:haha or boohoo - joke_list contains collection of jokes we will put into jokes_data | . | Next comes some functions to interact with our jokes . def initJokes(): initializes jokes_data | def getJokes(): returns the complete list of jokes | def getJoke(): returns a single joke from our list | ... many more function can be examined by reading comments below ... | . | . import random jokes_data = [] joke_list = [ &quot;If you give someone a program... you will frustrate them for a day; if you teach them how to program... you will &quot; &quot;frustrate them for a lifetime.&quot;, &quot;Q: Why did I divide sin by tan? A: Just cos.&quot;, &quot;UNIX is basically a simple operating system... but you have to be a genius to understand the simplicity.&quot;, &quot;Enter any 11-digit prime number to continue.&quot;, &quot;If at first you don&#39;t succeed; call it version 1.0.&quot;, &quot;Java programmers are some of the most materialistic people I know, very object-oriented&quot;, &quot;The oldest computer can be traced back to Adam and Eve. It was an apple but with extremely limited memory. Just &quot; &quot;1 byte. And then everything crashed.&quot;, &quot;Q: Why did Wi-Fi and the computer get married? A: Because they had a connection&quot;, &quot;Bill Gates teaches a kindergarten class to count to ten. 1, 2, 3, 3.1, 95, 98, ME, 2000, XP, Vista, 7, 8, 10.&quot;, &quot;Q: What’s a aliens favorite computer key? A: the space bar!&quot;, &quot;There are 10 types of people in the world: those who understand binary, and those who don’t.&quot;, &quot;If it wasn&#39;t for C, we’d all be programming in BASI and OBOL.&quot;, &quot;Computers make very fast, very accurate mistakes.&quot;, &quot;Q: Why is it that programmers always confuse Halloween with Christmas? A: Because 31 OCT = 25 DEC.&quot;, &quot;Q: How many programmers does it take to change a light bulb? A: None. It’s a hardware problem.&quot;, &quot;The programmer got stuck in the shower because the instructions on the shampoo bottle said: Lather, Rinse, Repeat.&quot;, &quot;Q: What is the biggest lie in the entire universe? A: I have read and agree to the Terms and Conditions.&quot;, &#39;An SQL statement walks into a bar and sees two tables. It approaches, and asks may I join you?&#39; ] # Initialize jokes def initJokes(): # setup jokes into a dictionary with id, joke, haha, boohoo item_id = 0 for item in joke_list: jokes_data.append({&quot;id&quot;: item_id, &quot;joke&quot;: item, &quot;haha&quot;: 0, &quot;boohoo&quot;: 0}) item_id += 1 # prime some haha responses for i in range(200): id = getRandomJoke()[&#39;id&#39;] addJokeHaHa(id) # prime some haha responses for i in range(50): id = getRandomJoke()[&#39;id&#39;] addJokeBooHoo(id) # Return all jokes from jokes_data def getJokes(): return(jokes_data) # Joke getter def getJoke(id): return(jokes_data[id]) # Return random joke from jokes_data def getRandomJoke(): return(random.choice(jokes_data)) # Liked joke def favoriteJoke(): best = 0 bestID = -1 for joke in getJokes(): if joke[&#39;haha&#39;] &gt; best: best = joke[&#39;haha&#39;] bestID = joke[&#39;id&#39;] return jokes_data[bestID] # Jeered joke def jeeredJoke(): worst = 0 worstID = -1 for joke in getJokes(): if joke[&#39;boohoo&#39;] &gt; worst: worst = joke[&#39;boohoo&#39;] worstID = joke[&#39;id&#39;] return jokes_data[worstID] # Add to haha for requested id def addJokeHaHa(id): jokes_data[id][&#39;haha&#39;] = jokes_data[id][&#39;haha&#39;] + 1 return jokes_data[id][&#39;haha&#39;] # Add to boohoo for requested id def addJokeBooHoo(id): jokes_data[id][&#39;boohoo&#39;] = jokes_data[id][&#39;boohoo&#39;] + 1 return jokes_data[id][&#39;boohoo&#39;] # Pretty Print joke def printJoke(joke): print(joke[&#39;id&#39;], joke[&#39;joke&#39;], &quot; n&quot;, &quot;haha:&quot;, joke[&#39;haha&#39;], &quot; n&quot;, &quot;boohoo:&quot;, joke[&#39;boohoo&#39;], &quot; n&quot;) # Number of jokes def countJokes(): return len(jokes_data) # Test Joke Model if __name__ == &quot;__main__&quot;: initJokes() # initialize jokes # Most likes and most jeered best = favoriteJoke() print(&quot;Most liked&quot;, best[&#39;haha&#39;]) printJoke(best) worst = jeeredJoke() print(&quot;Most jeered&quot;, worst[&#39;boohoo&#39;]) printJoke(worst) # Random joke print(&quot;Random joke&quot;) printJoke(getRandomJoke()) # Count of Jokes print(&quot;Jokes Count: &quot; + str(countJokes())) . Backend Interface for Web API (Control) . An application programming interface (API) is the medium by which different systems of software interact. In our applications we have two big systems:1. Python Backend that stores data beyond a single Web page2. GH Pages/Fastpages Frontend that is responsible for presenting data . To communicate data between Frontend and Backend, this section Backend code provides and interface to the Frontend using a Web Service Endpoint. Examples of endpoints are listed below and can be typed within a browser, which will return JSON data: . https://flask.nighthawkcodingsociety.com/api/jokes | https://flask.nighthawkcodingsociety.com/api/jokes/2 | https://flask.nighthawkcodingsociety.com/api/jokes/random | . As you can see, these Endpoints return JSON. They are NOT that readable by normal humans. However, they are very effective in passing requested data across the Internet. The Frontend code is responsible for formatting and presenting and interface that allows the typical computer user to interact with this data. . The next cell of code is Creating Endpoints that return JSON. This allows developers in the Frontend to interact with Backend data. API is a contract between the Frontend and Backend on how to share data. . FYI, there is NO output from this section . . from flask import Blueprint, jsonify # jsonify creates an endpoint response object from flask_restful import Api, Resource # used for REST API building import requests # used for testing import random # Blueprints allow this code to be procedurally abstracted from main.py, meaning code is not all in one place app_api = Blueprint(&#39;api&#39;, __name__, url_prefix=&#39;/api/jokes&#39;) # endpoint prefix avoid redundantly typing /api/jokes over and over # API generator https://flask-restful.readthedocs.io/en/latest/api.html#id1 api = Api(app_api) class JokesAPI: # not implemented, this would be where we would allow creation of a new Joke class _Create(Resource): def post(self, joke): pass # getJokes() class _Read(Resource): def get(self): return jsonify(getJokes()) # getJoke(id) class _ReadID(Resource): def get(self, id): return jsonify(getJoke(id)) # getRandomJoke() class _ReadRandom(Resource): def get(self): return jsonify(getRandomJoke()) # getRandomJoke() class _ReadCount(Resource): def get(self): count = countJokes() countMsg = {&#39;count&#39;: count} return jsonify(countMsg) # put method: addJokeHaHa class _UpdateLike(Resource): def put(self, id): addJokeHaHa(id) return jsonify(getJoke(id)) # put method: addJokeBooHoo class _UpdateJeer(Resource): def put(self, id): addJokeBooHoo(id) return jsonify(getJoke(id)) # building RESTapi interfaces, these routes are added to Web Server http://&lt;server&lt;/api/jokes api.add_resource(_Create, &#39;/create/&lt;string:joke&gt;&#39;) api.add_resource(_Read, &#39;/&#39;) # default, which returns all jokes api.add_resource(_ReadID, &#39;/&lt;int:id&gt;&#39;) api.add_resource(_ReadRandom, &#39;/random&#39;) api.add_resource(_ReadCount, &#39;/count&#39;) api.add_resource(_UpdateLike, &#39;/like/&lt;int:id&gt;/&#39;) api.add_resource(_UpdateJeer, &#39;/jeer/&lt;int:id&gt;/&#39;) . Frontend (View Simulation) and Hacks . This python codes tests endpoints on a server. This can be handy for development and testing when making modifications to the Jokes Web APIs. This code works off of the server endpoint/url, not from code cells above it in this notebook. . To work with this code and make observation for learning... . Run a local server from flask_portfolio project and the change server variable to be local | Observe the requests endpoints and the output, see if you can observe what is happening/changing on put requests | The &quot;requests&quot; are captured into a List, the List is used in the for loop to extract from RESTful API format. | Try running this with Debugging and observe what data is being created at each step (Required) | Try to format this data in Python print statements to be more readable (Required) | Start and stop local server and observe errors | . # server = &quot;http://127.0.0.1:5000/&quot; # run local server = &#39;https://flask.nighthawkcodingsociety.com/&#39; # run from web server url = server + &quot;api/jokes/&quot; responses = [] # responses list # Get the count of jokes on server count_response = requests.get(url+&quot;count&quot;) count_json = count_response.json() count = count_json[&#39;count&#39;] # Update likes/dislikes test sequence using random joke num = str(random.randint(0, count-1)) # test a random record responses.append( requests.get(url+num) # Get/read joke by id ) responses.append( requests.put(url+&quot;like/&quot;+num) # Put/add to like count ) responses.append( requests.put(url+&quot;jeer/&quot;+num) # Put/add to jeer count ) # Get a random joke responses.append( requests.get(url+&quot;random&quot;) # Get/read a random joke ) # Cycle through and print responses for response in responses: print(response) try: print(response.json()) except: print(&quot;data error&quot;) .",
            "url": "https://ylu-1258.github.io/YLu-Blog/techtalk/webapi",
            "relUrl": "/techtalk/webapi",
            "date": " • Oct 17, 2022"
        }
        
    
  
    
        ,"post44": {
            "title": "Javascript Web Page using an API",
            "content": "The code below holds the info that is being generated into an HTML table. . Key things to know: . &lt; table &gt; creates a TABLE | &lt; tr &gt; creates a ROW | &lt; th &gt; makes the text a column HEADER | &lt; tbody id = &quot;results&quot; &gt; defines an element id, to be used within JavaScript | . &lt;!-- HTML table fragment for page --&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;Joke&lt;/th&gt; &lt;th&gt;HaHa&lt;/th&gt; &lt;th&gt;Boohoo&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody id=&quot;result&quot;&gt; &lt;!-- javascript generated data --&gt; &lt;/tbody&gt; &lt;/table&gt; . Constant variables are declared here with keyword const . Key things to know: . The document object &quot;result&quot; represents table body in the HTML above. | If you want to access any element in an HTML page in JavaScript, you always start by accessing the document object. In this case, we are accessing &quot;result&quot; and defining a &quot;resultContainer&quot; | In the code, in following cells, document elements are created and organized for each Joke, each is added to the &quot;resultContainer&quot; as a row in the table body. | Accessing the api is done using the variables url and options, this is setup to fetch the Jokes from the backend | . // prepare HTML defined &quot;result&quot; container for new output const resultContainer = document.getElementById(&quot;result&quot;); // keys for joke reactions const HAHA = &quot;haha&quot;; const BOOHOO = &quot;boohoo&quot;; // prepare fetch urls const url = &quot;https://flask.nighthawkcodingsociety.com/api/jokes&quot;; const like_url = url + &quot;/like/&quot;; // haha reaction const jeer_url = url + &quot;/jeer/&quot;; // boohoo reaction // prepare fetch GET options const options = { method: &#39;GET&#39;, // *GET, POST, PUT, DELETE, etc. mode: &#39;cors&#39;, // no-cors, *cors, same-origin cache: &#39;default&#39;, // *default, no-cache, reload, force-cache, only-if-cached credentials: &#39;omit&#39;, // include, *same-origin, omit headers: { &#39;Content-Type&#39;: &#39;application/json&#39; // &#39;Content-Type&#39;: &#39;application/x-www-form-urlencoded&#39;, }, }; // prepare fetch PUT options, clones with JS Spread Operator (...) const put_options = {...options, method: &#39;PUT&#39;}; // clones and replaces method . The below code uses a function called fetch to gather the data from the backend. . Key things to understand: . The &quot;url&quot; &quot;response&quot; is checked in case the site is down and returns an error | On successful fetch, the code places each Joke in the HTML table body using a &quot;for&quot; loop and creating document elements from each &quot;row&quot; of the fetched &quot;data&quot;. | The creation of each Haha and Boohoo &quot;onclick&quot; &quot;button&quot; is also done in the same loop. | Updates to backend are setup to occur with each onclick, each click calls the &quot;reaction&quot; function | . // fetch the API fetch(url, options) // response is a RESTful &quot;promise&quot; on any successful fetch .then(response =&gt; { // check for response errors if (response.status !== 200) { error(&#39;GET API response failure: &#39; + response.status); return; } // valid response will have JSON data response.json().then(data =&gt; { console.log(data); for (const row of data) { // make &quot;tr element&quot; for each &quot;row of data&quot; const tr = document.createElement(&quot;tr&quot;); // td for joke cell const joke = document.createElement(&quot;td&quot;); joke.innerHTML = row.id + &quot;. &quot; + row.joke; // add fetched data to innerHTML // td for haha cell with onclick actions const haha = document.createElement(&quot;td&quot;); const haha_but = document.createElement(&#39;button&#39;); haha_but.id = HAHA+row.id // establishes a HAHA JS id for cell haha_but.innerHTML = row.haha; // add fetched &quot;haha count&quot; to innerHTML haha_but.onclick = function () { // onclick function call with &quot;like parameters&quot; reaction(HAHA, like_url+row.id, haha_but.id); }; haha.appendChild(haha_but); // add &quot;haha button&quot; to haha cell // td for boohoo cell with onclick actions const boohoo = document.createElement(&quot;td&quot;); const boohoo_but = document.createElement(&#39;button&#39;); boohoo_but.id = BOOHOO+row.id // establishes a BOOHOO JS id for cell boohoo_but.innerHTML = row.boohoo; // add fetched &quot;boohoo count&quot; to innerHTML boohoo_but.onclick = function () { // onclick function call with &quot;jeer parameters&quot; reaction(BOOHOO, jeer_url+row.id, boohoo_but.id); }; boohoo.appendChild(boohoo_but); // add &quot;boohoo button&quot; to boohoo cell // this builds ALL td&#39;s (cells) into tr (row) element tr.appendChild(joke); tr.appendChild(haha); tr.appendChild(boohoo); // this adds all the tr (row) work above to the HTML &quot;result&quot; container resultContainer.appendChild(tr); } }) }) // catch fetch errors (ie Nginx ACCESS to server blocked) .catch(err =&gt; { error(err + &quot; &quot; + url); }); . The below code uses fetch to update backend data using &quot;put_options&quot;. The purpose is to update Hahaa and Bohoo counters in backend. . Key things to understand: . The &quot;url&quot; &quot;response&quot; is checked to verify update occurred | The element id of button clicked is updated with the data returned from the API. | Note, the elemID is received as parameter. This data was setup when the button was created in former cell. | . // Reaction function to likes or jeers user actions function reaction(type, put_url, elemID) { // fetch the API fetch(put_url, put_options) // response is a RESTful &quot;promise&quot; on any successful fetch .then(response =&gt; { // check for response errors if (response.status !== 200) { error(&quot;PUT API response failure: &quot; + response.status) return; // api failure } // valid response will have JSON data response.json().then(data =&gt; { console.log(data); // Likes or Jeers updated/incremented if (type === HAHA) // like data element document.getElementById(elemID).innerHTML = data.haha; // fetched haha data assigned to haha Document Object Model (DOM) else if (type === BOOHOO) // jeer data element document.getElementById(elemID).innerHTML = data.boohoo; // fetched boohoo data assigned to boohoo Document Object Model (DOM) else error(&quot;unknown type: &quot; + type); // should never occur }) }) // catch fetch errors (ie Nginx ACCESS to server blocked) .catch(err =&gt; { error(err + &quot; &quot; + put_url); }); } // Something went wrong with actions or responses function error(err) { // log as Error in console console.error(err); // append error to resultContainer const tr = document.createElement(&quot;tr&quot;); const td = document.createElement(&quot;td&quot;); td.innerHTML = err; tr.appendChild(td); resultContainer.appendChild(tr); } . Hacks . The code below relates to the rapidapi you worked with last week. . What are some similarities you see with the javascript for the jokes api? | In a blogpost, break up the code in cells like done above and try to describe what this code is doing. | . &lt;!-- HTML table fragment for page --&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;Time&lt;/th&gt; &lt;th&gt;All-time Cases&lt;/th&gt; &lt;th&gt;Recorded Deaths&lt;/th&gt; &lt;th&gt;Active Cases&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;td id=&quot;time&quot;&gt;&lt;/td&gt; &lt;td id=&quot;total_cases&quot;&gt;&lt;/td&gt; &lt;td id=&quot;total_deaths&quot;&gt;&lt;/td&gt; &lt;td id=&quot;active_cases&quot;&gt;&lt;/td&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;Country&lt;/th&gt; &lt;th&gt;All-time Cases&lt;/th&gt; &lt;th&gt;Recorded Deaths&lt;/th&gt; &lt;th&gt;Active Cases&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody id=&quot;result&quot;&gt; &lt;!-- generated rows --&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;!-- Script is layed out in a sequence (no function) and will execute when page is loaded --&gt; &lt;script&gt; // prepare HTML result container for new output const resultContainer = document.getElementById(&quot;result&quot;); // prepare fetch options const url = &quot;https://flask.nighthawkcodingsociety.com/api/covid/&quot;; const headers = { method: &#39;GET&#39;, // *GET, POST, PUT, DELETE, etc. mode: &#39;cors&#39;, // no-cors, *cors, same-origin cache: &#39;default&#39;, // *default, no-cache, reload, force-cache, only-if-cached credentials: &#39;omit&#39;, // include, *same-origin, omit headers: { &#39;Content-Type&#39;: &#39;application/json&#39; // &#39;Content-Type&#39;: &#39;application/x-www-form-urlencoded&#39;, }, }; // fetch the API fetch(url, headers) // response is a RESTful &quot;promise&quot; on any successful fetch .then(response =&gt; { // check for response errors if (response.status !== 200) { const errorMsg = &#39;Database response error: &#39; + response.status; console.log(errorMsg); const tr = document.createElement(&quot;tr&quot;); const td = document.createElement(&quot;td&quot;); td.innerHTML = errorMsg; tr.appendChild(td); resultContainer.appendChild(tr); return; } // valid response will have json data response.json().then(data =&gt; { console.log(data); console.log(data.world_total) // World Data document.getElementById(&quot;time&quot;).innerHTML = data.world_total.statistic_taken_at; document.getElementById(&quot;total_cases&quot;).innerHTML = data.world_total.total_cases; document.getElementById(&quot;total_deaths&quot;).innerHTML = data.world_total.total_deaths; document.getElementById(&quot;active_cases&quot;).innerHTML = data.world_total.active_cases; // Country data for (const row of data.countries_stat) { console.log(row); // tr for each row const tr = document.createElement(&quot;tr&quot;); // td for each column const name = document.createElement(&quot;td&quot;); const cases = document.createElement(&quot;td&quot;); const deaths = document.createElement(&quot;td&quot;); const active = document.createElement(&quot;td&quot;); // data is specific to the API name.innerHTML = row.country_name; cases.innerHTML = row.cases; deaths.innerHTML = row.deaths; active.innerHTML = row.active_cases; // this builds td&#39;s into tr tr.appendChild(name); tr.appendChild(cases); tr.appendChild(deaths); tr.appendChild(active); // add HTML to container resultContainer.appendChild(tr); } }) }) // catch fetch errors (ie ACCESS to server blocked) .catch(err =&gt; { console.error(err); const tr = document.createElement(&quot;tr&quot;); const td = document.createElement(&quot;td&quot;); td.innerHTML = err; tr.appendChild(td); resultContainer.appendChild(tr); }); &lt;/script&gt; .",
            "url": "https://ylu-1258.github.io/YLu-Blog/techtalk/webfrontend",
            "relUrl": "/techtalk/webfrontend",
            "date": " • Oct 17, 2022"
        }
        
    
  
    
        ,"post45": {
            "title": "6 Collegeboard Criterias",
            "content": "Collegeboard’s 6 Criterias . What are we aiming to accomplish? . Program Purpose &amp; Function . An online single-player minesweeper game that can be played anywhere, anytime | Users could choose to store data such as high scores (minimum number of moves or time) if they wish. | . Data Abstraction . Using the sqlite3 library on python to store user names and each user’s corresponding information, such as high-score | Using python lists to store the state of the game, what nodes or cells are revealed, flagged, hidden, etc. | Creating a main class for the game and then establishing a constructor that initializes other object attributes for the main game to access or manipulate. | . Managing Complexity . Maintaining user data in a organized manner with software like databases (SQL). | Using loops to reduce repetitive code / allow for a dynamic number of times for the execution of our code. | . Procedural Abstraction . Establishing a set of enumerated types to link certain values with constants. Used to established the different states of a node or the particular status of that cell. | . | Using a main class to cover the Minesweeper Game: The class will contain a constructor, attributes, and methods to ensure smooth gameplay. | This will simplify the programming process too as each individual method under the class will “inherit” the attributes of the class. This expands the scope of our functions sort of “links” them together. (Methods work in harmony with each other) | . | Creating templates for SQL queries, JSON queries, and also html pages to reduce the amount of manual programing required | . Algorithm Implementation . Maintain a clear-board at the begining of the game. On the first input (click) from the user, start randomly generating N=(M//3) number of mines, where M is the total number of mines. Use the random module to generate a set of integers within the bounds of the array, generate two integers from each mine to indicate the row or coloumn coordinate. Finally, add both generated values into a tuple which will then be appended to a list containing all of the mine coordinates. | AS OF 10/10 Consider using the RapidAPI minesweeper API to generate a board. (However this limits our flexibility as the first mine clicked might be an actual mine, leading to an inevitable loss). | . | Use an overall gameloop that checks the status of the game on each “turn of the game” (iteration of the game). If gameOver == true, signify that the game is over Use control flow statements such as if-else-elif to determine wether or not a user has clicked on a mine. If yes, set gameOver to true, if not, continue with the current iteration of the loop. | Based on the current state of the board, if there are no mines left, return a win screen. If there are mines left, assume that the player clicked on a mine and return a lose screen. | . | . | Within each iteration, prompt the user to input a coordinate point (When testing, use terminal input with standard (x,y) input, for the final product, clicking each cell should return a coordinate automatically to the python backend). Hard part is detecting a nearby “safe grid” for each mine clicked, while subsequently revealing all the mines in the safe grid | We have decided to use a recursive approach to resolve this problem. The recursive approach will store a list of safe mines. It will first append the mine clicked by the user (assuming that the mine passes the safe check). Following this, the algorithm will then check the (r+1,c), (r+1,c-1), (r+1,c+1), (r,c-1), (r,c+1), (r-1,c), (r-1,c-1), and (r-1,c+1) coordinates to determine wether or not surrouding cells are safe or not. Recursively execute the same process for each the 8 mines adjacent to the initial cell and stop until an “edge-mine” is reached. | . | Communiate user input and program output via JSON queries between the python backend and the HTML/CSS/JS frontend. | . Testing Code . For testing our code, we will primarily experiment with our code either in linux for python programs, or in the browser for frontend testing. | We will make use of print() and Console.log() statements to help with debugging. | Certain events in python could be made easier if we used try/except exceptions to catch errors. | .",
            "url": "https://ylu-1258.github.io/YLu-Blog/markdown/apcsp/week%207/2022/10/09/Final-Project-Criterias.html",
            "relUrl": "/markdown/apcsp/week%207/2022/10/09/Final-Project-Criterias.html",
            "date": " • Oct 9, 2022"
        }
        
    
  
    
        ,"post46": {
            "title": "API Usage",
            "content": "| API Demonstration | . . What is an API? . An API (Application Programming Interface) is used as a intermediary bridge between two software applications that stardizes protocol and conventions. In simple words an API could help us to send and receive data. . import requests # Set number of rows to 10 r=10 # Set number of columns to 10 c=10 # Set number of mines to r*c/4 b = (r*c)//4 url = &quot;https://minesweeper1.p.rapidapi.com/boards/new&quot; querystring = {&quot;r&quot;:r,&quot;c&quot;:c,&quot;bombs&quot;:b} headers = { &quot;X-RapidAPI-Key&quot;: &quot;9443d1725fmsha0cfacd48534f3cp1802cfjsn326b60d92795&quot;, &quot;X-RapidAPI-Host&quot;: &quot;minesweeper1.p.rapidapi.com&quot; } response = requests.request(&quot;GET&quot;, url, headers=headers, params=querystring) board = response.json().get(&quot;board&quot;) for row in board: temp = [&quot;M&quot; if k == &quot;-1&quot; else k for k in [str(i) for i in row]] print(&quot; &quot;.join(temp)) #print(board) . 1 1 2 M M 1 1 M M 2 3 M 3 2 2 1 1 3 M 2 M M 3 0 0 1 1 3 2 2 M M 2 0 0 1 M 3 M 3 3 4 3 1 0 1 2 5 M M 2 M M 3 1 0 1 M M 3 2 M M M 1 0 1 2 2 1 1 2 3 3 2 1 0 0 1 1 0 0 0 2 M 2 1 1 2 M 0 0 0 2 M 2 1 M 2 1 . import requests url = &quot;https://stock-and-options-trading-data-provider.p.rapidapi.com/options/aapl&quot; headers = { &quot;X-RapidAPI-Proxy-Secret&quot;: &quot;a755b180-f5a9-11e9-9f69-7bf51e845926&quot;, &quot;X-RapidAPI-Key&quot;: &quot;9443d1725fmsha0cfacd48534f3cp1802cfjsn326b60d92795&quot;, &quot;X-RapidAPI-Host&quot;: &quot;stock-and-options-trading-data-provider.p.rapidapi.com&quot; } response = requests.request(&quot;GET&quot;, url, headers=headers) json = response.json() print(response.text) print(json[&quot;stock&quot;]) . def print_stock(json): stock_info = json[&quot;stock&quot;] blacklist = [&quot;longBusinessSummary&quot;, &quot;coinMarketCapLink&quot;, &quot;ebitda&quot;, &quot;maxAge&quot;, &quot;ebitdaMargins&quot;, &quot;logo_url&quot;, &quot;strikePrice&quot;, &quot;volumeAllCurrencies&quot;, &quot;startDate&quot;, &quot;circulatingSupply&quot;, &quot;navPrice&quot;] print(&quot;Current Stock Information for Ticker: &quot;, stock_info[&quot;symbol&quot;]) for key, value in stock_info.items(): if key not in blacklist: print(key + &quot;: &quot; + str(value)) print_stock(json) . Current Stock Information for Ticker: AAPL zip: 95014 sector: Technology fullTimeEmployees: 154000 city: Cupertino phone: 408 996 1010 state: CA country: United States companyOfficers: [] website: https://www.apple.com address1: One Apple Park Way industry: Consumer Electronics profitMargins: 0.25709 grossMargins: 0.43313998 operatingCashflow: 118224003072 revenueGrowth: 0.019 operatingMargins: 0.30533 targetLowPrice: 130 recommendationKey: buy grossProfits: 152836000000 freeCashflow: 83344621568 targetMedianPrice: 185 currentPrice: 140.42 earningsGrowth: -0.077 currentRatio: 0.865 returnOnAssets: 0.22204 numberOfAnalystOpinions: 41 targetMeanPrice: 183.5 debtToEquity: 205.984 returnOnEquity: 1.62816 targetHighPrice: 220 totalCash: 48230998016 totalDebt: 119691001856 totalRevenue: 387541991424 totalCashPerShare: 3.001 financialCurrency: USD revenuePerShare: 23.732 quickRatio: 0.697 recommendationMean: 1.9 exchange: NMS shortName: Apple Inc. longName: Apple Inc. exchangeTimezoneName: America/New_York exchangeTimezoneShortName: EDT isEsgPopulated: False gmtOffSetMilliseconds: -14400000 quoteType: EQUITY symbol: AAPL messageBoardId: finmb_24937 market: us_market annualHoldingsTurnover: None enterpriseToRevenue: 5.994 beta3Year: None enterpriseToEbitda: 17.929 52WeekChange: -0.019046307 morningStarRiskRating: None forwardEps: 6.46 revenueQuarterlyGrowth: None sharesOutstanding: 16070800384 fundInceptionDate: None annualReportExpenseRatio: None totalAssets: None bookValue: 3.61 sharesShort: 116076947 sharesPercentSharesOut: 0.0072000003 fundFamily: None lastFiscalYearEnd: 1632528000 heldPercentInstitutions: 0.59582 netIncomeToCommon: 99632996352 trailingEps: 6.05 lastDividendValue: 0.23 SandP52WeekChange: -0.16544336 priceToBook: 38.897507 heldPercentInsiders: 0.00071000005 nextFiscalYearEnd: 1695600000 yield: None mostRecentQuarter: 1656115200 shortRatio: 1.53 sharesShortPreviousMonthDate: 1660521600 floatShares: 16053877710 beta: 1.249815 enterpriseValue: 2322811584512 priceHint: 2 threeYearAverageReturn: None lastSplitDate: 1598832000 lastSplitFactor: 4:1 legalType: None lastDividendDate: 1659657600 morningStarOverallRating: None earningsQuarterlyGrowth: -0.106 priceToSalesTrailing12Months: 5.823012 dateShortInterest: 1663200000 pegRatio: 2.42 ytdReturn: None forwardPE: 21.736841 lastCapGain: None shortPercentOfFloat: 0.0072000003 sharesShortPriorMonth: 115400891 impliedSharesOutstanding: 0 category: None fiveYearAverageReturn: None previousClose: 140.09 regularMarketOpen: 140.42 twoHundredDayAverage: 159.1368 trailingAnnualDividendYield: 0.006353059 payoutRatio: 0.1471 volume24Hr: None regularMarketDayHigh: 141.89 averageDailyVolume10Day: 101319120 regularMarketPreviousClose: 140.09 fiftyDayAverage: 158.688 trailingAnnualDividendRate: 0.89 open: 140.42 toCurrency: None averageVolume10days: 101319120 expireDate: None algorithm: None dividendRate: 0.92 exDividendDate: 1659657600 regularMarketDayLow: 138.5729 currency: USD trailingPE: 23.209917 regularMarketVolume: 73717951 lastMarket: None maxSupply: None openInterest: None marketCap: 2256661643264 averageVolume: 80128214 dayLow: 138.5729 ask: 141.03 askSize: 1000 volume: 73717951 fiftyTwoWeekHigh: 182.94 fromCurrency: None fiveYearAvgDividendYield: 1.02 fiftyTwoWeekLow: 129.04 bid: 141.03 tradeable: False dividendYield: 0.0066000004 bidSize: 1800 dayHigh: 141.89 regularMarketPrice: 140.42 lastUpdated: 10/10/2022 .",
            "url": "https://ylu-1258.github.io/YLu-Blog/jupyter/apcsp/week%207/2022/10/07/Minesweeper-Stocks-api.html",
            "relUrl": "/jupyter/apcsp/week%207/2022/10/07/Minesweeper-Stocks-api.html",
            "date": " • Oct 7, 2022"
        }
        
    
  
    
        ,"post47": {
            "title": "Python RapidAPI",
            "content": "Python, RapidAPI Terms . APIs and tooling like Jupyter docs allows many opportunities in fields like Data Science. As more and more developers use APIs, they build standards in how you setup a client, send requests and receive information... . Covid19 RapidAPI Example . To begin the API journey. You need to find an API provider. . RapidAPI is a great option. You must setup and account, but there are many free options. | Goto this page for starters, the Corona virus World and India data- Under Code Snippets pick Python - Requests | . RapidAPI, you will select Python Requests type of code to work with you Notebook. . The url is the endpoint to which the API is directed | The headers is a dictionary data structure to send special messaging to the endpoint | The requests.request() python function is used to send a request and retrieve their responses | The response variable receives result of of the request in JSON text | . Next step, is to format the response according to your data science needs . &quot;&quot;&quot; Requests is a HTTP library for the Python programming language. The goal of the project is to make HTTP requests simpler and more human-friendly. &quot;&quot;&quot; import requests &quot;&quot;&quot; RapidAPI is the world&#39;s largest API Marketplace. Developers use Rapid API to discover and connect to thousands of APIs. &quot;&quot;&quot; url = &quot;https://corona-virus-world-and-india-data.p.rapidapi.com/api&quot; headers = { &#39;x-rapidapi-key&#39;: &quot;9443d1725fmsha0cfacd48534f3cp1802cfjsn326b60d92795&quot;, &#39;x-rapidapi-host&#39;: &quot;corona-virus-world-and-india-data.p.rapidapi.com&quot; } # Request Covid Data response = requests.request(&quot;GET&quot;, url, headers=headers) # print(response.text) # uncomment this line to see raw data # This code looks for &quot;world data&quot; print(&quot;World Totals&quot;) world = response.json().get(&#39;world_total&#39;) # turn response to json() so we can extract &quot;world_total&quot; for key, value in world.items(): # this finds key, value pairs in country print(key, value) print() # This code looks for USA in &quot;countries_stats&quot; print(&quot;Country Totals&quot;) countries = response.json().get(&#39;countries_stat&#39;) for country in countries: # countries is a list if country[&quot;country_name&quot;] == &quot;USA&quot;: # this filters for USA for key, value in country.items(): # this finds key, value pairs in country print(key, value) . World Totals total_cases 509,268,964 new_cases 204,268 total_deaths 6,242,509 new_deaths 630 total_recovered 461,827,849 active_cases 41,198,606 serious_critical 42,510 total_cases_per_1m_population 65,334 deaths_per_1m_population 800.9 statistic_taken_at 2022-04-24 11:18:01 Country Totals country_name USA cases 82,649,779 deaths 1,018,316 region total_recovered 80,434,925 new_deaths 0 new_cases 0 serious_critical 1,465 active_cases 1,196,538 total_cases_per_1m_population 247,080 deaths_per_1m_population 3,044 total_tests 1,000,275,726 tests_per_1m_population 2,990,303 . Digital Coin Example . This example provides digital coin feedback (ie Bitcoin). It include popularity, price, symbols, etc. . A valid X-RapidAPI-Key is required. Look in code for link to RapidAPI page | Read all comments in code for further guidance | . # RapidAPI page https://rapidapi.com/Coinranking/api/coinranking1/ # Begin Rapid API Code import requests url = &quot;https://coinranking1.p.rapidapi.com/coins&quot; querystring = {&quot;referenceCurrencyUuid&quot;:&quot;yhjMzLPhuIDl&quot;,&quot;timePeriod&quot;:&quot;24h&quot;,&quot;tiers[0]&quot;:&quot;1&quot;,&quot;orderBy&quot;:&quot;marketCap&quot;,&quot;orderDirection&quot;:&quot;desc&quot;,&quot;limit&quot;:&quot;50&quot;,&quot;offset&quot;:&quot;0&quot;} headers = { &quot;X-RapidAPI-Key&quot;: &quot;9443d1725fmsha0cfacd48534f3cp1802cfjsn326b60d92795&quot;, # place your key here &quot;X-RapidAPI-Host&quot;: &quot;coinranking1.p.rapidapi.com&quot; } response = requests.request(&quot;GET&quot;, url, headers=headers, params=querystring) print(response.text) # End Rapid API Code json = response.json() # convert response to python json object # Observe data from an API. This is how data transports over the internet in a &quot;JSON&quot; text form # - The JSON &quot;text&quot; is formed in dictionary {} and list [] divisions # - To read the result, Data Scientist of Developer converts JSON into human readable form # - Review the first line, look for the keys -- &quot;status&quot; and &quot;data&quot; . Formatting Digital Coin example . JSON text transferred from the API in the previous cell was converted to a Python Dictionary called json. The &quot;coins&quot; in the dictionary contain a list of the most relevant data. Look at the code and comments to see how the original text is turned into something understandable. Additionally, there are error check to make sure we are starting the code with the expectation that the API was run correctly. . &quot;&quot;&quot; This cell is dependent on valid run of API above. - try and except code is making sure &quot;json&quot; was properly run above - inside second try is code that is used to process Coin API data Note. Run this cell repeatedly to format data without re-activating API &quot;&quot;&quot; try: print(&quot;JSON data is Python type: &quot; + str(type(json))) try: # Extracting Coins JSON status, if the API worked status = json.get(&#39;status&#39;) print(&quot;API status: &quot; + status) print() # Extracting Coins JSON data, data about the coins data = json.get(&#39;data&#39;) # Procedural abstraction of Print code for coins def print_coin(c): print(c[&quot;symbol&quot;], c[&quot;price&quot;]) print(&quot;Icon Url: &quot; + c[&quot;iconUrl&quot;]) print(&quot;Rank Url: &quot; + c[&quot;coinrankingUrl&quot;]) # Coins data was observed to be a list for coin in data[&#39;coins&#39;]: print_coin(coin) print() except: print(&quot;Did you insert a valid key in X-RapidAPI-Key of API cell above?&quot;) print(json) except: print(&quot;This cell is dependent on running API call in cell above!&quot;) . This cell is dependent on running API call in cell above! . Go deeper into APIs . Web Development vs Jupyter Notebook. A notebook is certainly a great place to start. But, for your end of Trimester project we want you to build the skill to reference and use APIs within your Project. Here are some resources to get you started with this journey. . In the Nighthawk Coders APCSP you can find an Overview and Examples using APIs:APCSP APIs menu- Using Covid RapidAPI JavaScript frontend API code in APCSP Fastpages GitHub repo: https://github.com/nighthawkcoders/APCSP/blob/master/_posts/2022-07-10-PBL-rapidapi.md | . | Making a Jokes API (this will next API tech talk) Frontend. JavaScript frontend code in APCSP fastpages GitHub repo: https://github.com/nighthawkcoders/APCSP/blob/master/_posts/2022-07-10-PBL-jokes.md | Backend Endpoints. Python code that allows Frontend access: https://github.com/nighthawkcoders/flask_portfolio/blob/main/api.py | Backend Jokes Management. Python code that support Create, Read, Update, Delete (CRUD): https://github.com/nighthawkcoders/flask_portfolio/blob/main/model_jokes.py | . | . Hacks . Find and use an API as part of your project. An API and a little coding logic will be a big step toward getting meaningful data for a project. There are many API providers, find one that might work for your project to complete this hack. When picking an API you are looking for something that will work with either JavaScript Fetch or Python Request. Here are some samples, these are not qualified in any way. . RapidAPI- GitHub Project | No Key APIs Article | Twitter Developer | Google Developer | Reddit Developer | . Show API and format results in either Web Page or Jupyter Notebook. Ultimately, I will expect that we do APIs in backend (Python/Flask). However, for this Hack you can pick your preference. We will discuss pros and cons in next API tech talk. .",
            "url": "https://ylu-1258.github.io/YLu-Blog/week%207/jupyter/apcsp/2022/10/03/PBL-python_rapidapi.html",
            "relUrl": "/week%207/jupyter/apcsp/2022/10/03/PBL-python_rapidapi.html",
            "date": " • Oct 3, 2022"
        }
        
    
  
    
        ,"post48": {
            "title": "1.4 Identifying and Correcting Errors",
            "content": "College Board Big Idea 1 . Identifying and Correcting Errors (Unit 1.4) . Become familiar with types of errors and strategies to fixing them . Lightly Review Videos and take notes on topics with Blog | Complete assigned MCQ questions | . Here are some code segments you can practice fixing: . alphabet = &quot;abcdefghijklmnopqrstuvwxyz&quot; alphabetList = [] for i in alphabet: alphabetList.append(i) print(alphabetList) . The intended outcome is to determine where the letter is in the alphabet using a while loop . What is a good test case to check the current outcome? Why? | Make changes to get the intended outcome. | . letter = input(&quot;What letter would you like to check?&quot;) i = 0 while i &lt; 26: if alphabetList[i] == letter: print(&quot;The letter &quot; + letter + &quot; is the &quot; + str(i+1) + &quot; letter in the alphabet&quot;) i += 1 . The intended outcome is to determine where the letter is in the alphabet using a for loop . What is a good test case to check the current outcome? Why? | Make changes to get the intended outcome. | . letter = input(&quot;What letter would you like to check?&quot;) count = 1 for i in alphabetList: if i == letter: print(&quot;The letter &quot; + letter + &quot; is the &quot; + str(count) + &quot; letter in the alphabet&quot;) count += 1 . This code outputs the even numbers from 0 - 10 using a while loop. . Analyze this code to determine what can be changed to get the outcome to be odd numbers. (Code block below) | . evens = [] i = 0 while i &lt;= 10: evens.append(i) i += 2 print(evens) . This code should output the odd numbers from 0 - 10 using a while loop. . odds = [] i = 1 while i &lt;= 10: odds.append(i) i += 2 print(odds) . This code outputs the even numbers from 0 - 10 using a for loop. . Analyze this code to determine what can be changed to get the outcome to be odd numbers. (Code block below) | . numbers = [0,1,2,3,4,5,6,7,8,9,10] evens = [] for i in numbers: if (numbers[i] % 2 == 0): evens.append(numbers[i]) print(evens) . This code should output the odd numbers from 0 - 10 using a for loop. . numbers = [0,1,2,3,4,5,6,7,8,9,10] odds = [] for i in numbers: if (numbers[i] % 2 != 0): odds.append(numbers[i]) print(odds) . The intended outcome is printing a number between 1 and 100 once, if it is a multiple of 2 or 5 . What values are outputted incorrectly. Why? | Make changes to get the intended outcome. | . numbers = [] newNumbers = [] i = 0 while i &lt;= 100: numbers.append(i) i += 1 for i in numbers: if numbers[i] == 0: continue if numbers[i] % 5 == 0: newNumbers.append(numbers[i]) elif numbers[i] % 2 == 0: newNumbers.append(numbers[i]) print(newNumbers) . [2, 4, 5, 6, 8, 10, 12, 14, 15, 16, 18, 20, 22, 24, 25, 26, 28, 30, 32, 34, 35, 36, 38, 40, 42, 44, 45, 46, 48, 50, 52, 54, 55, 56, 58, 60, 62, 64, 65, 66, 68, 70, 72, 74, 75, 76, 78, 80, 82, 84, 85, 86, 88, 90, 92, 94, 95, 96, 98, 100] . Challenge . This code segment is at a very early stage of implementation. . What are some ways to (user) error proof this code? | The code should be able to calculate the cost of the meal of the user | . Hint: . write a “single” test describing an expectation of the program of the program | test - input burger, expect output of burger price | run the test, which should fail because the program lacks that feature | write “just enough” code, the simplest possible, to make the test pass | . Then repeat this process until you get program working like you want it to work. . menu = {&quot;burger&quot;: 3.99, &quot;fries&quot;: 1.99, &quot;drink&quot;: 0.99} total = 0 #shows the user the menu and prompts them to select an item print(&quot;Menu&quot;) for k,v in menu.items(): print(k + &quot; $&quot; + str(v)) #why does v have &quot;str&quot; in front of it? Because each key value in the menu is actually an integer type. #ideally the code should prompt the user multiple times num_of_items = int(input(&quot;How many items do you wish to buy? &quot;)) print(&quot;Now ordering {0} items&quot;.format(num_of_items)) counter = 0 shopping_cart = {&quot;burger&quot;:0,&quot;fries&quot;:0,&quot;drink&quot;:0} while counter &lt; num_of_items: item = input(&quot;Please select an item from the menu&quot;) try: total+=menu[item] shopping_cart[item]+=1 print(&quot;Processing order for {0}&quot;.format(item)) counter+=1 except: print(&quot;Product &quot;{0} &quot; is not on our menu, please try again...&quot;.format(item)) #code should add the price of the menu items selected by the user print(&quot;You bought {0} burgers, {1} fries, and {2} drinks for a total of {3}$&quot;.format(shopping_cart[&quot;burger&quot;],shopping_cart[&quot;fries&quot;],shopping_cart[&quot;drink&quot;],round(total,2))) . Menu burger $3.99 fries $1.99 drink $0.99 Now ordering 5 items Processing order for burger Processing order for burger Processing order for fries Processing order for drink Product &#34;ice cream&#34; is not on our menu, please try again... Processing order for fries You bought 2 burgers, 2 fries, and 1 drinks for a total of 12.95$ . Hacks . Now is a good time to think about Testing of your teams final project... . What errors may arise in your project? | What are some test cases that can be used? | Make sure to document any bugs you encounter and how you solved the problem. | What are “single” tests that you will perform on your project? Or, your part of the project? As Hack Design and Test plan action … Divide these “single” tests into Issues for Scrum Board prior to coding. FYI, related tests could be in same Issue by using markdown checkboxes to separate tests. | . | .",
            "url": "https://ylu-1258.github.io/YLu-Blog/week%207/jupyter/apcsp/2022/10/03/AP-error_testing.html",
            "relUrl": "/week%207/jupyter/apcsp/2022/10/03/AP-error_testing.html",
            "date": " • Oct 3, 2022"
        }
        
    
  
    
        ,"post49": {
            "title": "Collegeboard 1.1 - 1.3",
            "content": "Screenshots (Completion of Quiz) . Quiz 1.1 Collaboration: . Quiz 1.2 Program Function and Purpose: . Quiz 1.3 Program Design and Development: . Quiz 1.4 Identifying and Correcting Errors: . Notes from Collegeboard videos . 1.1 Collaboration . Learning Objectives CRD - 1.A Explain how innovations are improved through collaboration. CRD - 1.A.1 A computing innovation includes a program as an integral part of its function. CRD - 1.A.2 A computer innovation can be physica (i.e., self-driving car), non physical computing software (e.g., picture editing software), or a nonphysical computng concept (e.g., e-commerce). CRD - 1.A.3 Effective collaboration produces a computing innovation that reflects the diversity of talents and perspectives of those who designed it. CRD - 1.A.4 Collaboration that incudes diverse perspectives helps avoid bias in the development of computing innovations CRD - 1.C Demonstrate effective interpersonal skills during collaboration. CRD - 1.C.1 Effective collaborative teams practice interpersonal skills, including, but not limited to: Communication, Consesus Building, Conflict Resolution, Negotiation . Notes Different people in different departments work on different subjects and interests. . A program always start up with an idea, purpose and plan. A program must have requirements of the that meet the necessary constraints it must meet. . Developers are able to practice better interpersonal skills in a diverse team. . Pair Programming: One programmer types the code, while the other reviews each line of code. . Think-Pair-Share: Students think through a problem alone, pair with a partner to share ideas, and then share results with the class . 1.2: Program Function and Purpose . Learning Objectives CRD - 2.A - Describe the purpose of a computing innovation. CRD - 2.A.1 - The purpose of computing innovations is to solve problems or to pursue interests through creative expression. CRD - 2.A.2 - An understanding of the purpose of a computing innovation provides developers with an improved abiltiy to develop that essential knowledge on. . Notes Different innovations are contained under different categories of innovations . Program inputs are pecies of data that are sent to computers for processing and interpretation. The computer will then perform operations and manipulate teh data in order to produce a desirable. Inputs can either come from the user themselves, or from another program. . Every event in a program is associated with an action that supplies an input to the program. . Event Driven Programming: Program segments and code fragments are executed based on events that trigger specific control flow structures rather an a smooth, sequential flow. . A program is a collection of software statements that collectively serves a specific use and performs a certain task. . The program works for a variety of inputs and situations. (Think of it like a function) . 1.3: Program Design and Development . Learning Objectives CRD - 2.E - Develop a program using a development process. CRD - 2.F - Design a program and its user interface CRD - 2.E.1 - A development process can be ordered and intentional, or exploratory in nature. CRD - 2.E.2 - There are multiple development processes. The following phases are commonly used when developing a program: . Investigating and reflecting | Designing | Prototyping | Testing | . CRD - 2.E.3 - A development process that is iterative requires refinement and revision based on feedback, testing, or reflection throughout the process. This may require revisiting earlier phases of the process. CRD - 2.F.1 - The design of a program incorporates investigation to determine its requirements. CRD - 2.F.2 - Investigation in a development process is useful for understanding and identifying the program constraints, as well as the concerns and interests of the people who will use the program. CRD - 2.F.3 - Some ways investigation can be performed are as follows: . Collecting data through surveys | User testing | Interviews | . CRD - 2.F.4 - Program requirements describe how a program functions and may include a description of user interactions that a program must provide. CRD - 2.F.5 - A program’s specification defines the requirements for the program. CRD - 2.F.6 - In a development process, the design phase outlines how to accomplish a given program specification. CRD - 2.F.7 - The design phase of a program may include: . Brainstorming | Planning and storyboarding | Organizing the program into modules and functional components | Creation of diagrams that represent the layouts of the user interface | Development of a testing strategy for the program | . 1.4: Identifying and Correcting Errors . Learning Objectives CRD-2.I.1 - A logic error is a mistake in the algorithm or program that causes it to behave incorrectly or unexpectedly. CRD-2.I.2 - A syntax error is a mistake in the program where the rules of the programming language are not followed. CRD-2.I.3 - A run-time error is a mistake in the program that occurs during the execution of a program. Programming languages define their own runtime errors. CRD-2.I.4 - An overflow error is an error that occurs when a computer attempts to handle a number that is outside of the defined range of values. CRD-2.I.5 - The following are effective ways to find and correct errors: . test cases | hand tracing | visualizations | debuggers | adding extra output statement(s) CRD-2.J.1 - In the development process, testing uses defined inputs to ensure that an algorithm or program is producing the expected outcomes. Programmers use the results from testing to revise their algorithms or programs. CRD-2.J.2 - Defined inputs used to test a program should demonstrate the different expected outcomes that are at or just beyond the extremes (minimum and maximum) of input data. CRD-2.J.3 - Program requirements are needed to identify appropriate defined inputs for testing. | .",
            "url": "https://ylu-1258.github.io/YLu-Blog/markdown/apcsp/week%206/2022/10/02/Collegeboard-notes-unit-1.html",
            "relUrl": "/markdown/apcsp/week%206/2022/10/02/Collegeboard-notes-unit-1.html",
            "date": " • Oct 2, 2022"
        }
        
    
  
    
        ,"post50": {
            "title": "Coinflip",
            "content": "| Javascript Kernel Usage | Javascript Table Generator | Coin Flip | . Ayo what is this . Having trouble deciding on something? Why not try this coin flip! . Flip A Coin! . Heads: Tails: . | | &lt;/table&gt; .",
            "url": "https://ylu-1258.github.io/YLu-Blog/markdown/apcsp/week%205/2022/09/28/coinflip.html",
            "relUrl": "/markdown/apcsp/week%205/2022/09/28/coinflip.html",
            "date": " • Sep 28, 2022"
        }
        
    
  
    
        ,"post51": {
            "title": "Project Purpoes and Scrum Methodology",
            "content": "The Idea . Our team wishes to build a minesweeper game with a python backend that could consistently communicate with our front-end website to bring a new degree of sophistication and aesthetic to this classic Game. This project would be split into two main portions each with it’s own challenge. The first portion would be making the game engine in python, while the latter portion would be to design and create an aesthetically pleasing and functional frontend to display the Game Status . . The Frontend . The frontend aspect of this project should preferably display a grid of the game and incorporate some aspect of user input. Preliminary testing and experimentation could be done through textbox inputs of coordinate values, while prospective functions could include individual buttons on the Grid itself to provide a GUI for the user to interact with. In both cases, the frontend should be able to communicate with the python backend through JSON data to send and receive data. . More aspects about the Frontend aspects will be added with additional planning in the future. . The Backend . Arguably the more challenging half of this project, the backend would primarily serve to be the game engine of the minesweeper game. Additionally, the backend would also control and organize the various pages and menus on the website. In this writeup, we will only be focusing on the main goals and challenges faced by the Backend team. . Using Object Oriented Programming to create individual objects for the game. Some aspects that could be represented by such objects would be the gameboard and also the individual cells and nodes present within the game board. Doing so would simplify the algorithmic aspect of the game, as the board itself could have methods and attributes to help control the game logic. | Morever, using an object to represent an individual cell in the board would provide greater functionalities than just using a single variable | | Use Enumerated types with the python enum module to create different values for the type and status of each node. Each of these types could then be bound to a constant value which could then be printed on the screen . | Use Recursion to create an algorithm to recursively detect adjacent cells that are safe (AKA not mines). This algorithm would work in the following format Maintain a list of current cells already determined to be safe | Verify if the four adjacent (up, down, left, right) cells next to the selected cell are safe or not, if safe, store the coordinate point in the list, if not, record the cell as a “border cell”, terminate the recursive process, and run another helper function to determine the precise number of mines surrounding the cell | Re-call the function for each of the surrounding adjacent cells to identify other consecutive cells who are safe. | Return a list of the coordinates of a contiguous block of safe cells | Mark the cells to be safe and calculate number of surrounding mines for border cells. | | Verify the Game status, Game is won if: All mines are flagged | All safe cells are cleared Game is lost if: | A mine was dug by the user | | Return the final result back to the front end. If a safe cell was dug, send out a JSON containing an array of the coordinates of the cell and it’s safe neighbors. If a mine was dug, send a JSON containing a boolean value to signify the end of the game. | The Scrum Process . .",
            "url": "https://ylu-1258.github.io/YLu-Blog/markdown/apcsp/week%205/2022/09/26/Project-and-Scrum.html",
            "relUrl": "/markdown/apcsp/week%205/2022/09/26/Project-and-Scrum.html",
            "date": " • Sep 26, 2022"
        }
        
    
  
    
        ,"post52": {
            "title": "Project Details",
            "content": "The Idea . Our team wishes to build a minesweeper game with a python backend that could consistently communicate with our front-end website to bring a new degree of sophistication and aesthetic to this classic Game. This project would be split into two main portions each with it&#39;s own challenge. The first portion would be making the game engine in python, while the latter portion would be to design and create an aesthetically pleasing and functional frontend to display the Game Status . The Frontend . The frontend aspect of this project should preferably display a grid of the game and incorporate some aspect of user input. Preliminary testing and experimentation could be done through textbox inputs of coordinate values, while prospective functions could include individual buttons on the Grid itself to provide a GUI for the user to interact with. In both cases, the frontend should be able to communicate with the python backend through JSON data to send and receive data. . More aspects about the Frontend aspects will be added with additional planning in the future. . The Backend . Arguably the more challenging half of this project, the backend would primarily serve to be the game engine of the minesweeper game. Additionally, the backend would also control and organize the various pages and menus on the website. In this writeup, we will only be focusing on the main goals and challenges faced by the Backend team. . Using Object Oriented Programming to create individual objects for the game. Some aspects that could be represented by such objects would be the gameboard and also the individual cells and nodes present within the game board. Doing so would simplify the algorithmic aspect of the game, as the board itself could have methods and attributes to help control the game logic. | Morever, using an object to represent an individual cell in the board would provide greater functionalities than just using a single variable | | Use Enumerated types with the python enum module to create different values for the type and status of each node. Each of these types could then be bound to a constant value which could then be printed on the screen . | Use Recursion to create an algorithm to recursively detect adjacent cells that are safe (AKA not mines). This algorithm would work in the following format . Maintain a list of current cells already determined to be safe | Verify if the four adjacent (up, down, left, right) cells next to the selected cell are safe or not, if safe, store the coordinate point in the list, if not, record the cell as a &quot;border cell&quot;, terminate the recursive process, and run another helper function to determine the precise number of mines surrounding the cell | Re-call the function for each of the surrounding adjacent cells to identify other consecutive cells who are safe. | Return a list of the coordinates of a contiguous block of safe cells | Mark the cells to be safe and calculate number of surrounding mines for border cells. | | Verify the Game status, Game is won if: . All mines are flagged | All safe cells are cleared Game is lost if: | A mine was dug by the user | | Return the final result back to the front end. If a safe cell was dug, send out a JSON containing an array of the coordinates of the cell and it&#39;s safe neighbors. If a mine was dug, send a JSON containing a boolean value to signify the end of the game. . | CHALLENGE 1: OOP . Below is the currently implemented code for the gameboard and each individual node. . Here is the code for each individual node in the minesweeper: . from enum import Enum from random import randint . class MType(Enum): SAFE = 1 MINE = 2 class MStatus(Enum): UNKNOWN = 1 DUG = 2 FLAG = 3 class Node: def __init__(self): self.type = MType.SAFE self.status = MStatus.UNKNOWN self.value = &quot;-&quot; def setInitialValue(self): if self.type == MType.MINE: self.value=&quot;M&quot; . Here is the Code for the Minesweeper gameboard and its methods. . class NotInBoard(Exception): pass class Minesweeper: # Class constructor, containing game data def __init__(self): self.mine_coords = [] self.board = [] self.rows = 0 self.cols = 0 self.gameWin = False self.gameOver = False # Create the gameboard on a given input of rows and columns def generateBoard(self,r,c): self.rows = r self.cols = c self.board = [[Node() for i in range(self.rows)] for i in range(self.cols)] return self.board # Print the current board def printBoard(self): res = &quot;&quot; for row in self.board: concat = &quot;&quot; for column in row: concat = concat + column.value + &quot; &quot; concat = concat.strip() + &quot; n&quot; res = res+concat print(res) return 0 # Generate the coordinates of the mines AFTER the first user input # THE FIRST ACTION ON A CELL WOULD NEVER BE A MINE! def setMines(self): num_of_mines = (self.rows * self.cols)//4 counter = 0 while counter &lt;= num_of_mines: mine_coord = (randint(0,self.rows-1),randint(0,self.cols-1)) print(&quot;Mine coord: &quot;, mine_coord) if mine_coord in self.mine_coords: continue self.board[mine_coord[0]][mine_coord[1]].type = MType.MINE self.mine_coords.append(mine_coord) counter +=1 for i in range(self.rows): for j in range(self.cols): self.board[i][j].setInitialValue() return 0 # The first click of the game def firstClick(self): self.printBoard() try: ipt = input(&quot;What row and column? (format in row,column): &quot;) row, col = int(ipt.split(&quot;,&quot;)[0]), int(ipt.split(&quot;,&quot;)[1]) if row not in range(1,self.rows+1) or col not in range(1,self.cols+1): print(&quot;Values not in bound, try again&quot;) raise NotInBoard self.setMines() except NotInBoard: self.firstClick() # Following actions after the first-dig # checkAdkacent function will be created soon def digMine(self, r ,c): if self.board[r][c].type == MType.MINE: self.gameOver = True else: safe_mines = self.checkAdjacent(r,c) . CHALLENGE 2: Enumerated types . This part is kinda free tho ngl . from enum import Enum class MType(Enum): SAFE = 1 MINE = 2 class MStatus(Enum): UNKNOWN = 1 DUG = 2 FLAG = 3 . CHALLENGE 3: Recursive algorithm . Yet to be implemented . CHALLENGE 4: Yet to be implemented . Yet to be implemented . CHALLENGE 5: JSON communication . Yet to be implemented . Current Game Implementation . def main(): MS = Minesweeper() rows = int(input(&quot;How many rows do you want? &quot;)) cols = int(input(&quot;How many columns do you want? &quot;)) MS.generateBoard(cols, rows) MS.firstClick() MS.printBoard() # Define a while lop here to loop game. print(&quot;List of generated mine coordinates: &quot;, MS.mine_coords) main() . - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Mine coord: (3, 8) Mine coord: (9, 2) Mine coord: (4, 7) Mine coord: (7, 3) Mine coord: (9, 6) Mine coord: (8, 1) Mine coord: (5, 1) Mine coord: (7, 6) Mine coord: (5, 5) Mine coord: (8, 4) Mine coord: (7, 2) Mine coord: (4, 7) Mine coord: (8, 3) Mine coord: (2, 9) Mine coord: (0, 9) Mine coord: (5, 4) Mine coord: (7, 9) Mine coord: (2, 0) Mine coord: (0, 1) Mine coord: (4, 4) Mine coord: (4, 2) Mine coord: (6, 3) Mine coord: (0, 6) Mine coord: (3, 0) Mine coord: (9, 7) Mine coord: (8, 3) Mine coord: (4, 9) Mine coord: (9, 6) Mine coord: (3, 9) - M - - - - M - - M - - - - - - - - - - M - - - - - - - - M M - - - - - - - M M - - M - M - - M - M - M - - M M - - - - - - - M - - - - - - - - M M - - M - - M - M - M M - - - - - - - M - - - M M - - List of generated mine coordinates: [(3, 8), (9, 2), (4, 7), (7, 3), (9, 6), (8, 1), (5, 1), (7, 6), (5, 5), (8, 4), (7, 2), (8, 3), (2, 9), (0, 9), (5, 4), (7, 9), (2, 0), (0, 1), (4, 4), (4, 2), (6, 3), (0, 6), (3, 0), (9, 7), (4, 9), (3, 9)] .",
            "url": "https://ylu-1258.github.io/YLu-Blog/jupyter/apcsp/week%205/2022/09/25/Project-Purpose.html",
            "relUrl": "/jupyter/apcsp/week%205/2022/09/25/Project-Purpose.html",
            "date": " • Sep 25, 2022"
        }
        
    
  
    
        ,"post53": {
            "title": "Javascript Table Generator.",
            "content": "| Javascript Kernel Usage | Javascript Table Generator | Coin Flip | . . Defining the Class Object and Classes array . function Class(name, period, timeStart, timeEnd, teacher) { this.name = name; this.period = period; this.timeStart = timeStart; this.timeEnd = timeEnd; this.teacher = teacher; this.grade = &quot;&quot;; } Class.prototype.set_grade = function(grade) { this.grade = grade } var Classes = [new Class(&quot;AP Physics C: Mechanics&quot;, &quot;1&quot;, &quot;8:35&quot;, &quot;9:44&quot;, &quot;Mr. Liao&quot;), new Class(&quot;AP Calculus BC&quot;, &quot;2&quot;, &quot;9:49&quot;, &quot;10:58&quot;, &quot;Mrs. Lanzi&quot;), new Class(&quot;US History&quot;, &quot;3&quot;, &quot;11:13&quot;, &quot;12:22&quot;, &quot;Mr. Smith&quot;), new Class(&quot;AP Computer Science: Principles&quot;, &quot;4&quot;, &quot;12:57&quot;, &quot;2:06&quot;, &quot;Mr. Mort (the goat)&quot;), new Class(&quot;AP Physics C: Mechanics&quot;, &quot;5&quot;, &quot;2:36&quot;, &quot;3:45&quot;, &quot;Mrs. Dafoe&quot;)]; Classes[0].set_grade(&quot;A&quot;) Classes[1].set_grade(&quot;B+&quot;) Classes[2].set_grade(&quot;A+&quot;) Classes[3].set_grade(&quot;A+&quot;) Classes[4].set_grade(&quot;A&quot;) console.log(Classes) . [ Class { name: &#39;AP Physics C: Mechanics&#39;, period: &#39;1&#39;, timeStart: &#39;8:35&#39;, timeEnd: &#39;9:44&#39;, teacher: &#39;Mr. Liao&#39;, grade: &#39;A&#39; }, Class { name: &#39;AP Calculus BC&#39;, period: &#39;2&#39;, timeStart: &#39;9:49&#39;, timeEnd: &#39;10:58&#39;, teacher: &#39;Mrs. Lanzi&#39;, grade: &#39;B+&#39; }, Class { name: &#39;US History&#39;, period: &#39;3&#39;, timeStart: &#39;11:13&#39;, timeEnd: &#39;12:22&#39;, teacher: &#39;Mr. Smith&#39;, grade: &#39;A+&#39; }, Class { name: &#39;AP Computer Science: Principles&#39;, period: &#39;4&#39;, timeStart: &#39;12:57&#39;, timeEnd: &#39;2:06&#39;, teacher: &#39;Mr. Mort (the goat)&#39;, grade: &#39;A+&#39; }, Class { name: &#39;AP Physics C: Mechanics&#39;, period: &#39;5&#39;, timeStart: &#39;2:36&#39;, timeEnd: &#39;3:45&#39;, teacher: &#39;Mrs. Dafoe&#39;, grade: &#39;A&#39; } ] . Constructing a table from the Classes Array . function toHtml (Classes) { // HTML Style is build using inline structure var style = ( &quot;background-color: rgb(8, 0, 124);&quot; + &quot;border-color: rgb(220, 220, 220);&quot; + &quot;color: rgb(220, 220, 220);&quot; ); // HTML Body of Table is build as a series of concatenations (+=) var body = &quot;&quot;; // Heading for Array Columns body += &quot;&lt;tr&gt;&quot;; body += &quot;&lt;th&gt;&lt;strong&gt;&quot; + &quot;Class&quot; + &quot;&lt;/strong&gt;&lt;/th&gt;&quot;; body += &quot;&lt;th&gt;&lt;strong&gt;&quot; + &quot;Period #&quot; + &quot;&lt;/strong&gt;&lt;/th&gt;&quot;; body += &quot;&lt;th&gt;&lt;strong&gt;&quot; + &quot;Starting Time&quot; + &quot;&lt;/strong&gt;&lt;/th&gt;&quot;; body += &quot;&lt;th&gt;&lt;strong&gt;&quot; + &quot;Ending Time&quot; + &quot;&lt;/strong&gt;&lt;/th&gt;&quot;; body += &quot;&lt;th&gt;&lt;strong&gt;&quot; + &quot;Teacher&quot; + &quot;&lt;/strong&gt;&lt;/th&gt;&quot;; body += &quot;&lt;th&gt;&lt;strong&gt;&quot; + &quot;Grade&quot; + &quot;&lt;/strong&gt;&lt;/th&gt;&quot;; body += &quot;&lt;/tr&gt;&quot;; // Data of Array, iterate through each row of compsci.classroom for (var row of Classes) { // tr for each row, a new line body += &quot;&lt;tr&gt;&quot;; // td for each column of data body += &quot;&lt;td&gt;&quot; + row.name + &quot;&lt;/td&gt;&quot;; body += &quot;&lt;td&gt;&quot; + row.period + &quot;&lt;/td&gt;&quot;; body += &quot;&lt;td&gt;&quot; + row.timeStart + &quot;&lt;/td&gt;&quot;; body += &quot;&lt;td&gt;&quot; + row.timeEnd + &quot;&lt;/td&gt;&quot;; body += &quot;&lt;td&gt;&quot; + row.teacher + &quot;&lt;/td&gt;&quot;; body += &quot;&lt;td&gt;&quot; + row.grade + &quot;&lt;/td&gt;&quot;; // tr to end line body += &quot;&lt;tr&gt;&quot;; } // Build and HTML fragment of div, table, table body return ( &quot;&lt;div style=&#39;&quot; + style + &quot;&#39;&gt;&quot; + &quot;&lt;table&gt;&quot; + body + &quot;&lt;/table&gt;&quot; + &quot;&lt;/div&gt;&quot; ); }; // IJavaScript HTML processor receive parameter of defined HTML fragment $$.html(toHtml(Classes)); . ClassPeriod #Starting TimeEnding TimeTeacherGrade . AP Physics C: Mechanics | 1 | 8:35 | 9:44 | Mr. Liao | A | AP Calculus BC | 2 | 9:49 | 10:58 | Mrs. Lanzi | B+ | US History | 3 | 11:13 | 12:22 | Mr. Smith | A+ | AP Computer Science: Principles | 4 | 12:57 | 2:06 | Mr. Mort (the goat) | A+ | AP Physics C: Mechanics | 5 | 2:36 | 3:45 | Mrs. Dafoe | A | &lt;/table&gt;&lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; . .",
            "url": "https://ylu-1258.github.io/YLu-Blog/jupyter/apcsp/week%205/2022/09/25/JavaScript-Table.html",
            "relUrl": "/jupyter/apcsp/week%205/2022/09/25/JavaScript-Table.html",
            "date": " • Sep 25, 2022"
        }
        
    
  
    
        ,"post54": {
            "title": "JavaScript Writeup",
            "content": "| Javascript Kernel Usage | Javascript Table Generator | Coin Flip | . . What is Javascript? . Javascript is a fronend programming language that drives the web and internet. Every interaction on a website could be traced back to javascript and its functionalities. . Resources: W3Schools feecodecamp.org javascript30.com . Console.log output . Console.log is similar to the print() function in python. However, running this command does not alter the content of the webpgae in anyway. The output of this command actually outputs to a terminal &quot;Console&quot; on the webpage. This could be useful for debugging or reviewing our code. . console.log(&quot;Hello World!&quot;) . Hello World! . Variables . Variables exist in Javascript much similar to variables in python. We could define a variable in our javascript with the var keyword. . var msg = &quot;Hello World!&quot;; console.log(msg); . Hello World! . Functions . Functions could be defined in Javascript with the function keyword, a function name, and arguments to the function. Moreover, the code within the function would need to be wrapped in curly braces (squigglies). . function logIt(ipt) { console.log(ipt) } logIt(&quot;Hello World&quot;); logIt(&quot;Foo Bar!&quot;) logIt(1337) . Hello World Foo Bar! 1337 . Dynamically and loosely typed language (string, number) . Javascript is a loosely typed language which means that the type of information to be stored does not need to be specified in advance. . function logItType(output) { console.log(typeof output, &quot;:&quot;, output); } console.log(&quot;Looking at dynamic nature of types in JavaScript&quot;) logItType(&quot;hello&quot;); // String logItType(2020); // Number logItType([1, 2, 3]); // Object is generic for this Array, which similar to Python List . Looking at dynamic nature of types in JavaScript string : hello number : 2020 object : [ 1, 2, 3 ] . Building a function class . The this keyword refers to the function itself, similar to self in python. The variables defined in this way are isolated within their own scope. . Moreover, the JSON string created in the javascript file could be used as a message to communicate between frontend and backend. The JSON is a way to serialize data to transfer it over a network. Meaning we could use JSON to communicate between javascript programs and python programs. . // define a function to hold data for a Person function Person(name, ghID, classOf) { this.name = name; this.ghID = ghID; this.classOf = classOf; this.role = &quot;&quot;; } // define a setter for role in Person data Person.prototype.setRole = function(role) { this.role = role; } // define a JSON conversion &quot;method&quot; associated with Person Person.prototype.toJSON = function() { const obj = {name: this.name, ghID: this.ghID, classOf: this.classOf, role: this.role}; const json = JSON.stringify(obj); return json; } // make a new Person and assign to variable teacher var teacher = new Person(&quot;Mr M&quot;, &quot;jm1021&quot;, 1977); // object type is easy to work with in JavaScript logItType(teacher); // before role logItType(teacher.toJSON()); // ok to do this even though role is not yet defined // output of Object and JSON/string associated with Teacher teacher.setRole(&quot;Teacher&quot;); // set the role logItType(teacher); logItType(teacher.toJSON()); . object : Person { name: &#39;Mr M&#39;, ghID: &#39;jm1021&#39;, classOf: 1977, role: &#39;&#39; } string : {&#34;name&#34;:&#34;Mr M&#34;,&#34;ghID&#34;:&#34;jm1021&#34;,&#34;classOf&#34;:1977,&#34;role&#34;:&#34;&#34;} object : Person { name: &#39;Mr M&#39;, ghID: &#39;jm1021&#39;, classOf: 1977, role: &#39;Teacher&#39; } string : {&#34;name&#34;:&#34;Mr M&#34;,&#34;ghID&#34;:&#34;jm1021&#34;,&#34;classOf&#34;:1977,&#34;role&#34;:&#34;Teacher&#34;} .",
            "url": "https://ylu-1258.github.io/YLu-Blog/jupyter/apcsp/week%204/2022/09/21/JavaScript-Tutorial.html",
            "relUrl": "/jupyter/apcsp/week%204/2022/09/21/JavaScript-Tutorial.html",
            "date": " • Sep 21, 2022"
        }
        
    
  
    
        ,"post55": {
            "title": "Psychology and a Good Life",
            "content": "BIG IDEA 1: This enterprise is possible. . We can control and improve our happiness a lot more than we think. . Happiness may be somewhat herritable, but it is still mostly in our control and it’s our decisions that dictate our happiness. | If our genes control 50% of our happiness, then it stands to reason that we have control over the other 50%. | . BIG IDEA 2: We think life’s circumstances matter a ton, but they don’t matter nearly as much as we think . Rent, work, and School may be important, but are they more important to us than happiness? . Lottery winners are happy on the day they win the lottery But months later, lottery winners have the same happiness as they did before winning | . | The opposite applies to bad scenarios Unfortunate events may cause frustration and sadness in the instant they happen, but they will dissipate quickly over time too | We as humans get used to it | . | Individuals who experience catastrophic events are more likely to view life with a positive attitude, viewing their life as more precious | . BIG IDEA 3: Taking control of it, is kind of hard . Just how hard is it to be happy? . We can control our happiness through our behaviors | It takes daily work and effort to be happy Unhappy people can do happy things everyday and persevere that way to feel happy | . | Any big event in life requires effort, happiness is the same | A change in happiness requires permenant changes that persist every day of our life | training happiness is similar to exercising | The GI Joe fallacy: Knowing is half the battle. Just listening to something isn’t enough, you must apply it. | . BIG IDEA 4: One of the hard you have to do when you begin this enterprise, is recognizing your intuition don’t work . Our mind is lying to us . Our mind forcasts things that makes us happy, but in reality it does not | People correlate salary (money) with happiness People at different salary levels have different expectations, 30k -&gt; 50k -&gt; 100k … etc | expectations aren’t linear, it grows exponentially | . | Salary vs happiness levels off quickly | People who seek out material posessions are more likely to have lesser wellbeing, | . BIG IDEA 5: Make time for social connection . Very happy people prioritize social connections . Very happy people spend more time with others and less time alone Prioritize the stuff that works | Spend time with friends and families | . | Unhappy people spend more time alone than happy people | Experiments that spend time with strangers also showed that connecting with others showed significantly greater positivity than staying alone | .",
            "url": "https://ylu-1258.github.io/YLu-Blog/markdown/apcsp/human_day/2022/09/15/Psychology-and-a-good-life.html",
            "relUrl": "/markdown/apcsp/human_day/2022/09/15/Psychology-and-a-good-life.html",
            "date": " • Sep 15, 2022"
        }
        
    
  
    
        ,"post56": {
            "title": "College Board Create Performance Task",
            "content": "College Board Create Performance Task . Plan and Ideas . A program that creates a hang-man game for the user. . Row 1: Program Purpose and Function . Assesses students’ ability to explain how a code segment or program functions. Know the difference between program purpose and function The purpoes and function is what the program was meant to do Examples: The purpose of the Quiz program: To assess knowledge on a topic. The purpose of InfoDb: To store and access user information. . The purpose of this game is to create a word guessing game, where the user guesses certain letters, and the program will automatically fill in any letters present in the target word that the user inputs. After a given amount of tries, if the user hasn’t guessed the word, the game will fail itself. . Row 2: Data Abstraction . Assesses students’ ability to use abstraction to manage complexity in a program. The Abstraction and storage of data in a generalized container, e.g. variables, lists, dictionaries . The hangman game will predominantly use lists and variables to store the necessary data for the game. The variables could hold the different “stages” of the hangman ascii-art. On the otherhand, lists could be used to keep track of the already used letters, the target word, and the letters that were found in the target word. DOing so would allow for easy manipulation of the data stored and processed by the user. . Row 3: Managing Complexity . Assesses students’ ability to explain how abstraction manages complexity. The print_data function in the InfoDb project helps to reduce the amount of code that has to be written, as the code under print_data was abstracted from the function that was pre-defined. . Helps to reduce complexity in the code as everything is compartmentalized. . Individual functions and classes could be created to compartmentalize certain processes to reduce the clutter and repitition in the code. One function could be used to verify the prescence of a letter in a word, another function could be used to check if the game has been won, and a final main function could be made with a loop that runs the overall game. . Row 4: Prodecural Abstraction . Assesses students’ ability to use abstraction to manage complexity in a program. The print_data function used in the InfoDb project was abstracted into the various for loops that the program used. . The many functions and classes used in the hangman game could be abstracted in the program to manage the overall complexity of the game. . Row 5: Algorithm Implementation . Assesses students’ ability to implement and apply an algorithm. An algorithm is a methodology or a logical path that a computer follows to properly, . Ex: The percentage of the score in the quiz was an example of an algorithm Some algorithms that could be implemented in this project could be a way of keeping track of total number of attempts taken or checking if a game has been won, (e.g. all of the spaces have been filled out with letters) . Row 6: Testing . Assesses students’ ability to investigate the situation, context, or task. Testing is running over all the processes in a program and ensuring that everything works. The ability to find, detect, and fix any bugs apparent in the program. . Some testing that could be done on the project could be testing for accepting inputs from the user (subsituting a letter in when it’s found to be correct), or testing the proper game end and game start events. . How to Get a Five . REVIEW PAST NOTES FROM CLASS . Be sure to be clear on the vocab and content taught in the class. Be parepared to be able to properly define each vocabulary and provide examples to each vocab. Any questions that cannot be answered with my notes, ask the teammates first and then consult the internet. If the problems still remains unsolved, consult the teacher about it. . LOOK AT PREVIOUS COLLEGEBOARD SUBMISSIONS AND ASSIGNMENTS . Learn from the past submissions of other people, plan ahead of time what to consider and what to add to my project. .",
            "url": "https://ylu-1258.github.io/YLu-Blog/markdown/apcsp/week%202/2022/09/08/College-Board-Performance-Task.html",
            "relUrl": "/markdown/apcsp/week%202/2022/09/08/College-Board-Performance-Task.html",
            "date": " • Sep 8, 2022"
        }
        
    
  
    
        ,"post57": {
            "title": "Applab Quiz Write up",
            "content": "What is AppLab? . App Lab is a programming environment where you can make simple apps. Design an app, code in JavaScript with either blocks or text, then share your app with Teacher or Student Peers. The big limitations of App Lab, that makes it simple, is that the HTML and CSS are not available to developer/student coder, thus it is not directly transportable to an independent Web server. . Code.org account created. To create a new project, click on the AppLab section on the main page. . The block code used in code.org AppLab is a simplified version of the JavaScript language. Click on the Show Text button to edit the code directly. . Plan for the Quiz (Design) . ✔️ Verify answer . ✔️ Inccorect Answer Screen . ✔️ Counter to keep track of incorrect account . ✔️ Variable to keep track of current problem . ✔️ Free Response question prompts . Link to the quiz . . Successes . Was able to create a quiz where one screen was able to dynamically able to backtrack to the previous answer. | Free response input is now also supported by the quiz. | Background counter helps to keep a track of the number of inccorect attempts made. | Free response questions can accept multiple different forms of input. | . Discoveries / Challenges . Realized that you can convert between Int and string types with the toString() function. | Struggled to find a way to concatenate strings and integers but then found about about the concat() function. | The score box seemed to reset, realized that I just made the text box too small :/ | .",
            "url": "https://ylu-1258.github.io/YLu-Blog/markdown/apcsp/week%202/2022/09/07/Lecture-code-org-applab.html",
            "relUrl": "/markdown/apcsp/week%202/2022/09/07/Lecture-code-org-applab.html",
            "date": " • Sep 7, 2022"
        }
        
    
  
    
        ,"post58": {
            "title": "HTML writeup",
            "content": "Changing theme to Jekyll Midnight . The overall theme of the blog could be changed under the _config.yml file in the home directory of the blog. One can either edit the theme key pair value, or the remote_theme keypair value. However, the latter option must have the value jekyll-remote-theme defined under the plugins key. I decided to use the midnight theme becuase it looks cool as heck. . However, the first major problem I ran into was a porblem with my jekyll build, looking closer at the error message, I realized that there was a “Download Error”, presumably some problems occured while trying to install the new midnight theme. . . However, after doing some more research, I realized that the version of the theme must be specified with the @ at the end of the value, I initially didn’t supply this value, and thus received the error. After specifying the latest version, The Jekyll build passed, and my blog looked like this: . Top half of blog: . Bottom half of blog: . One immediate problems I noticed was that the chagne in theme completely removed parts of my blog’s navigation bar, as the tags and lecutre notes tab were no longer present at the top. This finding shows that other Jekyll themes are not fully compatible with the fastpages backend, and that the minima theme is prefered for compatibility. .",
            "url": "https://ylu-1258.github.io/YLu-Blog/markdown/apcsp/week%202/2022/09/03/HTML-writeup.html",
            "relUrl": "/markdown/apcsp/week%202/2022/09/03/HTML-writeup.html",
            "date": " • Sep 3, 2022"
        }
        
    
  
    
        ,"post59": {
            "title": "HTML Fragments",
            "content": "HTML Fragments and Markdown . HTML fragments are just a part of the total HTML source on our site. . Table Fragments Fastpages allows to build tables in HTML or Markdown. Everything on the site is built in HTML, the markdown is converted into individual HTML fragments which are assembled to form webpage. Tables that we create and use in markdown are also read, interpreted and turned into html tables. . tl;dr EVERYTHING ON THE WEBPAGE IS IN HTML . images “” element in HTML allows us to specify cetain image parameters, more flexible than markdown method. . links “” element with href in HTML . Web Page Layout . Design themes and layouts of the page. . In the _config.yml page, The theme can be changed with the remote_theme: , which could change the coloration of the text . NOTE: Fastpages only works with the Jekyll/minima theme, and not with any other theme . Additional coloration and customization could be attained with the sass (sassy) .",
            "url": "https://ylu-1258.github.io/YLu-Blog/markdown/apcsp/week%202/2022/08/31/HTML-fragments.html",
            "relUrl": "/markdown/apcsp/week%202/2022/08/31/HTML-fragments.html",
            "date": " • Aug 31, 2022"
        }
        
    
  
    
        ,"post60": {
            "title": "Summer Recommender System Algo Research",
            "content": "Import all of our required modules . import pandas as pd import numpy as np !pip install scikit-surprise from surprise import Dataset from surprise import Reader import scipy . Requirement already satisfied: scikit-surprise in /usr/local/lib/python3.7/dist-packages (1.1.1) Requirement already satisfied: scipy&gt;=1.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-surprise) (1.4.1) Requirement already satisfied: numpy&gt;=1.11.2 in /usr/local/lib/python3.7/dist-packages (from scikit-surprise) (1.19.5) Requirement already satisfied: six&gt;=1.10.0 in /usr/local/lib/python3.7/dist-packages (from scikit-surprise) (1.15.0) Requirement already satisfied: joblib&gt;=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-surprise) (1.0.1) . Import and process our data . ## Importing our ratings data ratings = pd.read_csv(&quot;rating.csv&quot;) anime = pd.read_csv(&quot;anime.csv&quot;) ## removing entries with no ratings and resetting indices ratings = ratings.head(125000) ## Apply to_numeric and removing all NaN values ratings = ratings.apply(pd.to_numeric, errors=&#39;coerce&#39;) ratings = ratings.dropna() ## Declaring some stuff for KNN calculation reader = Reader(rating_scale=(1, 10)) data = Dataset.load_from_df(ratings[[&quot;user_id&quot;, &quot;anime_id&quot;, &quot;rating&quot;]], reader) ratings . user_id anime_id rating . 0 1 | 20 | -1 | . 1 1 | 24 | -1 | . 2 1 | 79 | -1 | . 3 1 | 226 | -1 | . 4 1 | 241 | -1 | . ... ... | ... | ... | . 124995 1272 | 4382 | 10 | . 124996 1272 | 4672 | 5 | . 124997 1272 | 4792 | 6 | . 124998 1272 | 4898 | 8 | . 124999 1272 | 4910 | 6 | . 125000 rows × 3 columns . KNN . ratings[[&quot;user_id&quot;, &quot;anime_id&quot;, &quot;rating&quot;]] . user_id anime_id rating . 0 1 | 20 | -1 | . 1 1 | 24 | -1 | . 2 1 | 79 | -1 | . 3 1 | 226 | -1 | . 4 1 | 241 | -1 | . ... ... | ... | ... | . 124995 1272 | 4382 | 10 | . 124996 1272 | 4672 | 5 | . 124997 1272 | 4792 | 6 | . 124998 1272 | 4898 | 8 | . 124999 1272 | 4910 | 6 | . 125000 rows × 3 columns . from surprise import KNNWithMeans sim_options = { &quot;name&quot;: &quot;msd&quot;, &quot;min_support&quot;: 3, &quot;user_based&quot;: False, # Compute similarities between items } algo = KNNWithMeans(sim_options=sim_options) user_id = input(&quot;User Id &gt;&gt;&gt; &quot;) anime_id = input(&quot;Anime Id &gt;&gt;&gt; &quot;) trainingSet = data.build_full_trainset() algo.fit(trainingSet) prediction = algo.predict(int(user_id), int(anime_id)) from google.colab import output output.clear() print(&quot;User {0} would rate anime number {1}&quot;.format(str(user_id), str(anime_id))) prediction.est . User 3 would rate anime number 101 . 6.695240884293928 . from surprise import KNNWithZScore sim_options = { &quot;name&quot;: &quot;msd&quot;, &quot;min_support&quot;: 3, &quot;user_based&quot;: False, # Compute similarities between items } algo = KNNWithZScore(sim_options=sim_options) user_id = input(&quot;User Id &gt;&gt;&gt; &quot;) anime_id = input(&quot;Anime Id &gt;&gt;&gt; &quot;) trainingSet = data.build_full_trainset() algo.fit(trainingSet) prediction = algo.predict(int(user_id), int(anime_id)) from google.colab import output output.clear() print(&quot;User {0} would rate anime number {1}&quot;.format(str(user_id), str(anime_id))) prediction.est . User 3 would rate anime number 101 . 6.702780614370237 . SVD algorithm . Basic algorithms NormalPredictor NormalPredictor algorithm predicts a random rating based on the distribution of the training set, which is assumed to be normal. This is one of the most basic algorithms that do not do much work. BaselineOnly BaselineOnly algorithm predicts the baseline estimate for given user and item. k-NN algorithms KNNBasic KNNBasic is a basic collaborative filtering algorithm. KNNWithMeans KNNWithMeans is basic collaborative filtering algorithm, taking into account the mean ratings of each user. KNNWithZScore KNNWithZScore is a basic collaborative filtering algorithm, taking into account the z-score normalization of each user. KNNBaseline KNNBaseline is a basic collaborative filtering algorithm taking into account a baseline rating. Matrix Factorization-based algorithms SVD SVD algorithm is equivalent to Probabilistic Matrix Factorization SVDpp The SVDpp algorithm is an extension of SVD that takes into account implicit ratings. NMF NMF is a collaborative filtering algorithm based on Non-negative Matrix Factorization. It is very similar with SVD. Slope One SlopeOne is a straightforward implementation of the SlopeOne algorithm. Co-clustering Coclustering is a collaborative filtering algorithm based on co-clustering. We use “rmse” as our accuracy metric for the predictions. . KNNWithMeans . from surprise import KNNWithMeans from surprise import Dataset from surprise.model_selection import GridSearchCV data = Dataset.load_from_df(ratings[[&quot;user_id&quot;, &quot;anime_id&quot;, &quot;rating&quot;]], reader) sim_options = { &quot;name&quot;: [&quot;msd&quot;, &quot;cosine&quot;], &quot;min_support&quot;: [3, 4, 5], &quot;user_based&quot;: [False, True], } param_grid = {&quot;sim_options&quot;: sim_options} gs = GridSearchCV(KNNWithMeans, param_grid, measures=[&quot;rmse&quot;, &quot;mae&quot;], cv=3) gs.fit(data) print(gs.best_score[&quot;rmse&quot;]) print(gs.best_params[&quot;rmse&quot;]) print(gs.best_score[&quot;mae&quot;]) print(gs.best_params[&quot;mae&quot;]) # Cleaned # rmse 1.1961042065214869 # {&#39;sim_options&#39;: {&#39;name&#39;: &#39;msd&#39;, &#39;min_support&#39;: 3, &#39;user_based&#39;: True}} # mae 0.9066350800152176 #{&#39;sim_options&#39;: {&#39;name&#39;: &#39;msd&#39;, &#39;min_support&#39;: 3, &#39;user_based&#39;: False}} # Original #2.300630451838356 #{&#39;sim_options&#39;: {&#39;name&#39;: &#39;msd&#39;, &#39;min_support&#39;: 4, &#39;user_based&#39;: True}} #1.6207772901276576 #{&#39;sim_options&#39;: {&#39;name&#39;: &#39;msd&#39;, &#39;min_support&#39;: 5, &#39;user_based&#39;: True}} # runtime 4m 28s . Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the cosine similarity matrix... Done computing similarity matrix. Computing the cosine similarity matrix... Done computing similarity matrix. Computing the cosine similarity matrix... Done computing similarity matrix. Computing the cosine similarity matrix... Done computing similarity matrix. Computing the cosine similarity matrix... Done computing similarity matrix. Computing the cosine similarity matrix... Done computing similarity matrix. Computing the cosine similarity matrix... Done computing similarity matrix. Computing the cosine similarity matrix... Done computing similarity matrix. Computing the cosine similarity matrix... Done computing similarity matrix. Computing the cosine similarity matrix... Done computing similarity matrix. Computing the cosine similarity matrix... Done computing similarity matrix. Computing the cosine similarity matrix... Done computing similarity matrix. Computing the cosine similarity matrix... Done computing similarity matrix. Computing the cosine similarity matrix... Done computing similarity matrix. Computing the cosine similarity matrix... Done computing similarity matrix. Computing the cosine similarity matrix... Done computing similarity matrix. Computing the cosine similarity matrix... Done computing similarity matrix. Computing the cosine similarity matrix... Done computing similarity matrix. 2.300630451838356 {&#39;sim_options&#39;: {&#39;name&#39;: &#39;msd&#39;, &#39;min_support&#39;: 4, &#39;user_based&#39;: True}} 1.6207772901276576 {&#39;sim_options&#39;: {&#39;name&#39;: &#39;msd&#39;, &#39;min_support&#39;: 5, &#39;user_based&#39;: True}} . KNNWithZScore . from surprise import KNNWithZScore from surprise import Dataset from surprise.model_selection import GridSearchCV data = Dataset.load_from_df(ratings[[&quot;user_id&quot;, &quot;anime_id&quot;, &quot;rating&quot;]], reader) sim_options = { &quot;name&quot;: [&quot;msd&quot;, &quot;cosine&quot;], &quot;min_support&quot;: [3, 4, 5], &quot;user_based&quot;: [False, True], } param_grid = {&quot;sim_options&quot;: sim_options} gs = GridSearchCV(KNNWithZScore, param_grid, measures=[&quot;rmse&quot;, &quot;mae&quot;], cv=3) gs.fit(data) print(gs.best_score[&quot;rmse&quot;]) print(gs.best_params[&quot;rmse&quot;]) print(gs.best_score[&quot;mae&quot;]) print(gs.best_params[&quot;mae&quot;]) # Cleaned #rmse 1.2200524149502934 #{&#39;sim_options&#39;: {&#39;name&#39;: &#39;msd&#39;, &#39;min_support&#39;: 3, &#39;user_based&#39;: True}} #mae 0.9071607750307225 #{&#39;sim_options&#39;: {&#39;name&#39;: &#39;msd&#39;, &#39;min_support&#39;: 4, &#39;user_based&#39;: True}} # Original #2.3157416009374785 #{&#39;sim_options&#39;: {&#39;name&#39;: &#39;cosine&#39;, &#39;min_support&#39;: 4, &#39;user_based&#39;: True}} #1.608562753271584 #{&#39;sim_options&#39;: {&#39;name&#39;: &#39;cosine&#39;, &#39;min_support&#39;: 4, &#39;user_based&#39;: True}} #runtime 9m 28s . Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the cosine similarity matrix... Done computing similarity matrix. Computing the cosine similarity matrix... Done computing similarity matrix. Computing the cosine similarity matrix... Done computing similarity matrix. Computing the cosine similarity matrix... Done computing similarity matrix. Computing the cosine similarity matrix... Done computing similarity matrix. Computing the cosine similarity matrix... Done computing similarity matrix. Computing the cosine similarity matrix... Done computing similarity matrix. Computing the cosine similarity matrix... Done computing similarity matrix. Computing the cosine similarity matrix... Done computing similarity matrix. Computing the cosine similarity matrix... Done computing similarity matrix. Computing the cosine similarity matrix... Done computing similarity matrix. Computing the cosine similarity matrix... Done computing similarity matrix. Computing the cosine similarity matrix... Done computing similarity matrix. Computing the cosine similarity matrix... Done computing similarity matrix. Computing the cosine similarity matrix... Done computing similarity matrix. Computing the cosine similarity matrix... Done computing similarity matrix. Computing the cosine similarity matrix... Done computing similarity matrix. Computing the cosine similarity matrix... Done computing similarity matrix. 2.3157416009374785 {&#39;sim_options&#39;: {&#39;name&#39;: &#39;cosine&#39;, &#39;min_support&#39;: 4, &#39;user_based&#39;: True}} 1.608562753271584 {&#39;sim_options&#39;: {&#39;name&#39;: &#39;cosine&#39;, &#39;min_support&#39;: 4, &#39;user_based&#39;: True}} . BaselineOnly . from surprise import BaselineOnly from surprise import Dataset from surprise.model_selection import GridSearchCV # Misc data = Dataset.load_from_df(ratings[[&quot;user_id&quot;, &quot;anime_id&quot;, &quot;rating&quot;]], reader) bsl_options = {&#39;method&#39;: [&#39;als&#39;, &#39;sgd&#39;], &#39;n_epochs&#39;: [1,5,10], &#39;reg_u&#39;: [12], &#39;reg_i&#39;: [5] } param_grid = {&quot;bsl_options&quot;: bsl_options} gs = GridSearchCV(BaselineOnly, param_grid, measures=[&quot;rmse&quot;, &quot;mae&quot;], cv=3) gs.fit(data) print(gs.best_score[&quot;rmse&quot;]) print(gs.best_params[&quot;rmse&quot;]) print(gs.best_score[&quot;mae&quot;]) print(gs.best_params[&quot;mae&quot;]) # Original # rmse 1.1937039196145423 #{&#39;bsl_options&#39;: {&#39;method&#39;: &#39;als&#39;, &#39;n_epochs&#39;: 10, &#39;reg_u&#39;: 12, &#39;reg_i&#39;: 5}} # mae 0.9073417853583354 #{&#39;bsl_options&#39;: {&#39;method&#39;: &#39;als&#39;, &#39;n_epochs&#39;: 10, &#39;reg_u&#39;: 12, &#39;reg_i&#39;: 5}} # Cleaned #2.2457828423033965 #{&#39;bsl_options&#39;: {&#39;method&#39;: &#39;sgd&#39;, &#39;n_epochs&#39;: 10, &#39;reg_u&#39;: 12, &#39;reg_i&#39;: 5}} #1.6065790249351644 #{&#39;bsl_options&#39;: {&#39;method&#39;: &#39;sgd&#39;, &#39;n_epochs&#39;: 10, &#39;reg_u&#39;: 12, &#39;reg_i&#39;: 5}} # runtime 12s . Estimating biases using als... Estimating biases using als... Estimating biases using als... Estimating biases using als... Estimating biases using als... Estimating biases using als... Estimating biases using als... Estimating biases using als... Estimating biases using als... Estimating biases using sgd... Estimating biases using sgd... Estimating biases using sgd... Estimating biases using sgd... Estimating biases using sgd... Estimating biases using sgd... Estimating biases using sgd... Estimating biases using sgd... Estimating biases using sgd... 2.2457828423033965 {&#39;bsl_options&#39;: {&#39;method&#39;: &#39;sgd&#39;, &#39;n_epochs&#39;: 10, &#39;reg_u&#39;: 12, &#39;reg_i&#39;: 5}} 1.6065790249351644 {&#39;bsl_options&#39;: {&#39;method&#39;: &#39;sgd&#39;, &#39;n_epochs&#39;: 10, &#39;reg_u&#39;: 12, &#39;reg_i&#39;: 5}} . SVD . from surprise import SVD from surprise import Dataset from surprise.model_selection import GridSearchCV # Matrix facotization sim_options = { &quot;n_epochs&quot;: [5,10], &quot;lr_all&quot;: [0.002, 0.005], &quot;reg_all&quot;: [0.4,0.6] } gs = GridSearchCV(SVD, sim_options, measures=[&quot;rmse&quot;, &quot;mae&quot;], cv=3) gs.fit(data) print(gs.best_score[&quot;rmse&quot;]) print(gs.best_params[&quot;rmse&quot;]) print(gs.best_score[&quot;mae&quot;]) print(gs.best_params[&quot;mae&quot;]) # Original #rmse 1.2370006802584665 #{&#39;n_epochs&#39;: 10, &#39;lr_all&#39;: 0.005, &#39;reg_all&#39;: 0.4} #mae 0.9457554878434046 #{&#39;n_epochs&#39;: 10, &#39;lr_all&#39;: 0.005, &#39;reg_all&#39;: 0.4} # Cleaned #2.3105571863770495 #{&#39;n_epochs&#39;: 10, &#39;lr_all&#39;: 0.005, &#39;reg_all&#39;: 0.4} #1.7100669607848105 #{&#39;n_epochs&#39;: 10, &#39;lr_all&#39;: 0.005, &#39;reg_all&#39;: 0.4} #runtime 1m 44s . 2.3105571863770495 {&#39;n_epochs&#39;: 10, &#39;lr_all&#39;: 0.005, &#39;reg_all&#39;: 0.4} 1.7100669607848105 {&#39;n_epochs&#39;: 10, &#39;lr_all&#39;: 0.005, &#39;reg_all&#39;: 0.4} . KNNBasic . from surprise import KNNBasic from surprise import Dataset from surprise.model_selection import GridSearchCV # Clustering sim_options = { &quot;name&quot;: [&quot;msd&quot;, &quot;cosine&quot;], &quot;k&quot;: [20,40], &quot;min_k&quot;: [5, 7], &quot;verbose&quot;: [True] } gs = GridSearchCV(KNNBasic, sim_options, measures=[&quot;rmse&quot;, &quot;mae&quot;], cv=3) gs.fit(data) print(gs.best_score[&quot;rmse&quot;]) print(gs.best_params[&quot;rmse&quot;]) print(gs.best_score[&quot;mae&quot;]) print(gs.best_params[&quot;mae&quot;]) # Cleaned #rmse 1.3169600252251181 #{&#39;name&#39;: &#39;msd&#39;, &#39;k&#39;: 20, &#39;min_k&#39;: 5, &#39;verbose&#39;: True} #mae 0.9971319510312137 #{&#39;name&#39;: &#39;msd&#39;, &#39;k&#39;: 20, &#39;min_k&#39;: 5, &#39;verbose&#39;: True} # Original #2.61306062703497 #{&#39;name&#39;: &#39;msd&#39;, &#39;k&#39;: 20, &#39;min_k&#39;: 5, &#39;verbose&#39;: True} #1.7634939088675168 #{&#39;name&#39;: &#39;msd&#39;, &#39;k&#39;: 20, &#39;min_k&#39;: 5, &#39;verbose&#39;: True} # 4m 53 s . Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. 2.61306062703497 {&#39;name&#39;: &#39;msd&#39;, &#39;k&#39;: 20, &#39;min_k&#39;: 5, &#39;verbose&#39;: True} 1.7634939088675168 {&#39;name&#39;: &#39;msd&#39;, &#39;k&#39;: 20, &#39;min_k&#39;: 5, &#39;verbose&#39;: True} . NMF . from surprise import NMF from surprise import Dataset from surprise.model_selection import GridSearchCV # Matrix Factorization sim_options = { &quot;n_epochs&quot;: [5,10,15,20], &quot;biased&quot;: [True,False], &quot;reg_pu&quot;: [0.01, 0.06, 0.5, 1], &quot;reg_qi&quot;: [0.01, 0.06, 0.5, 1], &quot;verbose&quot;: [True] } gs = GridSearchCV(NMF, sim_options, measures=[&quot;rmse&quot;, &quot;mae&quot;], cv=3) gs.fit(data) print(gs.best_score[&quot;rmse&quot;]) print(gs.best_params[&quot;rmse&quot;]) print(gs.best_score[&quot;mae&quot;]) print(gs.best_params[&quot;mae&quot;]) # Cleaned #1.1889671285702674 #{&#39;n_epochs&#39;: 20, &#39;biased&#39;: True, &#39;reg_pu&#39;: 0.01, &#39;reg_qi&#39;: 1, &#39;verbose&#39;: True} #0.9011699311141764 #{&#39;n_epochs&#39;: 20, &#39;biased&#39;: True, &#39;reg_pu&#39;: 0.01, &#39;reg_qi&#39;: 1, &#39;verbose&#39;: True} # Original #2.2452077636572434 #{&#39;n_epochs&#39;: 10, &#39;biased&#39;: True, &#39;reg_pu&#39;: 0.5, &#39;reg_qi&#39;: 0.5, &#39;verbose&#39;: True} #1.6074999998727018 #{&#39;n_epochs&#39;: 20, &#39;biased&#39;: True, &#39;reg_pu&#39;: 1, &#39;reg_qi&#39;: 1, &#39;verbose&#39;: True} #runtime 11m 25s . Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 2.2452077636572434 {&#39;n_epochs&#39;: 10, &#39;biased&#39;: True, &#39;reg_pu&#39;: 0.5, &#39;reg_qi&#39;: 0.5, &#39;verbose&#39;: True} 1.6074999998727018 {&#39;n_epochs&#39;: 20, &#39;biased&#39;: True, &#39;reg_pu&#39;: 1, &#39;reg_qi&#39;: 1, &#39;verbose&#39;: True} . Normal Predictor . from surprise import NormalPredictor from surprise import Dataset from surprise.model_selection import GridSearchCV # Misc sim_options = { } gs = GridSearchCV(NormalPredictor, sim_options, measures=[&quot;rmse&quot;, &quot;mae&quot;], cv=3) gs.fit(data) print(gs.best_score[&quot;rmse&quot;]) print(gs.best_params[&quot;rmse&quot;]) print(gs.best_score[&quot;mae&quot;]) print(gs.best_params[&quot;mae&quot;]) # Cleaned #rmse 2.0923401858242667 # {} #mae 1.6582413347539269 # {} # Original #4.876980925979471 #{} #3.9231832724710536 #{} #runtime 2s . 4.876980925979471 {} 3.9231832724710536 {} . SlopeOne . from surprise import SlopeOne from surprise import Dataset from surprise.model_selection import GridSearchCV # Clustering param_grid = { } gs = GridSearchCV(SlopeOne, param_grid, measures=[&quot;rmse&quot;, &quot;mae&quot;], cv=3) gs.fit(data) print(gs.best_score[&quot;rmse&quot;]) print(gs.best_params[&quot;rmse&quot;]) print(gs.best_score[&quot;mae&quot;]) print(gs.best_params[&quot;mae&quot;]) # Cleaned #1.2261258397066668 #{} #0.9248136286120494 #{} # Original #2.3148602570198133 #{} #1.646961825093937 #{} #runtime 22s . 2.3148602570198133 {} 1.646961825093937 {} . CoClustering . from surprise import CoClustering from surprise import Dataset from surprise.model_selection import GridSearchCV # Clustering sim_options = { &quot;n_cltr_u&quot;: [2,3,4], &quot;n_cltr_i&quot;: [2,3,4], &quot;n_epochs&quot;: [10,15,20], &quot;verbose&quot; : [True] } gs = GridSearchCV(CoClustering, sim_options, measures=[&quot;rmse&quot;, &quot;mae&quot;], cv=3) gs.fit(data) print(gs.best_score[&quot;rmse&quot;]) print(gs.best_params[&quot;rmse&quot;]) print(gs.best_score[&quot;mae&quot;]) print(gs.best_params[&quot;mae&quot;]) # Cleaned #1.2533606254732572 #{&#39;n_cltr_u&#39;: 4, &#39;n_cltr_i&#39;: 2, &#39;n_epochs&#39;: 10, &#39;verbose&#39;: True} #0.9453075653296411 #{&#39;n_cltr_u&#39;: 4, &#39;n_cltr_i&#39;: 2, &#39;n_epochs&#39;: 10, &#39;verbose&#39;: True} # Original #2.3921675151385458 #{&#39;n_cltr_u&#39;: 3, &#39;n_cltr_i&#39;: 2, &#39;n_epochs&#39;: 20, &#39;verbose&#39;: True} #1.687391909272779 #{&#39;n_cltr_u&#39;: 4, &#39;n_cltr_i&#39;: 2, &#39;n_epochs&#39;: 15, &#39;verbose&#39;: True} #runtime 10m 37s . Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 2.3921675151385458 {&#39;n_cltr_u&#39;: 3, &#39;n_cltr_i&#39;: 2, &#39;n_epochs&#39;: 20, &#39;verbose&#39;: True} 1.687391909272779 {&#39;n_cltr_u&#39;: 4, &#39;n_cltr_i&#39;: 2, &#39;n_epochs&#39;: 15, &#39;verbose&#39;: True} . KNNBaseline . # Clustering from surprise import KNNBaseline from surprise import Dataset from surprise.model_selection import GridSearchCV data = Dataset.load_from_df(ratings[[&quot;user_id&quot;, &quot;anime_id&quot;, &quot;rating&quot;]], reader) sim_options = { &quot;name&quot;: [&quot;msd&quot;, &quot;cosine&quot;], &quot;min_support&quot;: [3, 4, 5], &quot;user_based&quot;: [False, True], } param_grid = {&quot;sim_options&quot;: sim_options} gs = GridSearchCV(KNNBaseline, param_grid, measures=[&quot;rmse&quot;, &quot;mae&quot;], cv=3) gs.fit(data) print(gs.best_score[&quot;rmse&quot;]) print(gs.best_params[&quot;rmse&quot;]) print(gs.best_score[&quot;mae&quot;]) print(gs.best_params[&quot;mae&quot;]) # Cleaned #1.1832580437001543 #{&#39;sim_options&#39;: {&#39;name&#39;: &#39;msd&#39;, &#39;min_support&#39;: 5, &#39;user_based&#39;: False}} #0.8893399468276982 #{&#39;sim_options&#39;: {&#39;name&#39;: &#39;msd&#39;, &#39;min_support&#39;: 5, &#39;user_based&#39;: False}} # Original #2.2179062356553265 #{&#39;sim_options&#39;: {&#39;name&#39;: &#39;msd&#39;, &#39;min_support&#39;: 5, &#39;user_based&#39;: False}} #1.54544242417866 #{&#39;sim_options&#39;: {&#39;name&#39;: &#39;msd&#39;, &#39;min_support&#39;: 5, &#39;user_based&#39;: False}} #runtime 5m 15s . Estimating biases using als... Computing the msd similarity matrix... Done computing similarity matrix. Estimating biases using als... Computing the msd similarity matrix... Done computing similarity matrix. Estimating biases using als... Computing the msd similarity matrix... Done computing similarity matrix. Estimating biases using als... Computing the msd similarity matrix... Done computing similarity matrix. Estimating biases using als... Computing the msd similarity matrix... Done computing similarity matrix. Estimating biases using als... Computing the msd similarity matrix... Done computing similarity matrix. Estimating biases using als... Computing the msd similarity matrix... Done computing similarity matrix. Estimating biases using als... Computing the msd similarity matrix... Done computing similarity matrix. Estimating biases using als... Computing the msd similarity matrix... Done computing similarity matrix. Estimating biases using als... Computing the msd similarity matrix... Done computing similarity matrix. Estimating biases using als... Computing the msd similarity matrix... Done computing similarity matrix. Estimating biases using als... Computing the msd similarity matrix... Done computing similarity matrix. Estimating biases using als... Computing the msd similarity matrix... Done computing similarity matrix. Estimating biases using als... Computing the msd similarity matrix... Done computing similarity matrix. Estimating biases using als... Computing the msd similarity matrix... Done computing similarity matrix. Estimating biases using als... Computing the msd similarity matrix... Done computing similarity matrix. Estimating biases using als... Computing the msd similarity matrix... Done computing similarity matrix. Estimating biases using als... Computing the msd similarity matrix... Done computing similarity matrix. Estimating biases using als... Computing the cosine similarity matrix... Done computing similarity matrix. Estimating biases using als... Computing the cosine similarity matrix... Done computing similarity matrix. Estimating biases using als... Computing the cosine similarity matrix... Done computing similarity matrix. Estimating biases using als... Computing the cosine similarity matrix... Done computing similarity matrix. Estimating biases using als... Computing the cosine similarity matrix... Done computing similarity matrix. Estimating biases using als... Computing the cosine similarity matrix... Done computing similarity matrix. Estimating biases using als... Computing the cosine similarity matrix... Done computing similarity matrix. Estimating biases using als... Computing the cosine similarity matrix... Done computing similarity matrix. Estimating biases using als... Computing the cosine similarity matrix... Done computing similarity matrix. Estimating biases using als... Computing the cosine similarity matrix... Done computing similarity matrix. Estimating biases using als... Computing the cosine similarity matrix... Done computing similarity matrix. Estimating biases using als... Computing the cosine similarity matrix... Done computing similarity matrix. Estimating biases using als... Computing the cosine similarity matrix... Done computing similarity matrix. Estimating biases using als... Computing the cosine similarity matrix... Done computing similarity matrix. Estimating biases using als... Computing the cosine similarity matrix... Done computing similarity matrix. Estimating biases using als... Computing the cosine similarity matrix... Done computing similarity matrix. Estimating biases using als... Computing the cosine similarity matrix... Done computing similarity matrix. Estimating biases using als... Computing the cosine similarity matrix... Done computing similarity matrix. 2.2179062356553265 {&#39;sim_options&#39;: {&#39;name&#39;: &#39;msd&#39;, &#39;min_support&#39;: 5, &#39;user_based&#39;: False}} 1.54544242417866 {&#39;sim_options&#39;: {&#39;name&#39;: &#39;msd&#39;, &#39;min_support&#39;: 5, &#39;user_based&#39;: False}} . SVDpp . # Matrix facotrization from surprise import SVDpp from surprise import Dataset from surprise.model_selection import GridSearchCV data = Dataset.load_from_df(ratings[[&quot;user_id&quot;, &quot;anime_id&quot;, &quot;rating&quot;]], reader) param_grid = { &quot;n_epochs&quot;: [5,10], &quot;lr_all&quot;: [0.002, 0.005], &quot;reg_all&quot;: [0.4,0.6] } gs = GridSearchCV(SVDpp, param_grid, measures=[&quot;rmse&quot;, &quot;mae&quot;], cv=3) gs.fit(data) print(gs.best_score[&quot;rmse&quot;]) print(gs.best_params[&quot;rmse&quot;]) print(gs.best_score[&quot;mae&quot;]) print(gs.best_params[&quot;mae&quot;]) # Cleaned #1.236439713814388 #{&#39;n_epochs&#39;: 10, &#39;lr_all&#39;: 0.005, &#39;reg_all&#39;: 0.4} #0.9482547980069017 #{&#39;n_epochs&#39;: 10, &#39;lr_all&#39;: 0.005, &#39;reg_all&#39;: 0.4} # Original #2.30757862188976 #{&#39;n_epochs&#39;: 10, &#39;lr_all&#39;: 0.005, &#39;reg_all&#39;: 0.4} #1.6811210573467745 #{&#39;n_epochs&#39;: 10, &#39;lr_all&#39;: 0.005, &#39;reg_all&#39;: 0.4} #runtime 27m 43s . 2.30757862188976 {&#39;n_epochs&#39;: 10, &#39;lr_all&#39;: 0.005, &#39;reg_all&#39;: 0.4} 1.6811210573467745 {&#39;n_epochs&#39;: 10, &#39;lr_all&#39;: 0.005, &#39;reg_all&#39;: 0.4} .",
            "url": "https://ylu-1258.github.io/YLu-Blog/jupyter/interests/2022/08/29/Summer-Research-ML-results.html",
            "relUrl": "/jupyter/interests/2022/08/29/Summer-Research-ML-results.html",
            "date": " • Aug 29, 2022"
        }
        
    
  
    
        ,"post61": {
            "title": "Data Abstraction in Python",
            "content": "Lists and Dictionaries . Data Abstraction . To properly manage our data while coding, we might require different types of data structures to successfully attain the goal of our program. Properly storing our data makes it easier for us to share information with fellow devs, across the internet, and also working in other technlogical fields. . The following are some comm ways to stored structured data in programing . Variables . Variables are contianers of data. They are the most primative and basic structure of data containing only a reference and a value, the value of the variable can be dynamic, and the value can be referenced to by the name of the variable. There are many primative data types in the python programing language, but a unique characteristic of python is that variables are given a data type at assignment. . Some primative data types are integers, floats, strings (array of chars), chars, etc. | msg = &quot;Hello World&quot; print(msg) . Hello World . Python Lists . Lists are sequential containers of data that could store multiple values per key. Lists in python are denoted by brackets. We can check that our list is of the list data type with the type() function . Always be precise and detailed when noting down variable names. . Each list has a length attribute that denotes how big or long the list is. in python, this is achieved via the len() function . langs = [&quot;cpp&quot;, &quot;python&quot;, &quot;html&quot;, &quot;css&quot;, &quot;java&quot;, &quot;javascript&quot;] print(langs) # Getting the data type print(type(langs)) # Printing the length print(len(langs)) # Printing the 3rd element print(langs[2]) . [&#39;cpp&#39;, &#39;python&#39;, &#39;html&#39;, &#39;css&#39;, &#39;java&#39;, &#39;javascript&#39;] &lt;class &#39;list&#39;&gt; 6 html . Dictionaries . Dictionaries are sequential containers of data that are similar to lists but contain pairs of keys and values. Dictionaries in python are denoted by curly braces. . We can access the keys of the dictionary with the .keys() method, and we can access the values of the dictionary with the .values() method. Additionally, we can also use the .get() method to get a value from a key. . Dictionaries are very similar to a JSON in how it stores its data. . info = { &quot;name&quot;: &quot;Alex Lu&quot;, &quot;age&quot;: 15, &quot;score&quot;: 3, &quot;langs&quot;: &quot;cpp&quot; } print(&quot;info: &quot;, info, &quot;type: &quot; + str(type(info)), &quot;Length: &quot; + str(len(info))) for key in info.keys(): print(key) print(&quot;&quot;) for val in info.values(): print(val) print(&quot; Now using .get()&quot;) for key in info.keys(): print(key + &quot;:&quot;, info.get(key)) . info: {&#39;name&#39;: &#39;Alex Lu&#39;, &#39;age&#39;: 15, &#39;score&#39;: 3, &#39;langs&#39;: &#39;cpp&#39;} type: &lt;class &#39;dict&#39;&gt; Length: 4 name age score langs Alex Lu 15 3 cpp Now using .get() name: Alex Lu age: 15 score: 3 langs: cpp . More Data Abstraction in Python . To add to a list, we can use the .append() method to add in more inputs to the list. I have modified the InfoDb List from class and added my own entries and values into the dictionary. . InfoDb = [] # InfoDB is a data structure with expected Keys and Values # Append to List a Dictionary of key/values related to a person InfoDb.append({ &quot;FirstName&quot;: &quot;John&quot;, &quot;LastName&quot;: &quot;Mortensen&quot;, &quot;Gender&quot;: &quot;Male&quot;, &quot;DOB&quot;: &quot;October 21&quot;, &quot;Residence&quot;: &quot;San Diego&quot;, &quot;Email&quot;: &quot;jmortensen@powayusd.com&quot;, &quot;Fav_Color&quot;: &quot;Unknown&quot;, &quot;Sleep_Schedule&quot; : &quot;Unknown&quot;, &quot;is_Teacher&quot;: True, &quot;Hobbies&quot;: [&quot;Teaching&quot;, &quot;Programming&quot;], &quot;Games&quot;: [] }) InfoDb.append({ &quot;FirstName&quot;: &quot;Alex&quot;, &quot;LastName&quot;: &quot;Lu&quot;, &quot;Gender&quot;: &quot;Male&quot;, &quot;DOB&quot;: &quot;November 29, 2006&quot;, &quot;Residence&quot;: &quot;San Diego&quot;, &quot;Email&quot;: &quot;maodou1258@gmail.com&quot;, &quot;Fav_Color&quot;: &quot;White&quot;, &quot;Sleep_Schedule&quot; : &quot;1am&quot;, &quot;is_Teacher&quot;: False, &quot;Hobbies&quot;: [&quot;Programming&quot;, &quot;Tennis&quot;, &quot;Reading&quot;, &quot;Sleeping&quot;], &quot;Games&quot;: [&quot;League of Legends&quot;, &quot;VALORANT&quot;, &quot;Minecraft&quot;, &quot;OSU&quot;] }) InfoDb.append({ &quot;FirstName&quot;: &quot;Evan&quot;, &quot;LastName&quot;: &quot;Aparri&quot;, &quot;Gender&quot;: &quot;Male&quot;, &quot;DOB&quot;: &quot;November 10, 2005&quot;, &quot;Residence&quot;: &quot;San Diego&quot;, &quot;Email&quot;: &quot;evanaparri@gmail.com&quot;, &quot;Fav_Color&quot;: &quot;Blue&quot;, &quot;Sleep_Schedule&quot; : &quot;2am&quot;, &quot;is_Teacher&quot;: False, &quot;Hobbies&quot;: [&quot;Running&quot;, &quot;Reading&quot;, &quot;Eating Asian Food/ Food Connoisseur&quot;, &quot;Sleeping&quot;, &quot;Homework&quot;, &quot;Programming&quot;], &quot;Games&quot;: [] }) # Print the data structure print(InfoDb) . [{&#39;FirstName&#39;: &#39;John&#39;, &#39;LastName&#39;: &#39;Mortensen&#39;, &#39;Gender&#39;: &#39;Male&#39;, &#39;DOB&#39;: &#39;October 21&#39;, &#39;Residence&#39;: &#39;San Diego&#39;, &#39;Email&#39;: &#39;jmortensen@powayusd.com&#39;, &#39;Fav_Color&#39;: &#39;Unknown&#39;, &#39;Hobbies&#39;: [&#39;Teaching&#39;, &#39;Programming&#39;], &#39;Sleep_Schedule&#39;: &#39;Unknown&#39;, &#39;Games&#39;: [], &#39;is_Teacher&#39;: True}, {&#39;FirstName&#39;: &#39;Alex&#39;, &#39;LastName&#39;: &#39;Lu&#39;, &#39;Gender&#39;: &#39;Male&#39;, &#39;DOB&#39;: &#39;November 29, 2006&#39;, &#39;Residence&#39;: &#39;San Diego&#39;, &#39;Email&#39;: &#39;maodou1258@gmail.com&#39;, &#39;Fav_Color&#39;: &#39;White&#39;, &#39;Hobbies&#39;: [&#39;Programming&#39;, &#39;Tennis&#39;, &#39;Reading&#39;, &#39;Sleeping&#39;], &#39;Games&#39;: [&#39;League of Legends&#39;, &#39;VALORANT&#39;, &#39;Minecraft&#39;, &#39;OSU&#39;], &#39;Sleep_Schedule&#39;: &#39;1am&#39;, &#39;is_Teacher&#39;: False}, {&#39;FirstName&#39;: &#39;Evan&#39;, &#39;LastName&#39;: &#39;Aparri&#39;, &#39;Gender&#39;: &#39;Male&#39;, &#39;DOB&#39;: &#39;November 10, 2005&#39;, &#39;Residence&#39;: &#39;San Diego&#39;, &#39;Email&#39;: &#39;evanaparri@gmail.com&#39;, &#39;Fav_Color&#39;: &#39;Blue&#39;, &#39;Hobbies&#39;: [&#39;Running&#39;, &#39;Reading&#39;, &#39;Eating Asian Food/ Food Connoisseur&#39;, &#39;Sleeping&#39;, &#39;Homework&#39;, &#39;Programming&#39;], &#39;Games&#39;: [], &#39;Sleep_Schedule&#39;: &#39;2am&#39;, &#39;is_Teacher&#39;: False}] . If we want to add values to InfoDb, this could easily be achived with the .append() method . def add_entry(): name = input(&quot;First and last name seperated by spaces: &quot;).split() gender = input(&quot;What is your gender? &quot;) birthdate = input(&quot;When were you born? &quot;) # Not gonna care about date time formatting cuz all programmers hate that residence = input(&quot;What city do you live in? &quot;) email = input(&quot;What is your email? &quot;) color = input(&quot;What is your favorite color? &quot;) sleep = input(&quot;When do you sleep? &quot;) is_teacher = bool_input(&quot;Are you a teacher? [yes/no] &quot;) hobbies = [] print(&quot;Now asking for Hobbies &quot;) hobbies = multi_input(hobbies) games = [] print(&quot;Now asking for Games &quot;) games = multi_input(games) entry = { &quot;FirstName&quot;: name[0], &quot;LastName&quot;: name[1], &quot;Gender&quot;: gender, &quot;DOB&quot;: birthdate, &quot;Residence&quot;: residence, &quot;Email&quot;: email, &quot;Fav_Color&quot;: color, &quot;Sleep_Schedule&quot;: sleep, &quot;is_Teacher&quot;: is_teacher, &quot;Hobbies&quot;: hobbies, &quot;Games&quot; : games } return entry def bool_input(prompt): while True: try: return {&quot;yes&quot;:True,&quot;no&quot;:False}[input(prompt).lower()] except KeyError: print(&quot;Invalid input please enter yes or no&quot;) def multi_input(arr): i = 0 temp = &quot;&quot; while temp != &quot;cancel&quot;: i+=1 temp = input(&quot;Please enter item number {0}, type cancel to proceede &quot;.format(str(i))) if temp.lower != &quot;cancel&quot;: arr.append(temp) return arr new_entry = add_entry() InfoDb.append(new_entry) print(InfoDb[-1]) . Now asking for Hobbies Now asking for Games {&#39;FirstName&#39;: &#39;Yixuan&#39;, &#39;LastName&#39;: &#39;Lu&#39;, &#39;Gender&#39;: &#39;Male&#39;, &#39;DOB&#39;: &#39;Nov 29&#39;, &#39;Residence&#39;: &#39;Shanghai&#39;, &#39;Email&#39;: &#39;maodou12581@gmail.com&#39;, &#39;Fav_Color&#39;: &#39;Black&#39;, &#39;Sleep_Schedule&#39;: &#39;3am&#39;, &#39;is_Teacher&#39;: False, &#39;Hobbies&#39;: [&#39;Programming&#39;, &#39;Soccer&#39;, &#39;cancel&#39;], &#39;Games&#39;: [&#39;Overwatch&#39;, &#39;Fortnite&#39;, &#39;Geometry Dash&#39;, &#39;cancel&#39;]} . Formatting and printing the data in the list . def print_data(d_rec): print(d_rec[&quot;FirstName&quot;], d_rec[&quot;LastName&quot;]) # using comma puts space between values print(&quot; t&quot;, &quot;Gender:&quot;, d_rec[&quot;Gender&quot;]) print(&quot; t&quot;, &quot;Residence:&quot;, d_rec[&quot;Residence&quot;]) # t is a tab indent print(&quot; t&quot;, &quot;Birth Day:&quot;, d_rec[&quot;DOB&quot;]) print(&quot; t&quot;, &quot;Email:&quot;, d_rec[&quot;Email&quot;]) print(&quot; t&quot;, &quot;Favorite Color:&quot;, d_rec[&quot;Fav_Color&quot;]) print(&quot; t&quot;, &quot;Is Teacher?:&quot;, d_rec[&quot;is_Teacher&quot;]) print(&quot; t&quot;, &quot;Hours of Sleep:&quot;, d_rec[&quot;Sleep_Schedule&quot;]) print(&quot; t&quot;, &quot;Hobbies: &quot; + &quot;, &quot;.join(d_rec[&quot;Hobbies&quot;])) print(&quot; t&quot;, &quot;Games: &quot; + &quot;, &quot;.join(d_rec[&quot;Games&quot;])) print() # for loop algorithm iterates on length of InfoDb def for_loop(): print(&quot;For loop output n&quot;) for record in InfoDb: print_data(record) for_loop() . For loop output John Mortensen Gender: Male Residence: San Diego Birth Day: October 21 Email: jmortensen@powayusd.com Favorite Color: Unknown Is Teacher?: True Hours of Sleep: Unknown Hobbies: Teaching, Programming Games: Alex Lu Gender: Male Residence: San Diego Birth Day: November 29, 2006 Email: maodou1258@gmail.com Favorite Color: White Is Teacher?: False Hours of Sleep: 1am Hobbies: Programming, Tennis, Reading, Sleeping Games: League of Legends, VALORANT, Minecraft, OSU Evan Aparri Gender: Male Residence: San Diego Birth Day: November 10, 2005 Email: evanaparri@gmail.com Favorite Color: Blue Is Teacher?: False Hours of Sleep: 2am Hobbies: Running, Reading, Eating Asian Food/ Food Connoisseur, Sleeping, Homework, Programming Games: Yixuan Lu Gender: Male Residence: Shanghai Birth Day: Nov 29 Email: maodou12581@gmail.com Favorite Color: Black Is Teacher?: False Hours of Sleep: 3am Hobbies: Programming, Soccer, cancel Games: Overwatch, Fortnite, Geometry Dash, cancel . Now we print the entires of InfoDb in reverse . The first method uses array slicing, while the second method iterates in reverse order starting from the last index of the array, this is accomplished through the reversed() function paired with the range() and len() functions. . def for_loop_reversed_1(arr): print(&quot;Cheesed Reverse For loop output:&quot;) for i in arr[::-1]: print_data(i) def for_loop_reversed_2(arr): for i in reversed(range(0,len(arr))): print_data(arr[i]) # for_loop_reversed_1(InfoDb) does the same thing for_loop_reversed_2(InfoDb) . Yixuan Lu Gender: Male Residence: Shanghai Birth Day: Nov 29 Email: maodou12581@gmail.com Favorite Color: Black Is Teacher?: False Hours of Sleep: 3am Hobbies: Programming, Soccer, cancel Games: Overwatch, Fortnite, Geometry Dash, cancel Evan Aparri Gender: Male Residence: San Diego Birth Day: November 10, 2005 Email: evanaparri@gmail.com Favorite Color: Blue Is Teacher?: False Hours of Sleep: 2am Hobbies: Running, Reading, Eating Asian Food/ Food Connoisseur, Sleeping, Homework, Programming Games: Alex Lu Gender: Male Residence: San Diego Birth Day: November 29, 2006 Email: maodou1258@gmail.com Favorite Color: White Is Teacher?: False Hours of Sleep: 1am Hobbies: Programming, Tennis, Reading, Sleeping Games: League of Legends, VALORANT, Minecraft, OSU John Mortensen Gender: Male Residence: San Diego Birth Day: October 21 Email: jmortensen@powayusd.com Favorite Color: Unknown Is Teacher?: True Hours of Sleep: Unknown Hobbies: Teaching, Programming Games: . Four methods of looping . In the methods below, I will use for-loops, for-loops with indices, while loops, and recursion to print all the data within InfoDB . For Loop . def for_loop(arr): print(&quot;Output of Database&quot;) # The for loop declares a local variable &quot;i&quot; and assigns each DB record to i over each iteration for i in arr: # Using the predermined print_data function to neatly format our data in a human friendly manner print_data(i) print(type(i)) # Note that &quot;i&quot; will retain its value outside of the for-loop, but not outside of the function for_loop(InfoDb) . Output of Database John Mortensen Gender: Male Residence: San Diego Birth Day: October 21 Email: jmortensen@powayusd.com Favorite Color: Unknown Is Teacher?: True Hours of Sleep: Unknown Hobbies: Teaching, Programming Games: Alex Lu Gender: Male Residence: San Diego Birth Day: November 29, 2006 Email: maodou1258@gmail.com Favorite Color: White Is Teacher?: False Hours of Sleep: 1am Hobbies: Programming, Tennis, Reading, Sleeping Games: League of Legends, VALORANT, Minecraft, OSU Evan Aparri Gender: Male Residence: San Diego Birth Day: November 10, 2005 Email: evanaparri@gmail.com Favorite Color: Blue Is Teacher?: False Hours of Sleep: 2am Hobbies: Running, Reading, Eating Asian Food/ Food Connoisseur, Sleeping, Homework, Programming Games: Yixuan Lu Gender: Male Residence: Shanghai Birth Day: Nov 29 Email: maodou12581@gmail.com Favorite Color: Black Is Teacher?: False Hours of Sleep: 3am Hobbies: Programming, Soccer, cancel Games: Overwatch, Fortnite, Geometry Dash, cancel &lt;class &#39;dict&#39;&gt; . For loop with index . def for_loop_with_index(arr): print(&quot;Output of Database&quot;) # Using the range function to print out data based on the index in the array for i in range(0,len(arr)): # Using index notation to print data print_data(arr[i]) for_loop_with_index(InfoDb) . Output of Database John Mortensen Gender: Male Residence: San Diego Birth Day: October 21 Email: jmortensen@powayusd.com Favorite Color: Unknown Is Teacher?: True Hours of Sleep: Unknown Hobbies: Teaching, Programming Games: Alex Lu Gender: Male Residence: San Diego Birth Day: November 29, 2006 Email: maodou1258@gmail.com Favorite Color: White Is Teacher?: False Hours of Sleep: 1am Hobbies: Programming, Tennis, Reading, Sleeping Games: League of Legends, VALORANT, Minecraft, OSU Evan Aparri Gender: Male Residence: San Diego Birth Day: November 10, 2005 Email: evanaparri@gmail.com Favorite Color: Blue Is Teacher?: False Hours of Sleep: 2am Hobbies: Running, Reading, Eating Asian Food/ Food Connoisseur, Sleeping, Homework, Programming Games: Yixuan Lu Gender: Male Residence: Shanghai Birth Day: Nov 29 Email: maodou12581@gmail.com Favorite Color: Black Is Teacher?: False Hours of Sleep: 3am Hobbies: Programming, Soccer, cancel Games: Overwatch, Fortnite, Geometry Dash, cancel . While Loop . def while_loop(arr): print(&quot;Output of Database&quot;) # Initializing a counter variable to keep track of the current index of InfoDb i = 0 # While loop to run certain processes before i becomes equal to the max index + 1, terminate once i is equal to len(arr) while i &lt; len(arr): print_data(arr[i]) i+=1 # I FORGOT TO DO THIS OH GOD WHY MY VSCODE IS BROKEN while_loop(InfoDb) . Output of Database John Mortensen Gender: Male Residence: San Diego Birth Day: October 21 Email: jmortensen@powayusd.com Favorite Color: Unknown Is Teacher?: True Hours of Sleep: Unknown Hobbies: Teaching, Programming Games: Alex Lu Gender: Male Residence: San Diego Birth Day: November 29, 2006 Email: maodou1258@gmail.com Favorite Color: White Is Teacher?: False Hours of Sleep: 1am Hobbies: Programming, Tennis, Reading, Sleeping Games: League of Legends, VALORANT, Minecraft, OSU Evan Aparri Gender: Male Residence: San Diego Birth Day: November 10, 2005 Email: evanaparri@gmail.com Favorite Color: Blue Is Teacher?: False Hours of Sleep: 2am Hobbies: Running, Reading, Eating Asian Food/ Food Connoisseur, Sleeping, Homework, Programming Games: Yixuan Lu Gender: Male Residence: Shanghai Birth Day: Nov 29 Email: maodou12581@gmail.com Favorite Color: Black Is Teacher?: False Hours of Sleep: 3am Hobbies: Programming, Soccer, cancel Games: Overwatch, Fortnite, Geometry Dash, cancel . Recursion . def recursion(arr, counter): # Check if the current index is within bounds if counter &lt; len(arr): # prints data print_data(arr[counter]) # Recalling the same function for counter+1 index, printing the next item in the list recursion(arr, counter+1) # If previous if statement does not pass, return 0 and terminate the recursion return 0 recursion(InfoDb, 0) . John Mortensen Gender: Male Residence: San Diego Birth Day: October 21 Email: jmortensen@powayusd.com Favorite Color: Unknown Is Teacher?: True Hours of Sleep: Unknown Hobbies: Teaching, Programming Games: Alex Lu Gender: Male Residence: San Diego Birth Day: November 29, 2006 Email: maodou1258@gmail.com Favorite Color: White Is Teacher?: False Hours of Sleep: 1am Hobbies: Programming, Tennis, Reading, Sleeping Games: League of Legends, VALORANT, Minecraft, OSU Evan Aparri Gender: Male Residence: San Diego Birth Day: November 10, 2005 Email: evanaparri@gmail.com Favorite Color: Blue Is Teacher?: False Hours of Sleep: 2am Hobbies: Running, Reading, Eating Asian Food/ Food Connoisseur, Sleeping, Homework, Programming Games: Yixuan Lu Gender: Male Residence: Shanghai Birth Day: Nov 29 Email: maodou12581@gmail.com Favorite Color: Black Is Teacher?: False Hours of Sleep: 3am Hobbies: Programming, Soccer, cancel Games: Overwatch, Fortnite, Geometry Dash, cancel . 0 . Other cool things to do with lists and Dictionaries . Some cool list operations including for loops and recursion in cluding reversing, list comprehension, and recursion . print(&quot;List comprehension, create a list on the spot&quot;) my_arr1 = [2&lt;&lt;i for i in range(0,10)] print(&quot;List of squares of 2: &quot;, my_arr1) print(&quot; nReversing a list&quot;) my_arr2 = [1,2,3,4,5] print(&quot;Original List: &quot;, my_arr2) print(&quot;Reversed List: &quot;, my_arr2[::-1]) print(&quot; nSorting a list&quot;) my_arr3 = [67,84,19,43,175,78] print(&quot;Sorted ascending: &quot;, sorted(my_arr3)) print(&quot;Sorted descending: &quot;, sorted(my_arr3, reverse=True)) print(&quot; nEXTRA: Qsort using list comprehension&quot;) def Qsort(arr): if arr == []: return [] pivot = arr[0] l = Qsort([i for i in arr[1:] if i &lt; pivot]) r = Qsort([i for i in arr[1:] if i &gt;= pivot]) return l + [pivot] + r print(Qsort(my_arr3)) . List comprehension, create a list on the spot List of squares of 2: [2, 4, 8, 16, 32, 64, 128, 256, 512, 1024] Reversing a list Original List: [1, 2, 3, 4, 5] Reversed List: [5, 4, 3, 2, 1] Sorting a list Sorted ascending: [19, 43, 67, 78, 84, 175] Sorted descending: [175, 84, 78, 67, 43, 19] EXTRA: Qsort using list comprehension [19, 43, 67, 78, 84, 175] . Creating a Quiz with Dictionaries . q_a = { &quot;What is the derivative of position?&quot; : [&quot;velocity&quot;], &quot;What is the integral of acceleration?&quot; : [&quot;velocity&quot;], &quot;Name a product of photosynthesis&quot; : [&quot;glucose&quot;, &quot;oxygen&quot;], &quot;What rhetorical device includes the repitition of a word or phrase at the end of clauses?&quot; : [&quot;epistrophe&quot;] } # Returns a tuple of lists of questions and values def get_pair(dict): return [q for q, a in dict.items()], [a for q, a in dict.items()] # Function to calculate percentage def percentage(x, y): return &#39;{0:.2f}&#39;.format(100 * float(x)/float(y)) # Some States correct = 0 total = len(q_a) # Get the answer + question pairs q_a_pairs = get_pair(q_a) # Ask and evaluate questions for i in range(0,len(q_a_pairs[0])): rsp = input(&quot;QUESTION: &quot; + q_a_pairs[0][i]) # Use .lower to maintain case insensitivity if rsp.lower() in q_a_pairs[1][i]: print(rsp + &quot; was correct!&quot;) correct+=1 else: print(&quot;Wrong, {0} was incorrect, the correct answer was: {1}&quot;.format(rsp, &quot;, &quot;.join(q_a_pairs[1][i]))) print(&quot;Congrats, you got {0}% on this quiz&quot;.format(percentage(correct,total))) . velocity was correct! Wrong, position was incorrect, the correct answer was: velocity oxygen was correct! EpiStroPhe was correct! Congrats, you got 75.00% on this quiz . It&#39;s just too free 😎 .",
            "url": "https://ylu-1258.github.io/YLu-Blog/jupyter/apcsp/week%202/2022/08/29/Lecture-Data-Abstraction.html",
            "relUrl": "/jupyter/apcsp/week%202/2022/08/29/Lecture-Data-Abstraction.html",
            "date": " • Aug 29, 2022"
        }
        
    
  
    
        ,"post62": {
            "title": "2022 08 29 Alumni Panel",
            "content": "Alumni Panel . Questions and answers from CS alumni panel. . Questions . Q: How did you realize whant you wanted to do as your major? . A: . Colein: Realized that one can code pretty well, other friends and family members suggested to learn compsci. Comp-sci is a competitive field that’s about perspective. | Anthony: Did programing from a young age, found it interesting and kept doing it. | Andrew: Took Intro to CS and really liked it and liked the techy and teamwork aspect of cs. | Maybel: Didn’t really like the cs class or subject, new that it was important to learn. Interned with northrop with their respective tasks. You are still going have to learn scripting and programming even if transitiong into another field. | Allison: Cognitive sciences really helped with the UI/UX field of CS, especially in the front-end dev part. | Nitya: Made own games and programs in free time and during summer. Took CSP and saw how she could apply cs to real world projects. | . Q: What were some challenges in comp sci . A: . Colein: Starting out sucks a lot, error messages and stuff made things annoying and hard to approach. Stuff on the internet will help you | Anthony: Clashing Egos, working together is hard because CS majors have a big ego, very frustrating. Follow your own curve and focus on your path. | Andrew: The theory of CS is hard and the curriculumn in college is not structured and introduced new topics that people didn’t really focus on. Different from high school computer science courses. | Maybel: Hard to find the motivation to CS, there is always someone better than you at CS. The robotics team had a lot of people that were good at CS, never had the motivation to work or study cs. Got really frustrated with the syntax of the problem. Realized that google was the best friend for syntax. | Allison: Hard time starting out with CS. The temptation to BS one’s work. Temptation to cut corners, must have a mindset to go above and beyond in order to truly succeed at CS. | Nitya: Getting started in a new area of computer science is ver yahrd and it’s daunting to try and get used to how you get started on coding. Got confused on where to start coding. | . Q: Are there any PBL scenarios in College . A: . Colein: Not that many project based learning scenarios in college, probably only 1. However, even if schools are theory based, projects are bound to show up in college. | Anthony: CS courses are bound to have projects. | Andrew: PBL is argely embedded with computer science, there are a lot of tests based on theory though. | Maybel: College in the first few years is just common education, the teacher does not have the time to answer all questions and not everyone can get 1 on 1 sessions. People struggle with this so hey used PBL to create study groups with one another. Some project classes are based on a lot of proejcts such as machine learning or artificial intellegence classes. Some people are going to freeload, but that’s okay, it’s an opportunity to learn. Always ask questions whie interning. | Allison: - | Nitya: - | . Q: How have you applied CS in your major if you are not majoring in CS? . Colein: A lot of menial tasks such as spreadsheet organizing and copy pasting could be sped up by a script. Coding is everywhere, mechanical students will use MATLAB and a lot of the computer science topics to help them with the work | Anthony: - | Andrew: Computer science can help in other fields that require mathematics, it can run many operations really fast. | Maybel: A lot of softwares may require certain stress tests. 3D printing could be simplified with CS and other algorithms.Makes stuff a lot faster in order to reduce manual labor. CS extends far beyond the CS major. A lot of people use jupyter notebooks for techincal papers for interactivity and such. Really nice for a research paper. | Allison: Psychology + computer science for Cog-sci. A lot of front-end programming with UI/UX. A lot of courses for stats also require a lot of calculations, programming certain programs could help with many tasks. It is inevitable. Psychology classes also have CS. | Nitya: - | . Q: For existing college students, have you had a job at your school? . Colein: Really important to create a resume for job applications. Push yourself to be above and beyond in order to be different from the other students. Visit office hours regularly. Reference yourself to the professor to think about how you are different rom the others. Becoming a TA is alsovery valuable and important. Sometimes you just have to figure it out! Visualized fight data, databases, web development, python automation, cyber security, etc. | Anthony: Code ninjas, franchise. | Andrew: Tutor at the college. Worked with a lot of tools such as APIs, web dev, javascript, html, and python. | Maybel: Robotics prfessor is a part time professor at UCSD that trains autonomous vehicals. Some professors have clubs and some have research clubs. As the professor if there’s any slots open in their labs and just pass them your resume. Interned at Northrop with Colein. Creating websites for flight data. | Allison: Had an internship with a church group, frontend UI dev. | Nitya: Northrop Grumman high school internship program. https://ngc.avature.net/events/Register?folderld=%2012369 &lt;—— note to self | . Q: Did you ever focus on personal projects along with school . Colein: In mechanical engineering, used computer science to mirror components to create mechanical parts. A lot of manipulation for commands. Simulations are useful, Computer Science can help with that. Set up servers to run the processes. | Anthony: Made a project for basketball teams. | Andrew: Companies will look for personal projects to see that you actually know what you are doing. Just 3 or 4 could go the extra mile of making the job. Good way to show off knowledge about algorithms and data structures. Pathfinding algorithms, graph theory, and also sorting algorithms. Data visualization is a really big field. | Maybel: Triton AI, improve autonomous tracking abilities. | Allison: - | Nitya: - | . Q: Clubs at Del Norte . Colein: CS in people managing, organizastion skills are very important | Anthony: | Andrew: | Maybel: School and robotics. | Allison: | Nitya: | . Q: Tips for internships and jobs . Colein: | Anthony: Do not look at other people’s linkedin, huge demoralizer. | Andrew: Build a good network of people in order to get refferals from others. Really important if you want to get into a FAANG company. Make a lot of connectons on LinkedIn. | Maybel: Have a resume, get a list of your achievements. Know when to find your internships, starts hiring from Sept-November, when most of the interns there are finishing their internships. Get a resume done by early september for summer internships. Have proof of work that you have done. Call a bunch of engineers to decide what to do. Personal projects and teams are important, differentiates you from other students. MORE THAN JUST GPA AND MAJOR. SKILLS &gt; NUMBERS. Ask local and smaller companies about internships, ask about high school internships and such. Find stuff you are interested in. Referrels are really important in the filtering process. | Allison: | Nitya: | .",
            "url": "https://ylu-1258.github.io/YLu-Blog/markdown/apcsp/week%202/2022/08/29/Alumni-panel.html",
            "relUrl": "/markdown/apcsp/week%202/2022/08/29/Alumni-panel.html",
            "date": " • Aug 29, 2022"
        }
        
    
  
    
        ,"post63": {
            "title": "An Empirical Study On Performance Measures Of Collaborative Filtering Recommendation Algorithms",
            "content": "An Empirical Study on Performance Measures of Collaborative Filtering Recommendation Algorithms . Alex Lu . maodou1258@gmail.com . Abstract . In recent times, recommendation engines have become increasingly popular within many industries. The focus of such an engine is to implement an algorithm to successfully make recommendations based on user preferences. Because of the utility provided by such tools, many industries heavily rely on such engines to provide accurate product recommendations to customers. Some popular algorithms implemented include clustering, matrix factorization, and deep learning implementations. Such algorithms are utilized within Collaborative Filtering methods and are simulated using pre-made datasets containing user and item information. Performance metrics are then applied to the algorithm to test the accuracy of the model. Results show that deep learning algorithms provide greater . performance when compared to other applications. . Keywords - Collaborative filtering, RMSE, Deep Learning, MAE, matrix factorization, Performance Measure . 1. Introduction . Latterly, recommendation engines serve a great purpose in many online services and enhance the consumer experience by providing lists of recommended items to users. Many big corporations such as Netflix, Amazon, Youtube, and others provide item recommendations through such methods to promote the sales and usage of their goods. The general idea of a recommender system is to return a list of items that the user would find interesting. Implementing various different types of engines provides insight towards which algorithm is the most suitable for certain scenarios. . Different approaches by different algorithms could have varying performances depending on the type of recommendation required, and the core nature of the dataset used. This research is primarily conducted on collaborative-filtering techniques, but similar research could be conducted on content-based implementations. Examples of potential implementations could be clustering and matrix factorization approaches. The main accuracy metrics utilized in the research are the RMSE and MAE metrics which are further explained later in the paper. . Two main research questions would serve as the focus of the research. More conclusions could be drawn from the data collected, but the main aim of the research are as follows: . I) What CF implementation provides the accuracy measure for a standard data set containing user, item and rating information? II) How does data sparsity affect the prediction accuracy of the models implemented? . 2. Background . Recommendation engines are programs used to provide item recommendations to users based on filtering each item and returning a . possible predicted rating for the user. The two main implementations of recommendation engines are content-based filtering and collaborative filtering. . 2.1 Content-based Filtering . A content-based filtering engine takes into account the user’s own item history and focuses on keywords in items rather than similarity between users [1]. However, such implementation can lead to a scenario where providing a broad range of recommendations would become impossible. Because of the nature of content-based filtering engines, items that were utilized by similar users wouldn’t be recommended to the main user purely because of the lack of a keyword or phrase. A dataset for such an engine would incorporate items along with a detailed profile for each item. . Despite its narrower scope when providing recommendations, content-based filtering systems often reduce the amount of data needed to make accurate predictions. For collaborative filtering, a greater range of data is required for a proper calculation of an item. To put it simply, the more data the engine has, the more . accurate the prediction is. However, with the nature of content-based filtering, any amount of data would suffice, providing that there are items that match the recommendation requirements. . 2.2 Collaborative Filtering . A collaborative filtering engine takes into account other user’s ratings, and returns . algorithms. This paper would primarily focus on model-based algorithms and approaches. The similarity between users could be found in various methods. For this study, a total of twelve algorithms were implemented. The main focus was placed on nearest neighbors, matrix factorization, and deep learning algorithms. Figure 2.1 maps out the various . . recommendations based on similarity. Collaborative filtering is split into two main approaches, model-based and memory based . algorithms implemented in this research. 2.3 Nearest Neighbor . Nearest neighbor, or more specifically, k nearest neighbors is a collaborative filtering . algorithm used to find similarities between items and users based on the total distance between two neighbors [2]. A weight system could also be applied so that a neighbor closer to the main user would have more weight on the recommendations than one who is further away. . Four of the twelve algorithms used in the research were from this category. KNNBasic, KNNWithZScore, KNNWithMeans, and KNNBaseline were such implementations of this category of algorithms. . 2.4 Matrix Factorization . Matrix Factorization is yet again another implementation of collaborative filtering. Simply put, a matrix factorization relates two separate values together under a specific value to create a grid or matrix of the data [3]. However, as data is not evenly distributed, some cells in the matrix are left empty and would need to be filled in to provide recommendations. Such values are known as “latent features”. . Some matrix factorizations implemented in the research were Negative Matrix Factorization, Singular Value Decomposition, and SVD++. . 2.5 Deep Learning . A deep learning model utilizes a neural network to process and calculate information. Much like a human brain, deep learning attempts to make predictions based on data much like a human brain. . The deep learning model implemented in this experiment utilizes an embedding layer structure [4]. Using such a structure could organize data into a vector of discrete values which could then produce similarity results and tests through the distance between vectors. These types of embedding layers could be generated through frameworks such as “Pytorch” or “Tensorflow”, however, this research would be implemented in Pytorch. The model could then be trained through a series of epochs and eventually provide predictions. . 2.6 Miscellaneous surprise algorithms . The miscellaneous category comprises the leftover algorithms in the surprise package that do not fit into any of the other categories in the research. Such algorithms were as listed: Co . Clustering, Normal Predictor, Baseline-Only, and Slope-One. . According to Sahar Karat Co-Clustering is a collaborative filtering algorithm used to provide recommendations by “A simultaneous clustering of the rows and columns of a matrix” [5]. Classical clustering algorithms only focus on one specific type of data, while co . clustering could be used to accommodate two simultaneously. . Normal predictor is an algorithm provided by the surprise package that provides random user recommendations based on the data distribution in the dataset. The “Maximum Likelihood Estimation” method is utilized in this calculation. . Baseline-Only is yet another surprise algorithm that makes baseline predictions based on the data provided. There are two main ways to implement this algorithm. The first implementation method is “Stochastic Gradient . Descent” (SGD), which calculates a gradient of the dataset by a random selection of data. The other implementation is to use “Alternating Least Squares” (ALS) which is a matrix factorization algorithm that works well with sparser sets of data. . Slope-One is an example of an item-based collaborative filtering recommendation algorithm. Predictions made with this model are generally based on personal ratings as well as similar community ratings. . Further information and implementations on the methods listed could be found on the surprise documentations [6]. . 2.7 Accuracy Metrics . Two accuracy metrics were used in the process of this research. Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE) [11]. . RMSE is a quadratic model that assigns much larger weights to larger errors observed. Accuracy is calculated through RMSE by taking the square root of the mean of the sum of the squared difference of each predicted . value and its corresponding actual value. The formula is displayed in Figure 2.2 . . (Figure 2.2) . MAE is a linear metric model that linearly scales error. This model is typically used when higher errors are not important to the observation. Accuracy is calculated through MAE by taking the mean of the sum of the absolute values of the differences between the predicted and actual values. The formula is displayed in Figure 2.3 . . (Figure 2.3) . 3. Related Work . Various research topics have already been analyzed in the field of collaborative filtering. A few similar pieces of research provide similar methods and implementations. A plethora of CF models has been implemented and researched. . 3.1 Similar Research . Similar research was conducted by Mojdeh Saadati et al. [7] on the implementations of various models on movie recommender systems. The two models used in the experiment were the matrix factorization model SVD and a deep learning implementation of the Restricted Boltzmann Machine. The performance of the implementations was measured in the RMSE metric. However, in this research, we go over a broader spectrum of algorithms from each group of implementations and produce results for a larger picture between the different implementations. . 3.2 Further Research . He et al. [8] Conducted Research on how Matrix Factorization could be implemented with Deep Learning to create a better performing model for collaborative filtering. The proposed idea was to use two main models, the Generalized Matrix Factorization (GMF), and the Multi-Layer Perceptron (MLP) to create a hybrid between deep learning and matrix factorization implementations to create a new neural matrix factorization model . dubbed “NeuMF”. NeuMf was then compared to other well-known models such as KNN and ALS implementations and proved to have better accuracy and less training loss. . Yedder et al. [9] Researched the performance of the restricted Boltzmann machine. Various hidden units, learning rates, and other factors were incorporated into the research. Problems such as data sparsity were also encountered in the research and required other methods of implementation. This research also utilized the RMSE accuracy metric to rate the performance of the model. The research indicated an excellent RMSE measure of 0.46. . 4. Approach . This study was broken up into three parts, cleaning, implementation, and evaluation. We will first check for invalid values throughout the dataset, then begin to implement algorithms, and finally calculate performance values. . 4.1 Dataset . The data that would be primarily used in this study is the Kaggle dataset “Anime . Recommendations Database” [10] based on the data collected from myanimelist.net and is included in the reference section of the study. The two files contained within the database are the “ratings.csv” and “anime.csv”. The research would mostly work with the ratings file, but the anime dataset could be further implemented in future research incorporating variables such as show genre and overall ratings. The anime dataset is separated into three separate columns. The user_id, anime_id, and ratings columns are provided for analyzing similarities between users. The format and example of the file are illustrated in Figure 4.1. . (Figure 4.1) . The python module “pandas” was used to import the data into a data frame which could . then be cleaned and analyzed. Making a copy of the dataset to work on is recommended. Maintaining two different data frames would help compare the results on the original to the results in the cleaned dataset. . The first thing to take note of while cleaning is to remove all rows with a rating of “-1”. Such rows represent user data that does not have a specified rating for the specific show that they watched. This step is crucial as having such data in the project would result in a sparse dataset, which directly decreases performance as shown later in the results. A total of 7.8 million rows of data were in the dataset, after cleaning, about 6.3 million rows remained. The distribution of this data is shown in figure 4.2. . . (Figure 4.2) . Another issue arose again while trying to apply the dataset to our engine. Certain cells in . our dataset possibly had ‘NaN’ values, which would in turn cause an error in actual predictions where a value of ‘NaN’ would be returned instead of an integer of the predicted rating. To resolve this issue, we first applied the ‘to_numeric’ method from the pandas module to convert each value into workable integer or float values. After, we could then finally lower the runtime of our experiment by decreasing the size of our dataset from 6 million down to 125,000. This number was picked for easier splitting while implementing other algorithms and deep learning which require testing and training datasets. From there, we then calculated the distribution of the ratings by counting the number of each rating in our dataset. After clearing, it’s paramount to re index the rows in the data frame. Removing the invalid A distribution of the ratings is demonstrated in Figure 4.3. . . (Figure 4.3) . A correlation heatmap could also be used to detect any possible correlations between the values given, however, as of now, there is no apparent relation. . . (Figure 4.4) . 4.2 Implementation . The python package scikit-surprise contains most of the algorithms implemented in this research. First and foremost, we must declare a rating scale for our dataset by utilizing the reader class, and also initializing our dataset with the Dataset class. We set a new . “ratings“ object over the range of one to ten, which is represented in our data. Using the three columns from our pandas data frame, we could then load the data into surprise using the Dataset.load_from_df() method. . After the data is loaded, we could then specify a “param_list” dictionary. In the dictionary, the name of the specific parameters would be stored as the keys and the possible values as a list in the values. Further explanation of what specific params do for each algorithm is also provided in the surprise documentation. . The param_list dictionary could then be imputed into the GridSearchCV() method along with the algorithm name, performance measures, and a cross-validation iterator of 3. There are other arguments as well, but for the sake of our research, these four should be enough. The final step is to then fit our premade data with our GridSearchCV method with the .fit() function. Once everything is set up, the accuracy metrics could then just be retrieved via the best_score iterator. . The deep learning algorithm specifically was generated using an embedding layer with . dropout layers. The entire network consists of 4 total layers. After setting up the net, we can begin to train our model and loop over varying amounts of epochs to find the accuracy that is desired. . To determine the RMSE and the MAE values of this approach, we could separate our dataset into predictions and truth arrays. We can then apply the formulas for both RMSE and MAE as shown in Figures 2.2 and 2.3. We can implement these metrics in two ways. The first method is to use a NumPy array to subject each vector of values from each other and then to apply the formula to our newly generated values. Our second approach is more basic and rudimentary. We could subtract each truth value from each prediction value in our function by declaring a function and then apply the formula to our sum. . 5. Results . In this research, two sets of results were collected. The data collected on the cleaned dataset would serve to be our solution to the first question. However, the data collected on the original dataset would be utilized to answer . the second question concerning the performance measures on sparse datasets. 5.1 Cleaned Dataset . We have gathered both RMSE and MAE values for each algorithm used on our cleaned dataset from our implementations. In the first part of this experiment, the data set has already been cleaned of any invalid values and has reduced sparsity. The majority of the algorithms implemented all showed similar results except a couple of outliers and certain points. A table of the data collected is shown in Figure 5.1 . . (Figure 5.1) . Using the RMSE as the x-axis and MAE for the y-axis, we can plot a scatter plot of our data (Figure 5.2). . . (Figure 5.2) . Removing the Normal Predictor outlier value provides us with a clearer image of the differences in the lower valued points (Figure 5.3). . . (Figure 5.3) . Our data shows that the worst-performing algorithm that we had implemented was the Normal Predictor algorithm from the surprise package, while the best-performing implementation was the Deep Learning algorithm based on neural networks. . 5.1 Pre-cleaned Dataset . Loading up a new dataset, we can effectively run all our algorithms again while maintaining the sparsity of the original dataset. The only cleaning that had to be done was to remove all NaN values and also convert all data to numeric values. The results of running the . implementations on the sparser dataset are recorded in the figure below (Figure 4.5) . . (Figure 5.4) . The results of the various algorithms on this sparse dataset were also plotted on a scatter plot as shown below in Figure 5.5. . . (Figure 5.5) . Once again, removing the normal predictor outlier gives a clearer representation of our other data points (Figure 5.6). . . (Figure 5.6) . Although the values have increased by a considerable amount in the sparse dataset, the common trend remains between the various data points, and no changes are observed in the best and worst algorithms. Comparing the . results of the cleaned and original data sets, a clear difference could be observed in the RMSE and MAE measures (Figures 5.7-8) . (Figure 5.7) . (Figure 5.8) . 6. Discussion . After analyzing the data collected from the research, the deep learning algorithm and the KNNBaseline implementations were observed to be the best performing with the least error observed with both RMSE and MAE metrics. . A possible reason for the results could be the usage of randomness in the calculation of the Normal Predictor algorithm. The deep learning algorithm may have performed the best because of the various training cycles allocated to it which helped to create a more accurate model after each iteration. Vice versa, the opposite could also be applied to the two worst performing algorithms KNNBasic and Normal Predictor. Such implementations . had basic calculations and weren’t able to take into account outliers and other potential biases in the data. . There was an attempt in the research to implement a Restricted Boltzmann Machine (RBM) model, however, the implementation gave varying results and was difficult to judge the extra ratings of the implementation. This . research could be further pursued in the future with the addition of more deep learning implementations and a narrower focus on the subject of deep learning as a whole. From the results acquired, deep learning has been shown to have improved results compared to other algorithms. Focused research on deep learning implementations would provide the reasoning behind deep learning accuracy. . 7. Conclusion . In this research, we explored various machine learning algorithms, K-Nearest neighbors, Matrix Factorization, Deep learning, etc. Several approaches were implemented from the categories mentioned then tested for accuracy measures. . In both the cleaned and original datasets, the deep learning implementation was shown to the least margin of error when making recommendations. From this, it could be deduced that deep learning is a viable method for collaborative filtering engines working with user, item and rating data. Although different sets of data have varying optimal algorithms, . deep learning was still shown to be extremely accurate compared to other tested algorithms. Analyzing the data collected from the sparse dataset, we can conclude that a sparser set of data would result in less accurate recommendations, sometimes up to double the margin of error observed. Because of this observation, it can be concluded that collaborative filtering best performs with dense datasets. . References . [1] Kirzhner, Elena. “Machine Learning. Explanation of Collaborative . Filtering vs Content Based . Filtering.” Medium, Codeburst, 11 . May 2018, codeburst.io/explanation of-recommender-systems-in . information-retrieval-13077e1d916c. [2] Harrison, Onel. “Machine Learning Basics with the K-Nearest Neighbors Algorithm.” Medium, Towards Data Science, 14 July 2019, . towardsdatascience.com/machine . learning-basics-with-the-k-nearest . neighbors-algorithm-6a6e71d01761. . [3] Chen, Denise. “Recommendation System - Matrix Factorization.” . Medium, Towards Data Science, 9 . July 2020, . towardsdatascience.com/recommend ation-system-matrix-factorization . d61978660b4b. . [4] Sivanantham, Balavivek. . “Recommendation System Implementation With Deep Learning and PyTorch.” Medium, The Startup, 18 Aug. 2020, . medium.com/swlh/recommendation system-implementation-with-deep learning-and-pytorch-a03ee84a96f4. . [5] Karat, Sahar. “Co . Clustering.” Data Science . Made Simpler, 5 Mar. 2016, datasciencemadesimpler.wor dpress.com/tag/co . clustering/. . [6] Hug, Nicholas. “Welcome to Surprise’ Documentation.” Welcome to Surprise’ . Documentation! - Surprise 1 Documentation, 2015, . surprise.readthedocs.io/en/sta ble/. . [7] Mojdeh Saadati, Syed Shihab, Mohammed Shaiqur Rahman “Movie Recommender Systems: Implementation and Performance . Evaluation.” Semantic Scholar, 2019, . www.semanticscholar.org/paper/Mo vie-Recommender-Systems%3A Implementation-and-Saadati Shihab/01470f39285213e53f365ce0 1417b18d12467563#citing-papers. . [8] Xiangnan, He, et al. “Neural Collaborative Filtering.” . International World Wide Web Conference Committee, 3 Apr. 2017. . [9] Yedder, Hanene Ben, et al. “Modeling Prediction in . Recommender Systems Using Restricted Boltzmann Machine.” . IEEE Explore, IEEE, 5 Oct. 2017, ieeexplore.ieee.org/abstract/documen t/8122923. . [10] CooperUnion. “Anime . Recommendations Database.” . Kaggle, Kaggle, 21 Dec. 2016, . www.kaggle.com/CooperUnion/ani me-recommendations-database. . [11] Kampakis, Stylianos. “Performance Measures: RMSE and MAE.” The Data Scientist, The Data Scientist, 26 Nov. 2020, . thedatascientist.com/performance measures-rmse-mae/. .",
            "url": "https://ylu-1258.github.io/YLu-Blog/2022/08/28/An-Empirical-Study-on-Performance-Measures-of-Collaborative-Filtering-Recommendation-Algorithms.html",
            "relUrl": "/2022/08/28/An-Empirical-Study-on-Performance-Measures-of-Collaborative-Filtering-Recommendation-Algorithms.html",
            "date": " • Aug 28, 2022"
        }
        
    
  
    
        ,"post64": {
            "title": "Alex Lu   Lab #1_ Measurements And Graphical Analysis",
            "content": "Lab #1: Measurements and Graphical Analysis . Alex Lu . Purpose: . Given disks of different radii, determine the relationship between the mass and the radius of the disks through graphical method and calculate the uncertainty associated with the measured value. We will learn about linearization and use it to create a mathematical model. . Materials and Equipment: . Balance . Meter stick . Circular disks (identical thickness and uniform density but different radii) . Graphic Calculator or online graphing tool . Procedure: . Substitute equations to get a relationship between mass and radius. . | Use the relationship between mass and radius to determine what variable should be processed to linearize the data . | Measure the radius (in cm) of the cylinder with the ruler with one end of the ruler at the center of the disk and the other on the edge. . | Zero out the balance and then measure the mass (in grams) of the cylinder. . | Stack the metal cylinders and measure the collective height of all cylinders. Divide by the total number of cylinders multiplied by 2 to the power of the number of folds to find the height of each individual cylinder. . | Organize the collected data into a data table . | Plot the original radius and mass . | Plot the processed radius and mass . | Plug height and other constants to create a relationship between mass and radius. . | Equations and calculations: . Let: . ( rho) be density . | (m) be mass . | (v) be volume . | (a) be the surface area of the cylinder . | (r) be the radii of the cylinder . | (h) be the height of the cylinder . | . Thus: . If ( rho = frac{m}{v}) and (v = ah) then . (m = rho v) . (m = rho text{ah}) . If (a = pi r^{2}), then . (m = rho pi r^{2}h) . Since both ( rho) and ( pi) are constants and (h) is negligible, ∴ (m propto r^{2}) . Since mass is directly proportional to radius squared, the graph could be linearized if (r^{2}) is plotted instead. . Precision and Uncertainty . The balanced used to measure the mass of the metal disks was accurate up to a hundredth of a gram, while the ruler used to measure the radius and height of the disks was accurate up to 1 millimeter (0.1 cm). As such, the radius was measured to the nearest hundredth of a centimeter or a tenth of a millimeter. . | Data Tables . m = mass . r = radii . H = height . Mass in Grams, Radius and height in cm, and Radius Squared of Each Metal Disk . Disk m (g) h (cm) r (cm)   r^2 (cm^2) . 1 | 0.07 | 0.0025 | 2.30 |   | 5.29 | . 2 | 0.14 | 0.0025 | 3.00 |   | 9.0 | . 3 | 0.25 | 0.0025 | 4.30 |   | 18.49 | . 4 | 0.37 | 0.0025 | 4.90 |   | 24.01 | . 5 | 0.73 | 0.0025 | 7.10 |   | 50.41 | . Graph of Non-Linearized Data: . . Equation: (y = 0.0133921x^{2} + 0.112825x - 0.0238674) . R2 : (0.9967) . This graph has non-linear data as it’s represented by a quadratic model. . Graph of Linearized Data: . . Line of the best fit equation: (y = 0.0145569x - 0.000100527) . r = (0.9983) . r2 = (0.9966 ) . This graph has linear data as it’s represented by a linear model. . Analysis Questions: . 1) What is the independent variable in your y = mx +b formula? . Considering the mathematic relationship between disk radius r and disk mass m, the independent variable from my line of best fit represents the radius of the disk squared. . 2) What does the slope represent in your y = mx +b formula? Show dimensionally that indeed that is what your slope represents and that the formula is valid dimensionally. . Using the (m = rho v) equation and the volume relationship, we can express the disk’s mass in terms of the radius and the height in this relationship: (m = rho pi r^{2}h). Since density (( rho)) and ( pi) are constants, and height ((h)) is negligible, we can essentially group these 3 constants into one value serving as the coefficient to the radius squared term. Thus, the slope is the product of the density, height, and ( pi). Since height has its units expressed in cm, density has units expressed in g/cm³, and ( pi) is a constant wth no units, the slope is a combination of all 3 constants and has units of g/cm2. . Because our slope has units of g/cm2, and x represents the radius squared expressed by cm2, and since our desired output ((y)) is mass in grams (g), the y-intercept ((b)) must have a unit of grams (g) in order for both sides of the equation to have the same units (grams). . (g = frac{g}{cm^{2}} cdot cm^{2} + g) . (g = g + g) . (g = g) . 3) Should the “b” in your y = mx + b formula be zero? Explain your answer. . The y-intercept ((b)) in my (y = mx + b) formula shouldn’t be zero unless the data naturally generates a line of best fit that passes through the origin. Forcing the y-intercept to pass through the origin would either alter the slope ((m)) or remove the y-intercept completely. This would result in an inaccurate interpretation of the model as the line is no longer set evenly between the data points. Thus we cannot guarantee the model’s integrity for future data points, rendering our model useless. . 4) Measure/estimate the “thickness” of your cylinders. Use that value to find the experimental density of your cylinders. Find a percent difference between your found density and the actual density. The actual material is aluminum. . (y = 0.0145569x - 0.000100527) . (slope = pi rho h) . ( rho = density) . ( rho = frac{ text{slope}}{h pi} = frac{0.0145569 frac{g}{cm^{2}}}{ pi(0.0025cm)} = 1.85 frac{g}{cm^{3}}) . The density of the metal disks according to my linear model is (1.85 frac{g}{cm^{3}}) , Since the metal used in the disk is Aluminum (density = (2.70 frac{g}{cm^{3}})) we can use the following calculations to determine our percent error . ( % Error = left | frac{Actual - Expected}{ text{Expected}} right | *100 % ) | . ( % Error = left | frac{1.85 frac{g}{cm^{3}} - 2.70 frac{g}{cm^{3}}}{2.70 frac{g}{cm^{3}}} right | *100 % = 31.5 % ) | . ∴ Percent difference = 31.5% . 5) Errors. Make sure you explain why your number is bigger or smaller than (if positive or negative difference.) . The number that I obtained is smaller than the actual value (31.5% error) because the instruments that we used to measure the disks’ dimensions had inaccuracies. The ruler that we used to measure the disks’ radius is only precise up to millimeters, and the balanced used to measure mass is only accurate up to the hundredth’s place, while actual instruments used to measure the actual value are much more precise. Additionally, the method I used to measure the height of the disks also had inherent inaccuracies, as folding the aluminum disks over each other may create tiny air pockets that add additional magnitude to my measured height, resulting in an inaccurate height measurement. The uncertainty of the ruler and also the inaccurate way of measuring height may have contributed to a greater height value than the actual value, resulting in a lower density value when I divided. Lastly, the disks weren’t in a perfectly circular shape, thus the direction that I measured the disk in might also have affected the end result, as measuring in different directions would produce different radii. . Synthesis Questions: . 1) In this experiment, if we had used disks with a greater thickness, would the slope of your best fit line have been different? Would your experimental value for density be the same? Explain. . Since slope represents the product of thickness, ( pi), and density, having a greater measured thickness would result in a much larger slope value. However, our experimental value of density would still be around the same, as an increase in thickness would also result in a proportionally large increase in mass, which will offset the thickness increase. . 2. How would your graph of m versus r2 be different if you had used disks of the same . thickness but made out of steel? Draw a second line on your m versus r2 plot that . represents disks made of steel. . The graph would be of m versus r2 would be different in that the slope of the graph would be much greater, this is because steel has a much higher density of (7.85 frac{g}{cm^{3}}) compared to aluminum’s (2.7 frac{g}{cm^{3}}) . . (Note: The blue line is a rough sketch of what the line of best fit would look like on the same scale as the aluminum graph is steel was used instead. This is not an accurate representation, just an approximation) . 3. Another group of students has acquired data for the exact same experiment; however, their disks are made of an unknown material that they are trying to determine. The group’s m versus r2 data produced a line of best fit with slope equal to 122 kg/m2. Each disk they measured had the same 0.5 cm thickness. Calculate the density of the unknown material and use the table below to help determine what material their disks are made of. . Work: . (Slope = 122kg/m^{2} , h = 0.5cm, slope = rho pi h) . (slope = 122 frac{ text{kg}}{m^{2}}( frac{1000g}{1kg})( frac{1m}{100cm})^{2} = 12.2 frac{g}{cm^{2}} = rho pi h) . ( rho = frac{ text{slope}}{ pi h} = frac{12.2 frac{g}{cm^{2}}}{ pi*0.5cm} = 7.77 frac{g}{cm^{3}}) . The closet material to a density of (7.77 frac{g}{cm^{3}}) is iron, which has a density of (7.8 frac{g}{cm^{3}}). The disks are most likely made out of iron. . Multiple Choice Questions: . 1. You perform the same experiment, but this time you plot a linear relationship between mass and the circumference of the disks rather than the radius. What is the slope of the linear plot? . Work: . (slope = rho pi h) . (m = rho v = rho pi r^{2}h) . (v = pi r^{2}h) . Let (c) be circumference . (c = 2 pi r) . (c^{2} = 4 pi^{2}r^{2}) . (r^{2} = frac{c^{2}}{4 pi^{2}}) . (m = rho pi h( frac{c^{2}}{4 pi^{2}}) = frac{ rho hc^{2}}{4 pi} = ( frac{ rho h}{4 pi})c^{2}) . () . Since ( rho) (density), h (height), and (4 pi) are all constants, these 3 values could be combined to form the slope for the linear plot, which is (( frac{ text{ph}}{4 pi})), Hence, E is the correct answer choice. . 2. Skipped . 3. Consider an experiment in which a student measures the mass and diameter of 10 . different-sized spheres, all made of the same material of uniform density ρ. For this . student to create a linear graph relating the mass of the sphere to its radius r, the . student would need to plot mass m versus which quantity: . ( text{let v be the volume of a sphere}) . (v = frac{4}{3} pi r^{3}) . (m = pv = ( frac{4p pi}{3})r^{3}) . Since (( frac{4p pi}{3})) is a constant, the student must plot a linear graph relating m versus the quality of radius cubed. Hence, option C is correct. .",
            "url": "https://ylu-1258.github.io/YLu-Blog/2022/08/28/Alex-Lu-Lab-1_-Measurements-and-Graphical-Analysis.html",
            "relUrl": "/2022/08/28/Alex-Lu-Lab-1_-Measurements-and-Graphical-Analysis.html",
            "date": " • Aug 28, 2022"
        }
        
    
  
    
        ,"post65": {
            "title": "Installation Checks for Alex Lu",
            "content": "Defining color vars . RED=&#39; 033[0;31m&#39; GREEN=&#39; 033[0;32m&#39; BLUE=&#39; 033[0;34m&#39; MAGENTA=&#39; 033[0;35m&#39; CYAN=&#39; 033[0;36m&#39; NC=&#39; 033[0;0m&#39; . Checking for python installation . function CheckPythonInstall() { echo -e &quot;${BLUE}Checking python version${NC}&quot; if [[ $(python --version) ]]; then pyversion=$(python --version | cut -d&quot; &quot; -f2) echo -e &quot;${GREEN} - Python version ${MAGENTA}$pyversion${GREEN} has been installed! ${NC}&quot; else echo -e &quot;${RED} - Python not found ${NC}&quot; fi } CheckPythonInstall #python --version #python3 --version . Checking python version - Python version 3.9.12 has been installed! . Checking for Java intsallation . function CheckJavaInstall() { echo -e &quot;${CYAN}Checking java version${NC}&quot; if [[ $(java --version) ]]; then javaversion=$(java --version | head -n 1 | cut -d&quot; &quot; -f2) echo -e &quot;${GREEN} - Java version ${MAGENTA}$javaversion${GREEN} has been installed! ${NC}&quot; else echo -e &quot;${RED} - Java not found ${NC}&quot; fi } function CheckJavaCInstall() { echo -e &quot;${CYAN}Checking java compiler version${NC}&quot; if [[ $(javac --version) ]]; then javacversion=$(javac --version | head -n 1 | cut -d&quot; &quot; -f2) echo -e &quot;${GREEN} - JavaC version ${MAGENTA}$javacversion${GREEN} has been installed! ${NC}&quot; else echo -e &quot;${RED} - JavaC not found ${NC}&quot; fi } CheckJavaInstall CheckJavaCInstall . Checking java version - Java version 11.0.16 has been installed! Checking java compiler version - JavaC version 11.0.16 has been installed! . Checking For Anaconda installation . function CheckJupyterInstall() { echo -e &quot;${CYAN}Checking anaconda version${NC}&quot; if [[ $(conda --version) ]]; then condaversion=$(conda --version | cut -d&quot; &quot; -f2) echo -e &quot;${GREEN} - Anaconda version ${MAGENTA}$condaversion${GREEN} has been installed! ${NC}&quot; else echo -e &quot;${RED} - Anaconda not found ${NC}&quot; fi } CheckJupyterInstall . Checking jupyter version - Anaconda version 4.13.0 has been installed! . Checking for Anaconda package installation . function CheckCondaPackageInstall() { echo -e &quot;${CYAN}Checking jupyter package version${NC}&quot; if [[ $(conda list | grep $1) ]]; then packageversion=$(conda list | grep $1 | awk &#39;{print $2}&#39;) echo -e &quot;${GREEN} - Conda pacakge ${MAGENTA}$1${GREEN} version ${MAGENTA}$packageversion${GREEN} has been installed! ${NC}&quot; else echo -e &quot;${RED} - Conda package $1 not found ${NC}&quot; fi } CheckCondaPackageInstall &quot;nodejs&quot; . Checking jupyter package version - Conda pacakge nodejs version 6.11.2 has been installed! . Checking installed Jupyter kernels . function CheckJupyterKernelInstall() { echo -e &quot;${CYAN}Checking jupyter ${MAGENTA}$1${CYAN} kernel installation${NC}&quot; if [[ $(jupyter kernelspec list | grep $1) ]]; then kernelpath=$(jupyter kernelspec list | grep $1 | awk &#39;{print $2}&#39;) echo -e &quot;${GREEN} - Jupyter ${MAGENTA}$1${GREEN} kernel has been found at ${MAGENTA}$kernelpath${NC}&quot; else echo -e &quot;${RED} - Jupyter ${MAGENTA}$1${GREEN} kernel not found ${NC}&quot; fi } kernels=&quot;bash javascript python3&quot; for i in $kernels; do CheckJupyterKernelInstall $i done . Checking jupyter bash kernel installation - Jupyter bash kernel has been found at /home/eris29/.local/share/jupyter/kernels/bash Checking jupyter javascript kernel installation - Jupyter javascript kernel has been found at /home/eris29/.local/share/jupyter/kernels/javascript Checking jupyter python3 kernel installation - Jupyter python3 kernel has been found at /home/eris29/.local/share/jupyter/kernels/python3 .",
            "url": "https://ylu-1258.github.io/YLu-Blog/bash_checks",
            "relUrl": "/bash_checks",
            "date": " • Aug 26, 2022"
        }
        
    
  
    
        ,"post66": {
            "title": "Intro to Pandas",
            "content": "Pandas . I will use a dataset of Fortune 1000 from Kaggle datasets to explore pandas library here. . The Fortune 1000 dataset is from the Fortune website. It contains U.S. company data for the year 2021. The dataset is 1000 rows and 18 columns. . Features: . Company - values are the name of the company Rank - The 2021 rank established by Fortune (1-1000) | Rank Change - The change in the rank from 2020 to 2021. There is only a rank change listed if the company is currently in the top 500 and was previously in the top 500. | Revenue - Revenue of each company in millions. This is the criteria used to rank each company. | Profit - Profit of each company in millions. Num. of Employees - The number of employees each company employs. | Sector - The sector of the market the company operates in. | City - The city where the company&#39;s headquarters is located. | State - The state where the company&#39;s headquarters is located | Newcomer - Indicates whether or not the company is new to the top Fortune 500 (&quot;yes&quot; or &quot;no&quot;). No value will be listed for companies outside of the top 500. | CEO Founder - Indicates whether the CEO of the company is also the founder (&quot;yes&quot; or &quot;no&quot;). | CEO Woman - Indicates whether the CEO of the company is a woman (&quot;yes&quot; or &quot;no&quot;). | Profitable - Indicates whether the company is profitable or not (&quot;yes&quot; or &quot;no&quot;). | Prev. Rank - The 2020 rank of the company, as established by Fortune. There will only be previous rank data for the top 500 companies. | CEO - The name of the CEO of the company | Website - The url of the company website | Ticker - The stock ticker symbol of public companies. Some rows will have empty values because the company is a private corporation. | Market Cap - The market cap (or value) of the company in millions. Some rows will have empty values because the company is private. Market valuations were determined on January 20, 2021. | . !wget -nc /content/ https://datasets21.s3-us-west-1.amazonaws.com/Fortune_1000.csv . /content/: Scheme missing. File ‘Fortune_1000.csv’ already there; not retrieving. . import pandas as pd f1000 = pd.read_csv(&#39;Fortune_1000.csv&#39;,index_col=0) . f1000.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Index: 1000 entries, Walmart to Liberty Oilfield Services Data columns (total 17 columns): # Column Non-Null Count Dtype -- -- 0 rank 1000 non-null int64 1 rank_change 1000 non-null float64 2 revenue 1000 non-null float64 3 profit 998 non-null float64 4 num. of employees 1000 non-null int64 5 sector 1000 non-null object 6 city 1000 non-null object 7 state 1000 non-null object 8 newcomer 500 non-null object 9 ceo_founder 1000 non-null object 10 ceo_woman 1000 non-null object 11 profitable 1000 non-null object 12 prev_rank 1000 non-null object 13 CEO 992 non-null object 14 Website 1000 non-null object 15 Ticker 938 non-null object 16 Market Cap 960 non-null object dtypes: float64(3), int64(2), object(12) memory usage: 140.6+ KB . f1000.head(3) . rank rank_change revenue profit num. of employees sector city state newcomer ceo_founder ceo_woman profitable prev_rank CEO Website Ticker Market Cap . company . Walmart 1 | 0.0 | 523964.0 | 14881.0 | 2200000 | Retailing | Bentonville | AR | no | no | no | yes | 1.0 | C. Douglas McMillon | https://www.stock.walmart.com | WMT | 411690 | . Amazon 2 | 3.0 | 280522.0 | 11588.0 | 798000 | Retailing | Seattle | WA | no | yes | no | yes | 5.0 | Jeffrey P. Bezos | https://www.amazon.com | AMZN | 1637405 | . Exxon Mobil 3 | -1.0 | 264938.0 | 14340.0 | 74900 | Energy | Irving | TX | no | no | no | yes | 2.0 | Darren W. Woods | https://www.exxonmobil.com | XOM | 177923 | . Select data using those labels . Because the axes in pandas have labels, I can select data using those labels — unlike in NumPy, where I needed to know the exact index location. To do this, I can use the DataFrame.loc[] attribute. The syntax for DataFrame.loc[] is: . df.loc[row_label, column_label] . Select Single Column . companies = f1000.loc[:,&#39;profit&#39;] print(companies) print(type(f1000)) print(type(companies)) . company Walmart 14881.0 Amazon 11588.0 Exxon Mobil 14340.0 Apple 55256.0 CVS Health 6634.0 ... Mr. Cooper Group 274.0 Herc Holdings 47.5 Healthpeak Properties 45.5 SPX FLOW -95.1 Liberty Oilfield Services 39.0 Name: profit, Length: 1000, dtype: float64 &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; &lt;class &#39;pandas.core.series.Series&#39;&gt; . Series object . print(f1000.loc[:,&#39;revenue&#39;]) print(f1000.loc[&#39;Apple&#39;,&#39;revenue&#39;]) . company Walmart 523964.0 Amazon 280522.0 Exxon Mobil 264938.0 Apple 260174.0 CVS Health 256776.0 ... Mr. Cooper Group 2007.0 Herc Holdings 1999.0 Healthpeak Properties 1997.4 SPX FLOW 1996.3 Liberty Oilfield Services 1990.3 Name: revenue, Length: 1000, dtype: float64 260174.0 . Select multiple columns . List of columns | Slice of columns | . f1000[[&#39;rank&#39;,&#39;revenue&#39;]] #f1000.loc[:,&#39;rank&#39;:&#39;sector&#39;] . rank revenue . company . Walmart 1 | 523964.0 | . Amazon 2 | 280522.0 | . Exxon Mobil 3 | 264938.0 | . Apple 4 | 260174.0 | . CVS Health 5 | 256776.0 | . ... ... | ... | . Mr. Cooper Group 996 | 2007.0 | . Herc Holdings 997 | 1999.0 | . Healthpeak Properties 998 | 1997.4 | . SPX FLOW 999 | 1996.3 | . Liberty Oilfield Services 1000 | 1990.3 | . 1000 rows × 2 columns . Select rows by labels . f1000.loc[[&#39;Amazon&#39;, &#39;Apple&#39;]] #f1000.loc[&#39;Amazon&#39;:&#39;Apple&#39;] . rank rank_change revenue profit num. of employees sector city state newcomer ceo_founder ceo_woman profitable prev_rank CEO Website Ticker Market Cap . company . Amazon 2 | 3.0 | 280522.0 | 11588.0 | 798000 | Retailing | Seattle | WA | no | yes | no | yes | 5.0 | Jeffrey P. Bezos | https://www.amazon.com | AMZN | 1637405 | . Apple 4 | -1.0 | 260174.0 | 55256.0 | 137000 | Technology | Cupertino | CA | no | no | no | yes | 3.0 | Timothy D. Cook | https://www.apple.com | AAPL | 2221176 | . Series.value_counts() method . Series.value_counts() method. This method displays each unique non-null value in a column and their counts in order. . sector_value_counts = f1000[&#39;sector&#39;].value_counts(ascending=True) print(sector_value_counts) . Food &amp; Drug Stores 10 Telecommunications 11 Apparel 16 Motor Vehicles &amp; Parts 22 Aerospace &amp; Defense 22 Media 25 Household Products 26 Hotels, Restaurants &amp; Leisure 27 Chemicals 27 Engineering &amp; Construction 30 Wholesalers 35 Food, Beverages &amp; Tobacco 37 Transportation 38 Materials 46 Industrials 50 Business Services 52 Health Care 71 Retailing 75 Technology 109 Energy 109 Financials 162 Name: sector, dtype: int64 . f1000[&#39;sector&#39;].value_counts().loc[&#39;Technology&#39;] . 109 . #f1000[&#39;rank_change&#39;].max() #f1000[&#39;rank_change&#39;].min() f1000[&#39;rank_change&#39;].describe() . count 1000.000000 mean 0.426000 std 22.424169 min -186.000000 25% 0.000000 50% 0.000000 75% 0.000000 max 224.000000 Name: rank_change, dtype: float64 . f1000[&#39;rank_change&#39;].value_counts() . 0.0 544 -1.0 22 -2.0 18 2.0 16 4.0 15 ... -30.0 1 -43.0 1 98.0 1 86.0 1 -87.0 1 Name: rank_change, Length: 118, dtype: int64 . Exercise: . List out the numbers of companies in the Fortune 1000 of the top 3 states . top_3_states = f1000[&#39;state&#39;].value_counts().head(3) print(top_3_states) . CA 121 TX 95 NY 89 Name: state, dtype: int64 . Exercise: . find the company that employs the most people in California in the dataset. . I can use the DataFrame.sort_values() method to sort the rows on the employees column . f1000[f1000[&#39;state&#39;]==&#39;CA&#39;].sort_values(&#39;num. of employees&#39;,ascending=False).head(1) . rank rank_change revenue profit num. of employees sector city state newcomer ceo_founder ceo_woman profitable prev_rank CEO Website Ticker Market Cap . company . Wells Fargo 30 | -1.0 | 103915.0 | 19549.0 | 259800 | Financials | San Francisco | CA | no | no | no | yes | 29.0 | Charles W. Scharf | https://www.wellsfargo.com | WFC | 99941 | . Exercise: . find the unique list of states in the dataset . To identify the unique states, I can use the Series.unique() method. This method returns an array of unique values from any series. . states = f1000[&#39;state&#39;].unique() print(states) . [&#39;AR&#39; &#39;WA&#39; &#39;TX&#39; &#39;CA&#39; &#39;RI&#39; &#39;NE&#39; &#39;MN&#39; &#39;PA&#39; &#39;MI&#39; &#39;CT&#39; &#39;OH&#39; &#39;NY&#39; &#39;IL&#39; &#39;DC&#39; &#39;NC&#39; &#39;GA&#39; &#39;IN&#39; &#39;MA&#39; &#39;NJ&#39; &#39;VA&#39; &#39;MO&#39; &#39;TN&#39; &#39;KY&#39; &#39;ID&#39; &#39;MD&#39; &#39;OR&#39; &#39;FL&#39; &#39;WI&#39; &#39;CO&#39; &#39;OK&#39; &#39;LA&#39; &#39;DE&#39; &#39;AZ&#39; &#39;IA&#39; &#39;NV&#39; &#39;KS&#39; &#39;AL&#39; &#39;SC&#39; &#39;ND&#39; &#39;NH&#39; &#39;MS&#39; &#39;PR&#39; &#39;UT&#39; &#39;HI&#39; &#39;VT&#39; &#39;ME&#39;] . Practice: . I&#39;m going to produce the following dictionary of the top employer in each state: . create an empty dictionary, top_employer_by_state to store the results of the exercise. . | Use the Series.unique() method to create an array of unique values from the state column . | Use a for loop to iterate over the array unique states. In each iteration: . | Select only the rows that have a state name equal to the current iteration. | Use DataFrame.sort_values() to sort those rows by the num. of employees column in descending order. | Select the first row from the sorted dataframe and convert the Dataframe into a series using DataFrame.squeeze() | Extract the company name from the index label company by Series.name. | Assign the results to the top_employer_by_state dictionary, using the state name as the key, and the company name as the value. | . top_exmployer_by_state = {} states = f1000[&#39;state&#39;].unique() for state in states: selected_companies = f1000[f1000[&#39;state&#39;]==state] top_exmployer_by_state[state] = selected_companies.sort_values(&#39;num. of employees&#39;, ascending=False).head(1).squeeze().name . for key in top_exmployer_by_state: print(key, &#39; : &#39;, top_exmployer_by_state[key]) . AR : Walmart WA : Amazon TX : Yum China Holdings CA : Wells Fargo RI : CVS Health NE : Berkshire Hathaway MN : Target PA : Aramark MI : Ford Motor CT : XPO Logistics OH : Kroger NY : IBM IL : Walgreens Boots Alliance DC : Danaher NC : Lowe&amp;#8217;s GA : Home Depot IN : Anthem MA : TJX NJ : Cognizant Technology Solutions VA : Hilton Worldwide Holdings MO : Emerson Electric TN : FedEx KY : Humana ID : Albertsons MD : Marriott International OR : Nike FL : Publix Super Markets WI : Kohl&amp;#8217;s CO : VF OK : Helmerich &amp; Payne LA : Lumen Technologies DE : DuPont AZ : Republic Services IA : Casey&amp;#8217;s General Stores NV : MGM Resorts International KS : Yellow AL : Encompass Health SC : Sonoco Products ND : MDU Resources Group NH : PC Connection MS : Sanderson Farms PR : Popular UT : Nu Skin Enterprises HI : Hawaiian Holdings VT : NLV Financial ME : IDEXX Laboratories . Reference . pandas API reference . pandas vs. NumPy .",
            "url": "https://ylu-1258.github.io/YLu-Blog/jupyter/interests/2022/08/25/Pandas-for-Beginners.html",
            "relUrl": "/jupyter/interests/2022/08/25/Pandas-for-Beginners.html",
            "date": " • Aug 25, 2022"
        }
        
    
  
    
        ,"post67": {
            "title": "Intro to Python Lecture",
            "content": "Lecture - 08/25/2022 . To build up a relationshhip with my partner, I should comment in his blog and also open up review tickets to communicate with him . When working with others, always pull before you make any additions, this ensures that we are on the latest version and that there are no difference conflicts. . There are multiple “shells” installed on a linux machine, such as bash, dash and zsh. The commands that we enter into terminal are like a pseudo-language. We can create bash scripts to automate actions in terminal for us. . Bash - Analyzing part 1 . cd $project #cd means &quot;Change Directory&quot;, $project is a variable named project ls # lists directory ls -a # lists directory with hidden files ls -al # lists directory with hidden files in long format . By using bash we can easily create scripts that automate terminal operations for us. . The Cloud . The cloud contains all git repositories. Individual computers can clone repositories from the cloud and down to our own SSD. This creates a link between our local repository and the remote repository. . A pull action will pull any new updates made to the repository down to our local repository and update it with the latest changes. . A push action will push any new updates from our local repository up to our remote directory in the cloud and contribute to the git repository. .",
            "url": "https://ylu-1258.github.io/YLu-Blog/markdown/apcsp/week%201/2022/08/25/Lecture-Intro-Python.html",
            "relUrl": "/markdown/apcsp/week%201/2022/08/25/Lecture-Intro-Python.html",
            "date": " • Aug 25, 2022"
        }
        
    
  
    
        ,"post68": {
            "title": "Intro to Numpy",
            "content": "NumPy . NumPy is the fundamental package for scientific computing with Python. . At the core of the NumPy package, is the ndarray object or n-dimensional array. In programming, array describes a collection of elements, similar to a list. The word n-dimensional refers to the fact that ndarrays can have one or more dimensions. . Here I will directly convert a list to an ndarray using the numpy.array() constructor. To create a 1D ndarray, I can pass in a single list . import numpy as np #using the alias np array_data = np.array([1,2,3,4]) . print(array_data) print(type(array_data)) . [1 2 3 4] &lt;class &#39;numpy.ndarray&#39;&gt; . It&#39;s often useful to know the number of rows and columns in a ndarray. I can use the ndarray.shape attribute. . array_2D = np.array([[5, 10, 15], [20, 25, 30]]) print(array_2D.shape) . (2, 3) . Vectorization in NumPy . The NumPy library takes advantage of a processor feature called Single Instruction Multiple Data (SIMD) to process data faster. SIMD allows a processor to perform the same operation on multiple data points in a single processor cycle. . The concept of replacing for loops with operations applied to multiple data points at once is called vectorization, and ndarrays make vectorization possible. . numbers = [[6, 5], [1, 3], [5, 6], [1, 4], [3, 7], [5, 8], [3, 5], [8, 4]] print(*numbers, sep=&#39; n&#39;) . [6, 5] [1, 3] [5, 6] [1, 4] [3, 7] [5, 8] [3, 5] [8, 4] . sums = [] for row in numbers: row_sum = row[0] + row[1] sums.append(row_sum) print(sums) . [11, 4, 11, 5, 10, 13, 8, 12] . # Convert the list of lists to an ndarray np_numbers = np.array(numbers) sums = np_numbers[:,0] + np_numbers[:,1] print(sums) . [11 4 11 5 10 13 8 12] . When I selected each column, we used the syntax ndarray[:,c] where c is the column index I wanted to select. The colon selects all rows. . #print(np_numbers) #print(np_numbers[1]) #print(np_numbers[2:]) #print(np_numbers[3,1]) #print(np_numbers[:,1]) # Select specific row rows = [0, 2, 4] print(np_numbers[rows, :]) . [[6 5] [5 6] [3 7]] . Explore the numerical dataset . To explore two-dimensional (2D) ndarrays, I&#39;ll analyze New York City taxi trip data released by the city of New York. . !wget -nc /content/ https://datasets21.s3-us-west-1.amazonaws.com/nyc_taxis.csv . /content/: Scheme missing. File ‘nyc_taxis.csv’ already there; not retrieving. . !wget --help . GNU Wget 1.19.4, a non-interactive network retriever. Usage: wget [OPTION]... [URL]... Mandatory arguments to long options are mandatory for short options too. Startup: -V, --version display the version of Wget and exit -h, --help print this help -b, --background go to background after startup -e, --execute=COMMAND execute a `.wgetrc&#39;-style command Logging and input file: -o, --output-file=FILE log messages to FILE -a, --append-output=FILE append messages to FILE -d, --debug print lots of debugging information -q, --quiet quiet (no output) -v, --verbose be verbose (this is the default) -nv, --no-verbose turn off verboseness, without being quiet --report-speed=TYPE output bandwidth as TYPE. TYPE can be bits -i, --input-file=FILE download URLs found in local or external FILE -F, --force-html treat input file as HTML -B, --base=URL resolves HTML input-file links (-i -F) relative to URL --config=FILE specify config file to use --no-config do not read any config file --rejected-log=FILE log reasons for URL rejection to FILE Download: -t, --tries=NUMBER set number of retries to NUMBER (0 unlimits) --retry-connrefused retry even if connection is refused -O, --output-document=FILE write documents to FILE -nc, --no-clobber skip downloads that would download to existing files (overwriting them) --no-netrc don&#39;t try to obtain credentials from .netrc -c, --continue resume getting a partially-downloaded file --start-pos=OFFSET start downloading from zero-based position OFFSET --progress=TYPE select progress gauge type --show-progress display the progress bar in any verbosity mode -N, --timestamping don&#39;t re-retrieve files unless newer than local --no-if-modified-since don&#39;t use conditional if-modified-since get requests in timestamping mode --no-use-server-timestamps don&#39;t set the local file&#39;s timestamp by the one on the server -S, --server-response print server response --spider don&#39;t download anything -T, --timeout=SECONDS set all timeout values to SECONDS --dns-timeout=SECS set the DNS lookup timeout to SECS --connect-timeout=SECS set the connect timeout to SECS --read-timeout=SECS set the read timeout to SECS -w, --wait=SECONDS wait SECONDS between retrievals --waitretry=SECONDS wait 1..SECONDS between retries of a retrieval --random-wait wait from 0.5*WAIT...1.5*WAIT secs between retrievals --no-proxy explicitly turn off proxy -Q, --quota=NUMBER set retrieval quota to NUMBER --bind-address=ADDRESS bind to ADDRESS (hostname or IP) on local host --limit-rate=RATE limit download rate to RATE --no-dns-cache disable caching DNS lookups --restrict-file-names=OS restrict chars in file names to ones OS allows --ignore-case ignore case when matching files/directories -4, --inet4-only connect only to IPv4 addresses -6, --inet6-only connect only to IPv6 addresses --prefer-family=FAMILY connect first to addresses of specified family, one of IPv6, IPv4, or none --user=USER set both ftp and http user to USER --password=PASS set both ftp and http password to PASS --ask-password prompt for passwords --use-askpass=COMMAND specify credential handler for requesting username and password. If no COMMAND is specified the WGET_ASKPASS or the SSH_ASKPASS environment variable is used. --no-iri turn off IRI support --local-encoding=ENC use ENC as the local encoding for IRIs --remote-encoding=ENC use ENC as the default remote encoding --unlink remove file before clobber --xattr turn on storage of metadata in extended file attributes Directories: -nd, --no-directories don&#39;t create directories -x, --force-directories force creation of directories -nH, --no-host-directories don&#39;t create host directories --protocol-directories use protocol name in directories -P, --directory-prefix=PREFIX save files to PREFIX/.. --cut-dirs=NUMBER ignore NUMBER remote directory components HTTP options: --http-user=USER set http user to USER --http-password=PASS set http password to PASS --no-cache disallow server-cached data --default-page=NAME change the default page name (normally this is &#39;index.html&#39;.) -E, --adjust-extension save HTML/CSS documents with proper extensions --ignore-length ignore &#39;Content-Length&#39; header field --header=STRING insert STRING among the headers --max-redirect maximum redirections allowed per page --proxy-user=USER set USER as proxy username --proxy-password=PASS set PASS as proxy password --referer=URL include &#39;Referer: URL&#39; header in HTTP request --save-headers save the HTTP headers to file -U, --user-agent=AGENT identify as AGENT instead of Wget/VERSION --no-http-keep-alive disable HTTP keep-alive (persistent connections) --no-cookies don&#39;t use cookies --load-cookies=FILE load cookies from FILE before session --save-cookies=FILE save cookies to FILE after session --keep-session-cookies load and save session (non-permanent) cookies --post-data=STRING use the POST method; send STRING as the data --post-file=FILE use the POST method; send contents of FILE --method=HTTPMethod use method &#34;HTTPMethod&#34; in the request --body-data=STRING send STRING as data. --method MUST be set --body-file=FILE send contents of FILE. --method MUST be set --content-disposition honor the Content-Disposition header when choosing local file names (EXPERIMENTAL) --content-on-error output the received content on server errors --auth-no-challenge send Basic HTTP authentication information without first waiting for the server&#39;s challenge HTTPS (SSL/TLS) options: --secure-protocol=PR choose secure protocol, one of auto, SSLv2, SSLv3, TLSv1, TLSv1_1, TLSv1_2 and PFS --https-only only follow secure HTTPS links --no-check-certificate don&#39;t validate the server&#39;s certificate --certificate=FILE client certificate file --certificate-type=TYPE client certificate type, PEM or DER --private-key=FILE private key file --private-key-type=TYPE private key type, PEM or DER --ca-certificate=FILE file with the bundle of CAs --ca-directory=DIR directory where hash list of CAs is stored --crl-file=FILE file with bundle of CRLs --pinnedpubkey=FILE/HASHES Public key (PEM/DER) file, or any number of base64 encoded sha256 hashes preceded by &#39;sha256//&#39; and separated by &#39;;&#39;, to verify peer against --random-file=FILE file with random data for seeding the SSL PRNG HSTS options: --no-hsts disable HSTS --hsts-file path of HSTS database (will override default) FTP options: --ftp-user=USER set ftp user to USER --ftp-password=PASS set ftp password to PASS --no-remove-listing don&#39;t remove &#39;.listing&#39; files --no-glob turn off FTP file name globbing --no-passive-ftp disable the &#34;passive&#34; transfer mode --preserve-permissions preserve remote file permissions --retr-symlinks when recursing, get linked-to files (not dir) FTPS options: --ftps-implicit use implicit FTPS (default port is 990) --ftps-resume-ssl resume the SSL/TLS session started in the control connection when opening a data connection --ftps-clear-data-connection cipher the control channel only; all the data will be in plaintext --ftps-fallback-to-ftp fall back to FTP if FTPS is not supported in the target server WARC options: --warc-file=FILENAME save request/response data to a .warc.gz file --warc-header=STRING insert STRING into the warcinfo record --warc-max-size=NUMBER set maximum size of WARC files to NUMBER --warc-cdx write CDX index files --warc-dedup=FILENAME do not store records listed in this CDX file --no-warc-digests do not calculate SHA1 digests --no-warc-keep-log do not store the log file in a WARC record --warc-tempdir=DIRECTORY location for temporary files created by the WARC writer Recursive download: -r, --recursive specify recursive download -l, --level=NUMBER maximum recursion depth (inf or 0 for infinite) --delete-after delete files locally after downloading them -k, --convert-links make links in downloaded HTML or CSS point to local files --convert-file-only convert the file part of the URLs only (usually known as the basename) --backups=N before writing file X, rotate up to N backup files -K, --backup-converted before converting file X, back up as X.orig -m, --mirror shortcut for -N -r -l inf --no-remove-listing -p, --page-requisites get all images, etc. needed to display HTML page --strict-comments turn on strict (SGML) handling of HTML comments Recursive accept/reject: -A, --accept=LIST comma-separated list of accepted extensions -R, --reject=LIST comma-separated list of rejected extensions --accept-regex=REGEX regex matching accepted URLs --reject-regex=REGEX regex matching rejected URLs --regex-type=TYPE regex type (posix|pcre) -D, --domains=LIST comma-separated list of accepted domains --exclude-domains=LIST comma-separated list of rejected domains --follow-ftp follow FTP links from HTML documents --follow-tags=LIST comma-separated list of followed HTML tags --ignore-tags=LIST comma-separated list of ignored HTML tags -H, --span-hosts go to foreign hosts when recursive -L, --relative follow relative links only -I, --include-directories=LIST list of allowed directories --trust-server-names use the name specified by the redirection URL&#39;s last component -X, --exclude-directories=LIST list of excluded directories -np, --no-parent don&#39;t ascend to the parent directory Mail bug reports and suggestions to &lt;bug-wget@gnu.org&gt; . from csv import reader #Load data into the notebook with open(&#39;nyc_taxis.csv&#39;, &#39;r&#39;) as taxis_file: taxis = list(reader(taxis_file)) print(len(taxis)) print(len(taxis[0])) . 2014 15 . import numpy as np np_taxis = np.array(taxis) . print(np_taxis[:3]) np_taxis.shape . [[&#39;pickup_year&#39; &#39;pickup_month&#39; &#39;pickup_day&#39; &#39;pickup_dayofweek&#39; &#39;pickup_time&#39; &#39;pickup_location_code&#39; &#39;dropoff_location_code&#39; &#39;trip_distance&#39; &#39;trip_length&#39; &#39;fare_amount&#39; &#39;fees_amount&#39; &#39;tolls_amount&#39; &#39;tip_amount&#39; &#39;total_amount&#39; &#39;payment_type&#39;] [&#39;2016&#39; &#39;1&#39; &#39;1&#39; &#39;5&#39; &#39;0&#39; &#39;2&#39; &#39;4&#39; &#39;21.00&#39; &#39;2037&#39; &#39;52.00&#39; &#39;0.80&#39; &#39;5.54&#39; &#39;11.65&#39; &#39;69.99&#39; &#39;1&#39;] [&#39;2016&#39; &#39;1&#39; &#39;1&#39; &#39;5&#39; &#39;0&#39; &#39;2&#39; &#39;1&#39; &#39;16.29&#39; &#39;1520&#39; &#39;45.00&#39; &#39;1.30&#39; &#39;0.00&#39; &#39;8.00&#39; &#39;54.30&#39; &#39;1&#39;]] . (2014, 15) . I&#39;ll only work with a subset of the real data — approximately 90,000 yellow taxi trips to and from New York City airports between January and June 2016. This data set includes a 1/50th random sample. Below is information about selected columns from the dataset: . pickup_year: the year of the trip pickup_month: the month of the trip (January is 1, December is 12) | pickup_day: the day of the month of the trip | pickup_location_code: the airport or borough where the trip started | dropoff_location_code: the airport or borough where the trip ended | trip_distance: the distance of the trip in miles | trip_length: the length of the trip in seconds | fare_amount: the base fare of the trip, in dollars | total_amount: the total amount charged to the passenger, including all fees, tolls and tips | . Detailed information on all columns could be found here. . # Access the column by names np_taxis = np.genfromtxt(&#39;nyc_taxis.csv&#39;,delimiter=&#39;,&#39;, names= True) . References: . NumPy genfromtxt function - API Reference . Tutorial for genfromtxt . print(np_taxis.shape) print(np_taxis[10][&#39;pickup_location_code&#39;]) . z = np.array([1,2]) print(z.shape) . y = np.array([[1],[2]]) print(y.shape) . np_taxis = np.genfromtxt(&#39;nyc_taxis.csv&#39;,delimiter=&#39;,&#39;, skip_header=1) . print(np_taxis.shape) . print(np_taxis[10]) . np.set_printoptions(suppress=True) . print(np_taxis[10]) . print(np_taxis[1]) . cols = [5, 6, 9, 10] print(np_taxis[:,cols]) . Get the Frequency Table of taxis based on the Pickup Location . print(np.unique(np_taxis[:,6])) . unique, counts = np.unique(np_taxis[:,6], return_counts=True) print(unique) print(counts) . print(np_taxis[:,-2]) . # Calculate the mph of each trip trip_distance_miles = np_taxis[:,7] trip_length_seconds = np_taxis[:,8] trip_length_hours = trip_length_seconds / 3600 # 3600 seconds is one hour trip_mph = trip_distance_miles / trip_length_hours print(trip_mph) . total_amount = np_taxis[:,-2] # measures of central tendency mean = np.mean(total_amount) median = np.median(total_amount) # measures of dispersion min = np.amin(total_amount) max = np.amax(total_amount) range = np.ptp(total_amount) varience = np.var(total_amount) sd = np.std(total_amount) print(&quot;Descriptive analysis&quot;) print(&quot; n&quot;) print(&quot;Measures of Central Tendency&quot;) print(&quot;Mean =&quot;, mean) print(&quot;Median =&quot;, median) print(&quot;Measures of Dispersion&quot;) print(&quot;Minimum =&quot;, min) print(&quot;Maximum =&quot;, max) print(&quot;Range =&quot;, range) print(&quot;Varience =&quot;, varience) print(&quot;Standard Deviation =&quot;, sd) .",
            "url": "https://ylu-1258.github.io/YLu-Blog/jupyter/interests/2022/08/24/Numpy-for-Beginners.html",
            "relUrl": "/jupyter/interests/2022/08/24/Numpy-for-Beginners.html",
            "date": " • Aug 24, 2022"
        }
        
    
  
    
        ,"post69": {
            "title": "Site Organization and Configuration Lecture",
            "content": "Lecture - 08/24/2022 . Using documents as blog posts . To import past assignemnts and documents from document-based editors such as word and google docs, we can import such files as .docx documents and place them under the _word subdirectory in our blog. . NOTE: Doing so does not keep the formatting of the document, any font colors, size, styling is not preserved, additionall work with CSS is required for original effects. . Blog Front-Matter . Each markdown or jupyter post contains a set configurations at the head of the file known as Front-Matter Front-matter settings are seperated into two main groups, keys, and values. . keys: The name of the configuration or setting we wish to edit value: The value or data we grant to a specific configuration . IMPORTANT: It is crucial to always pair a key with a value, a blank value on the key overwrites the default value, making the key take on a null value and breaking the front-matter To define front matter in markdown, use the following format toc: true layout: post description: APCSP Lecture 2 categories: [markdown, notes] title: 08/24/2022 Lecture author: Alex Lu show_tags: true hide: true comments: true ... . To define front-matter in computational notebooks, use the following format # Jupyter Notebook Demonstration &gt; My first Jupyter notebook on my blog! - toc: true - title: First Jupyter Notebook - author: Alex Lu - badges: true - comments: true - categories: [jupyter] . NOTE: A title and and description must be specified with the # and &gt; characters respectively, furthermore, each front-matter key and value should be prefixed with a hyphen (-) similar to a markdown list. . Adding pages on the navbar . If we ever find the need to add a special page on the top of our site in the navbar, simply move the post into the _pages directory, and change the front matter key layout from post to page . NOTE: setting a table of contents in the front-matter does not work for a page, further tinkering with html is required. . _config.yml . Most of the blog’s default keys and values are defined within the _config.yml configuration file in the base directory of the blog. The values under _config.yml are in the standard key: value syntax prevalent in most .yml files. NOTABLE KEYS: . Key function . title | Title of site in upper left hand corner | . baseurl | The url path to the blog | . show_description | Display brief description of blog post uner blog lists | . show_image | Display image on post card | .",
            "url": "https://ylu-1258.github.io/YLu-Blog/markdown/apcsp/week%201/2022/08/24/Lecture-site-organization-and-configuration.html",
            "relUrl": "/markdown/apcsp/week%201/2022/08/24/Lecture-site-organization-and-configuration.html",
            "date": " • Aug 24, 2022"
        }
        
    
  
    
        ,"post70": {
            "title": "Python Quiz",
            "content": "Here is the code for my quiz . Colors really help to spruce things up a little, can you get 100% without looking at the code? . Example Playthrough: . import getpass class Colors: &#39;&#39;&#39; Colors class to print colored text to terminal, Does not work in fastpages :/ &#39;&#39;&#39; # Man I love ANSI PINK = &#39; 033[95m&#39; LILAC = &#39; 033[94m&#39; BLUE = &#39; 033[96m&#39; GREEN = &#39; 033[92m&#39; YELLOW = &#39; 033[93m&#39; RED = &#39; 033[91m&#39; ENDC = &#39; 033[0m&#39; BOLD = &#39; 033[1m&#39; UNDERLINE = &#39; 033[4m&#39; class Quiz(): # Main class for the quiz def __init__(self): # Quiz &quot;constructor&quot;, initiates list of questions and answers and other settings. self.ques = [] self.ans = [] self.colors = Colors() self.showans = False # Show correct answers after a wrong input self.total = 0 # Total questions self.correct = 0 # Total right self.skipped = 0 # Total skipped def addQues(self, question, answer): &#39;&#39;&#39; Adding Questions and answers to the quiz &#39;&#39;&#39; self.ques.append(question) self.ans.append(answer) self.total += 1 # increment total by 1 def askQues(self, idx): &#39;&#39;&#39; Helper function to ask one question, check answer, increment student data &#39;&#39;&#39; print(self.colors.PINK + &quot;QUESTION: &quot; + self.colors.ENDC + self.colors.BOLD + self.ques[idx] + self.colors.ENDC) # Print Question rsp = input(self.colors.YELLOW + &quot;What is your response? &quot; + self.colors.ENDC) # Get Response if rsp.lower() == self.ans[idx].lower(): # Check answer print(self.colors.GREEN + &quot;YOU ARE CORRECT!&quot; + self.colors.ENDC + &quot; Response &quot; + self.colors.BLUE + rsp + self.colors.ENDC + &quot; is correct!&quot;) self.correct += 1 elif rsp.lower() == &quot;/s&quot;: if self.showans: print(self.colors.YELLOW + &quot;Skipping....&quot; + &quot; The right answer was &quot; + self.colors.BLUE + self.ans[idx] + self.colors.ENDC) else: rint(self.colors.YELLOW + &quot;Skipping....&quot; + self.colors.ENDC) self.skipped += 1 else: if self.showans: print(self.colors.RED + &quot;Response &#39;&quot; + rsp + &quot;&#39; is incorrect.&quot; + self.colors.ENDC + &quot; The right answer was &quot; + self.colors.BLUE + self.ans[idx] + self.colors.ENDC) else: print(self.colors.RED + &quot;Response &#39;&quot; + rsp + &quot;&#39; is incorrect.&quot; + self.colors.ENDC) def percentage(self, x, y): &#39;&#39;&#39; Function to calculate percentage correct &#39;&#39;&#39; return 100 * float(x)/float(y) def playQuiz(self): &#39;&#39;&#39; Main Quiz loop, set settings and ask questions &#39;&#39;&#39; show_ans = input(self.colors.UNDERLINE + &quot;Would you like to show correct answers after incorrect responses? [y/n]&quot; + self.colors.ENDC + &quot; &quot;) print(self.colors.YELLOW + &quot;GOOD LUCK {0}!&quot;.format(getpass.getuser().upper()) + &quot; Type &#39;&quot; + self.colors.BLUE + &quot;/s&quot; + self.colors.YELLOW + &quot;&#39; to skip!&quot;) if show_ans in [&quot;y&quot;, &quot;yes&quot;, &quot;Y&quot;, &quot;YES&quot;]: # Multiple cases of user inputs, assume any other input / no input == False. self.showans = True for i in range(0,self.total): # Iterate over all questions, no repetitive code here self.askQues(i) # Print a little congratulations message print(self.colors.LILAC + &quot;Congratulations! you got &quot; + self.colors.GREEN + &#39;{0:.2f}&#39;.format(self.percentage(self.correct, self.total)) + &quot;%&quot; + self.colors.LILAC + &quot; and {0} questions skipped on this quiz!&quot;.format(self.colors.YELLOW + str(self.skipped) + self.colors.LILAC)) # Creating our quiz q1 = Quiz() # AYO CHEATER STOP LOOKIN HERE q1.addQues(&quot;Name the Python output command mentioned in this lesson?&quot;, &quot;print&quot;) q1.addQues(&quot;If you see many lines of code in order, what would College Board call it?&quot;, &quot;sequence&quot;) q1.addQues(&quot;What keyword in python is used to describe a function?&quot;, &quot;def&quot;) q1.addQues(&quot;What command is used to include other functions that were previously developed?&quot;, &quot;import&quot;) q1.addQues(&quot;What command is used to evaluate correct or incorrect response in this quiz?&quot;, &quot;if&quot;) q1.addQues(&quot;Each &#39;if&#39; command contains an &#39;_________&#39; to determine a true or false condition?&quot;, &quot;expression&quot;) q1.addQues(&quot;What is an input to a function or method called?&quot;, &quot;parameter&quot;) q1.addQues(&quot;If Input is data the computer receives, what is the data that the computer sends back?&quot;, &quot;output&quot;) q1.addQues(&quot;What is a reusable block of code called?&quot;, &quot;function&quot;) q1.addQues(&quot;What operator is used for string concatenation in Python?&quot;, &quot;+&quot;) q1.playQuiz() . GOOD LUCK ERIS29! Type &#39;/s&#39; to skip! QUESTION: Name the Python output command mentioned in this lesson? YOU ARE CORRECT! Response print is correct! QUESTION: If you see many lines of code in order, what would College Board call it? YOU ARE CORRECT! Response sequence is correct! QUESTION: What keyword in python is used to describe a function? YOU ARE CORRECT! Response def is correct! QUESTION: What command is used to include other functions that were previously developed? Response &#39;include&#39; is incorrect. The right answer was import QUESTION: What command is used to evaluate correct or incorrect response in this quiz? YOU ARE CORRECT! Response if is correct! QUESTION: Each &#39;if&#39; command contains an &#39;_________&#39; to determine a true or false condition? Skipping.... The right answer was expression QUESTION: What is an input to a function or method called? YOU ARE CORRECT! Response parameter is correct! QUESTION: If Input is data the computer receives, what is the data that the computer sends back? YOU ARE CORRECT! Response output is correct! QUESTION: What is a reusable block of code called? YOU ARE CORRECT! Response function is correct! QUESTION: What operator is used for string concatenation in Python? YOU ARE CORRECT! Response + is correct! Congratulations! you got 80.00% and 1 questions skipped on this quiz! .",
            "url": "https://ylu-1258.github.io/YLu-Blog/jupyter/apcsp/week%201/2022/08/23/alex-quiz.html",
            "relUrl": "/jupyter/apcsp/week%201/2022/08/23/alex-quiz.html",
            "date": " • Aug 23, 2022"
        }
        
    
  
    
        ,"post71": {
            "title": "Stein EC precis",
            "content": "Alex Lu Mrs. DaFoe APEL, Period 5 21 August, 2022 . The Hypocritical Truth of American Education . In his op-ed, “We’re teaching kids to follow their dreams. Maybe teach them to be helpful instead”, Joel Stein asserts that the American education system is hypocritical in its attempts to teach young Americans about stories of helping others, but never teach them the necessary skills to help others. To prove the sanctimoniousness of our education system, Stein begins with a personal anecdote from his elementary school days, a time where teachers struggled to teach their self-absorbed students about important historical figures. Joel then uses this anecdote in order to introduce his main point of criticism: The so-called “living wax museums” used by teachers today to dress up their students as heroic people allows the students to perform speeches in front of their classmates, giving them a taste of leadership through the lens of a past American hero. While this may sound good on paper, Stein provides yet another anecdote in paragraph 2, this time in the classroom of his 4th-grade son instead, wryly reminiscing the ironic atmosphere of the room一 opposite of what one would expect from an empowering speech. For example, he compares the classroom to a standup comedy show, where “each comedian has to drag in two audience members to fill the house.” Stein continues to scoff at the lunacy of the situation by recalling his son’s preposterous stories about JFK being a sailor and discovering “14 bags of Whoppahs and a Mahs bah.” Through the usage of such personal experiences, Stein develops pathos that appeals to the audience on the failures of our educational system to deliver the very values it must prioritize: knowledge, character, and education. It is essential that Stein appeals to his audience in this way so as to emotionally stir other teachers and parents in his audience to realize the fundamental issues in our schools and the lack of action to amend such ailments in society. Besides anecdotes and pathos, Stein uses exaggeration and satire to convey the ironic state of things. In paragraph 5, Stein openly reveals that each student concluded their speeches with the good qualities of their model and not with the impact or morals behind their actions. Due to this, Stein jokingly states that Malala Yousafzai, a Pakistani activist for female education, would “rescind the right of girls to go to this school” if she were ever to witness these children focusing on her “pursuing her dreams” and not her altruistic actions. Using Malala’s status as a child activist, Stein generates a satirical effect and humorously addresses the misrepresentation of these significant figures by clueless young children and the naive pedagogy constructed by the teachers who organize these events. Stein uses this same example to address how schools have drifted from teaching children to be helpful to others and instead molding them into potential leaders. Finally, he addresses such schools’ hypocrisy once again as they aim to teach students how to become good leaders but quickly turn critical if their students want to become the next “Nixon or Trump”. Through his anecdotes and satire, Stein humorously criticizes the hypocrisy of American schools while maintaining a serious attitude towards the end in an attempt to educate fellow parents and teachers on this crisis. .",
            "url": "https://ylu-1258.github.io/YLu-Blog/markdown/apel/2022/08/21/APEL-Stein-Precis.html",
            "relUrl": "/markdown/apel/2022/08/21/APEL-Stein-Precis.html",
            "date": " • Aug 21, 2022"
        }
        
    
  
    
        ,"post72": {
            "title": "First Jupyter Notebook",
            "content": "Let&#39;s Print Something! . Allow me to demonstrate the print() function in python! . var = &quot;World!&quot; print(&quot;Hello &quot; + var) . Hello World! . Let&#39;s Print some more! . Loops are really fun sometimes, let&#39;s say it a couple more times 🔁 . for i in range(5): print(&quot;Hello &quot; + var) . Hello World! Hello World! Hello World! Hello World! Hello World! . Have a Bogosort algorithm! . The real question is what time complexity this algorithm has 🤔 . from random import shuffle as sh def sorted(arr): length = len(arr) for i in range(0, length-1): if (arr[i] &gt; arr[i+1] ): return False return True def shuffle(arr): sh(arr) def Bogosort(arr): while not sorted(arr): shuffle(arr) return arr array = [23,10,49,9] print(Bogosort(array)) . [9, 10, 23, 49] .",
            "url": "https://ylu-1258.github.io/YLu-Blog/jupyter/2022/08/20/example-post.html",
            "relUrl": "/jupyter/2022/08/20/example-post.html",
            "date": " • Aug 20, 2022"
        }
        
    
  
    
        ,"post73": {
            "title": "Hello World!",
            "content": "Hello World! 🌎 . This is my first blog post! I’m Alex Lu, and I’m excited to learn more about how to build and develop my blog. . Have some code! 💻 . Ayo is that a recursive function? . def fibo(n): if n == 1: return 0 if n == 2: return 1 return fibo(n-1) + fibo(n-2) print(fibo(10)) # prints 34 . That was sick! . Thanks for visiting 🥳 . Have a cookie 🍪 .",
            "url": "https://ylu-1258.github.io/YLu-Blog/markdown/2022/08/19/hello-world.html",
            "relUrl": "/markdown/2022/08/19/hello-world.html",
            "date": " • Aug 19, 2022"
        }
        
    
  
    
        ,"post74": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://ylu-1258.github.io/YLu-Blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post75": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://ylu-1258.github.io/YLu-Blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  

  
      ,"page2": {
          "title": "Vocab",
          "content": "Vocab . The Vocab should be oriented in this fashion . Vocab | Definition | Example | . Vocab Definition Example . Pseudo-Code | A mere representation of actual code, used as a blue print of something we wish to write | DISPLAY(msg) | . Sequence | Any block of code that contains more than one singular line | #include &lt;bits/stdc++.h&gt;; using name space std; int main() { cout «“Hello World”; return 0 } | . Procedural Extraction and Data Extraction | taking little parts of code and putting them into different files and locations to be used as part of a whole | Index.html and other html fragments | .",
          "url": "https://ylu-1258.github.io/YLu-Blog/vocab/",
          "relUrl": "/vocab/",
          "date": ""
      }
      
  

  
      ,"page3": {
          "title": "Lecture Notes",
          "content": "What is this page? . This page contains all of the notes I’ve taken during Mr. Mortensen’s lectures, this might be useful while I’m trouble shooting my blog or other bugs in my code. If you’re another APCSP student, feel free to use this page to further your knowledge or review any points you missed in class! . Lectures TOC . Date Link and subject . 08 - 31 - 2022 | HTML Fragments | . 08 - 29 - 2022 | Data Abstraction in Python | . 08 - 25 - 2022 | Bash and Cloud Lecture | . 08 - 24 - 2022 | Blog posting and configuration Lecture | .",
          "url": "https://ylu-1258.github.io/YLu-Blog/notes/",
          "relUrl": "/notes/",
          "date": ""
      }
      
  

  
      ,"page4": {
          "title": "Javascript Submenu",
          "content": "| Javascript Kernel Usage | Javascript Table Generator | Coin Flip | .",
          "url": "https://ylu-1258.github.io/YLu-Blog/submenu/",
          "relUrl": "/submenu/",
          "date": ""
      }
      
  

  
      ,"page5": {
          "title": "About Me",
          "content": "Hello There! . My name is Alex Lu and if you want to reach out to me about anything, you can contact me through the methods below! . Discord: Eris29#2693 🎮 | Email: maodou1258@gmail.com 📧 | Phone: (858)-688-4567 📱 | . About Me 📓: . I am currently a Junior at Del Norte High School taking the APCSP course, I hope to major in computer science in the future and also pursue it as a prospective carreer path in the future . My Hobbies 🎾 . I have many hobbies that I do to pass my time, here are just a few of them: . Reading | Programming | Playing tennis | Playing video games | Watching youtube videos | And many more! | . My Interests 🔬: . I am interested in anything related to computers, I am currently working on various projects centered around: . Machine Learning 🤖 | Image processing 🖼️ | Webscraping 🌐 | Cyber Security 🐱‍💻 | .",
          "url": "https://ylu-1258.github.io/YLu-Blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  

  
  

  
  

  
  

  
  

  
  

  

  

  

  

  

  
  

  
      ,"page19": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://ylu-1258.github.io/YLu-Blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}