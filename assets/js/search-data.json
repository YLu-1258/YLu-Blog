{
  
    
        "post0": {
            "title": "Binary Calculator",
            "content": "Binary Math with Conversions . Plus Binary Octal Hexadecimal Decimal Minus . +1 | 00000000 | 0 | 0 | 0 | -1 | . Turn on | Turn on | Turn on | Turn on | Turn on | Turn on | Turn on | Turn on | . | | | | | | | | .",
            "url": "https://ylu-1258.github.io/YLu-Blog/markdown/apcsp/2022/11/16/XOR-calculator.html",
            "relUrl": "/markdown/apcsp/2022/11/16/XOR-calculator.html",
            "date": " • Nov 16, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "An Empirical Study On Performance Measures Of Collaborative Filtering Recommendation Algorithms",
            "content": "An Empirical Study on Performance Measures of Collaborative Filtering Recommendation Algorithms . Alex Lu . maodou1258@gmail.com . Abstract . In recent times, recommendation engines have become increasingly popular within many industries. The focus of such an engine is to implement an algorithm to successfully make recommendations based on user preferences. Because of the utility provided by such tools, many industries heavily rely on such engines to provide accurate product recommendations to customers. Some popular algorithms implemented include clustering, matrix factorization, and deep learning implementations. Such algorithms are utilized within Collaborative Filtering methods and are simulated using pre-made datasets containing user and item information. Performance metrics are then applied to the algorithm to test the accuracy of the model. Results show that deep learning algorithms provide greater . performance when compared to other applications. . Keywords - Collaborative filtering, RMSE, Deep Learning, MAE, matrix factorization, Performance Measure . 1. Introduction . Latterly, recommendation engines serve a great purpose in many online services and enhance the consumer experience by providing lists of recommended items to users. Many big corporations such as Netflix, Amazon, Youtube, and others provide item recommendations through such methods to promote the sales and usage of their goods. The general idea of a recommender system is to return a list of items that the user would find interesting. Implementing various different types of engines provides insight towards which algorithm is the most suitable for certain scenarios. . Different approaches by different algorithms could have varying performances depending on the type of recommendation required, and the core nature of the dataset used. This research is primarily conducted on collaborative-filtering techniques, but similar research could be conducted on content-based implementations. Examples of potential implementations could be clustering and matrix factorization approaches. The main accuracy metrics utilized in the research are the RMSE and MAE metrics which are further explained later in the paper. . Two main research questions would serve as the focus of the research. More conclusions could be drawn from the data collected, but the main aim of the research are as follows: . I) What CF implementation provides the accuracy measure for a standard data set containing user, item and rating information? II) How does data sparsity affect the prediction accuracy of the models implemented? . 2. Background . Recommendation engines are programs used to provide item recommendations to users based on filtering each item and returning a . possible predicted rating for the user. The two main implementations of recommendation engines are content-based filtering and collaborative filtering. . 2.1 Content-based Filtering . A content-based filtering engine takes into account the user’s own item history and focuses on keywords in items rather than similarity between users [1]. However, such implementation can lead to a scenario where providing a broad range of recommendations would become impossible. Because of the nature of content-based filtering engines, items that were utilized by similar users wouldn’t be recommended to the main user purely because of the lack of a keyword or phrase. A dataset for such an engine would incorporate items along with a detailed profile for each item. . Despite its narrower scope when providing recommendations, content-based filtering systems often reduce the amount of data needed to make accurate predictions. For collaborative filtering, a greater range of data is required for a proper calculation of an item. To put it simply, the more data the engine has, the more . accurate the prediction is. However, with the nature of content-based filtering, any amount of data would suffice, providing that there are items that match the recommendation requirements. . 2.2 Collaborative Filtering . A collaborative filtering engine takes into account other user’s ratings, and returns . algorithms. This paper would primarily focus on model-based algorithms and approaches. The similarity between users could be found in various methods. For this study, a total of twelve algorithms were implemented. The main focus was placed on nearest neighbors, matrix factorization, and deep learning algorithms. Figure 2.1 maps out the various . . recommendations based on similarity. Collaborative filtering is split into two main approaches, model-based and memory based . algorithms implemented in this research. 2.3 Nearest Neighbor . Nearest neighbor, or more specifically, k nearest neighbors is a collaborative filtering . algorithm used to find similarities between items and users based on the total distance between two neighbors [2]. A weight system could also be applied so that a neighbor closer to the main user would have more weight on the recommendations than one who is further away. . Four of the twelve algorithms used in the research were from this category. KNNBasic, KNNWithZScore, KNNWithMeans, and KNNBaseline were such implementations of this category of algorithms. . 2.4 Matrix Factorization . Matrix Factorization is yet again another implementation of collaborative filtering. Simply put, a matrix factorization relates two separate values together under a specific value to create a grid or matrix of the data [3]. However, as data is not evenly distributed, some cells in the matrix are left empty and would need to be filled in to provide recommendations. Such values are known as “latent features”. . Some matrix factorizations implemented in the research were Negative Matrix Factorization, Singular Value Decomposition, and SVD++. . 2.5 Deep Learning . A deep learning model utilizes a neural network to process and calculate information. Much like a human brain, deep learning attempts to make predictions based on data much like a human brain. . The deep learning model implemented in this experiment utilizes an embedding layer structure [4]. Using such a structure could organize data into a vector of discrete values which could then produce similarity results and tests through the distance between vectors. These types of embedding layers could be generated through frameworks such as “Pytorch” or “Tensorflow”, however, this research would be implemented in Pytorch. The model could then be trained through a series of epochs and eventually provide predictions. . 2.6 Miscellaneous surprise algorithms . The miscellaneous category comprises the leftover algorithms in the surprise package that do not fit into any of the other categories in the research. Such algorithms were as listed: Co . Clustering, Normal Predictor, Baseline-Only, and Slope-One. . According to Sahar Karat Co-Clustering is a collaborative filtering algorithm used to provide recommendations by “A simultaneous clustering of the rows and columns of a matrix” [5]. Classical clustering algorithms only focus on one specific type of data, while co . clustering could be used to accommodate two simultaneously. . Normal predictor is an algorithm provided by the surprise package that provides random user recommendations based on the data distribution in the dataset. The “Maximum Likelihood Estimation” method is utilized in this calculation. . Baseline-Only is yet another surprise algorithm that makes baseline predictions based on the data provided. There are two main ways to implement this algorithm. The first implementation method is “Stochastic Gradient . Descent” (SGD), which calculates a gradient of the dataset by a random selection of data. The other implementation is to use “Alternating Least Squares” (ALS) which is a matrix factorization algorithm that works well with sparser sets of data. . Slope-One is an example of an item-based collaborative filtering recommendation algorithm. Predictions made with this model are generally based on personal ratings as well as similar community ratings. . Further information and implementations on the methods listed could be found on the surprise documentations [6]. . 2.7 Accuracy Metrics . Two accuracy metrics were used in the process of this research. Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE) [11]. . RMSE is a quadratic model that assigns much larger weights to larger errors observed. Accuracy is calculated through RMSE by taking the square root of the mean of the sum of the squared difference of each predicted . value and its corresponding actual value. The formula is displayed in Figure 2.2 . . (Figure 2.2) . MAE is a linear metric model that linearly scales error. This model is typically used when higher errors are not important to the observation. Accuracy is calculated through MAE by taking the mean of the sum of the absolute values of the differences between the predicted and actual values. The formula is displayed in Figure 2.3 . . (Figure 2.3) . 3. Related Work . Various research topics have already been analyzed in the field of collaborative filtering. A few similar pieces of research provide similar methods and implementations. A plethora of CF models has been implemented and researched. . 3.1 Similar Research . Similar research was conducted by Mojdeh Saadati et al. [7] on the implementations of various models on movie recommender systems. The two models used in the experiment were the matrix factorization model SVD and a deep learning implementation of the Restricted Boltzmann Machine. The performance of the implementations was measured in the RMSE metric. However, in this research, we go over a broader spectrum of algorithms from each group of implementations and produce results for a larger picture between the different implementations. . 3.2 Further Research . He et al. [8] Conducted Research on how Matrix Factorization could be implemented with Deep Learning to create a better performing model for collaborative filtering. The proposed idea was to use two main models, the Generalized Matrix Factorization (GMF), and the Multi-Layer Perceptron (MLP) to create a hybrid between deep learning and matrix factorization implementations to create a new neural matrix factorization model . dubbed “NeuMF”. NeuMf was then compared to other well-known models such as KNN and ALS implementations and proved to have better accuracy and less training loss. . Yedder et al. [9] Researched the performance of the restricted Boltzmann machine. Various hidden units, learning rates, and other factors were incorporated into the research. Problems such as data sparsity were also encountered in the research and required other methods of implementation. This research also utilized the RMSE accuracy metric to rate the performance of the model. The research indicated an excellent RMSE measure of 0.46. . 4. Approach . This study was broken up into three parts, cleaning, implementation, and evaluation. We will first check for invalid values throughout the dataset, then begin to implement algorithms, and finally calculate performance values. . 4.1 Dataset . The data that would be primarily used in this study is the Kaggle dataset “Anime . Recommendations Database” [10] based on the data collected from myanimelist.net and is included in the reference section of the study. The two files contained within the database are the “ratings.csv” and “anime.csv”. The research would mostly work with the ratings file, but the anime dataset could be further implemented in future research incorporating variables such as show genre and overall ratings. The anime dataset is separated into three separate columns. The user_id, anime_id, and ratings columns are provided for analyzing similarities between users. The format and example of the file are illustrated in Figure 4.1. . (Figure 4.1) . The python module “pandas” was used to import the data into a data frame which could . then be cleaned and analyzed. Making a copy of the dataset to work on is recommended. Maintaining two different data frames would help compare the results on the original to the results in the cleaned dataset. . The first thing to take note of while cleaning is to remove all rows with a rating of “-1”. Such rows represent user data that does not have a specified rating for the specific show that they watched. This step is crucial as having such data in the project would result in a sparse dataset, which directly decreases performance as shown later in the results. A total of 7.8 million rows of data were in the dataset, after cleaning, about 6.3 million rows remained. The distribution of this data is shown in figure 4.2. . . (Figure 4.2) . Another issue arose again while trying to apply the dataset to our engine. Certain cells in . our dataset possibly had ‘NaN’ values, which would in turn cause an error in actual predictions where a value of ‘NaN’ would be returned instead of an integer of the predicted rating. To resolve this issue, we first applied the ‘to_numeric’ method from the pandas module to convert each value into workable integer or float values. After, we could then finally lower the runtime of our experiment by decreasing the size of our dataset from 6 million down to 125,000. This number was picked for easier splitting while implementing other algorithms and deep learning which require testing and training datasets. From there, we then calculated the distribution of the ratings by counting the number of each rating in our dataset. After clearing, it’s paramount to re index the rows in the data frame. Removing the invalid A distribution of the ratings is demonstrated in Figure 4.3. . . (Figure 4.3) . A correlation heatmap could also be used to detect any possible correlations between the values given, however, as of now, there is no apparent relation. . . (Figure 4.4) . 4.2 Implementation . The python package scikit-surprise contains most of the algorithms implemented in this research. First and foremost, we must declare a rating scale for our dataset by utilizing the reader class, and also initializing our dataset with the Dataset class. We set a new . “ratings“ object over the range of one to ten, which is represented in our data. Using the three columns from our pandas data frame, we could then load the data into surprise using the Dataset.load_from_df() method. . After the data is loaded, we could then specify a “param_list” dictionary. In the dictionary, the name of the specific parameters would be stored as the keys and the possible values as a list in the values. Further explanation of what specific params do for each algorithm is also provided in the surprise documentation. . The param_list dictionary could then be imputed into the GridSearchCV() method along with the algorithm name, performance measures, and a cross-validation iterator of 3. There are other arguments as well, but for the sake of our research, these four should be enough. The final step is to then fit our premade data with our GridSearchCV method with the .fit() function. Once everything is set up, the accuracy metrics could then just be retrieved via the best_score iterator. . The deep learning algorithm specifically was generated using an embedding layer with . dropout layers. The entire network consists of 4 total layers. After setting up the net, we can begin to train our model and loop over varying amounts of epochs to find the accuracy that is desired. . To determine the RMSE and the MAE values of this approach, we could separate our dataset into predictions and truth arrays. We can then apply the formulas for both RMSE and MAE as shown in Figures 2.2 and 2.3. We can implement these metrics in two ways. The first method is to use a NumPy array to subject each vector of values from each other and then to apply the formula to our newly generated values. Our second approach is more basic and rudimentary. We could subtract each truth value from each prediction value in our function by declaring a function and then apply the formula to our sum. . 5. Results . In this research, two sets of results were collected. The data collected on the cleaned dataset would serve to be our solution to the first question. However, the data collected on the original dataset would be utilized to answer . the second question concerning the performance measures on sparse datasets. 5.1 Cleaned Dataset . We have gathered both RMSE and MAE values for each algorithm used on our cleaned dataset from our implementations. In the first part of this experiment, the data set has already been cleaned of any invalid values and has reduced sparsity. The majority of the algorithms implemented all showed similar results except a couple of outliers and certain points. A table of the data collected is shown in Figure 5.1 . . (Figure 5.1) . Using the RMSE as the x-axis and MAE for the y-axis, we can plot a scatter plot of our data (Figure 5.2). . . (Figure 5.2) . Removing the Normal Predictor outlier value provides us with a clearer image of the differences in the lower valued points (Figure 5.3). . . (Figure 5.3) . Our data shows that the worst-performing algorithm that we had implemented was the Normal Predictor algorithm from the surprise package, while the best-performing implementation was the Deep Learning algorithm based on neural networks. . 5.1 Pre-cleaned Dataset . Loading up a new dataset, we can effectively run all our algorithms again while maintaining the sparsity of the original dataset. The only cleaning that had to be done was to remove all NaN values and also convert all data to numeric values. The results of running the . implementations on the sparser dataset are recorded in the figure below (Figure 4.5) . . (Figure 5.4) . The results of the various algorithms on this sparse dataset were also plotted on a scatter plot as shown below in Figure 5.5. . . (Figure 5.5) . Once again, removing the normal predictor outlier gives a clearer representation of our other data points (Figure 5.6). . . (Figure 5.6) . Although the values have increased by a considerable amount in the sparse dataset, the common trend remains between the various data points, and no changes are observed in the best and worst algorithms. Comparing the . results of the cleaned and original data sets, a clear difference could be observed in the RMSE and MAE measures (Figures 5.7-8) . (Figure 5.7) . (Figure 5.8) . 6. Discussion . After analyzing the data collected from the research, the deep learning algorithm and the KNNBaseline implementations were observed to be the best performing with the least error observed with both RMSE and MAE metrics. . A possible reason for the results could be the usage of randomness in the calculation of the Normal Predictor algorithm. The deep learning algorithm may have performed the best because of the various training cycles allocated to it which helped to create a more accurate model after each iteration. Vice versa, the opposite could also be applied to the two worst performing algorithms KNNBasic and Normal Predictor. Such implementations . had basic calculations and weren’t able to take into account outliers and other potential biases in the data. . There was an attempt in the research to implement a Restricted Boltzmann Machine (RBM) model, however, the implementation gave varying results and was difficult to judge the extra ratings of the implementation. This . research could be further pursued in the future with the addition of more deep learning implementations and a narrower focus on the subject of deep learning as a whole. From the results acquired, deep learning has been shown to have improved results compared to other algorithms. Focused research on deep learning implementations would provide the reasoning behind deep learning accuracy. . 7. Conclusion . In this research, we explored various machine learning algorithms, K-Nearest neighbors, Matrix Factorization, Deep learning, etc. Several approaches were implemented from the categories mentioned then tested for accuracy measures. . In both the cleaned and original datasets, the deep learning implementation was shown to the least margin of error when making recommendations. From this, it could be deduced that deep learning is a viable method for collaborative filtering engines working with user, item and rating data. Although different sets of data have varying optimal algorithms, . deep learning was still shown to be extremely accurate compared to other tested algorithms. Analyzing the data collected from the sparse dataset, we can conclude that a sparser set of data would result in less accurate recommendations, sometimes up to double the margin of error observed. Because of this observation, it can be concluded that collaborative filtering best performs with dense datasets. . References . [1] Kirzhner, Elena. “Machine Learning. Explanation of Collaborative . Filtering vs Content Based . Filtering.” Medium, Codeburst, 11 . May 2018, codeburst.io/explanation of-recommender-systems-in . information-retrieval-13077e1d916c. [2] Harrison, Onel. “Machine Learning Basics with the K-Nearest Neighbors Algorithm.” Medium, Towards Data Science, 14 July 2019, . towardsdatascience.com/machine . learning-basics-with-the-k-nearest . neighbors-algorithm-6a6e71d01761. . [3] Chen, Denise. “Recommendation System - Matrix Factorization.” . Medium, Towards Data Science, 9 . July 2020, . towardsdatascience.com/recommend ation-system-matrix-factorization . d61978660b4b. . [4] Sivanantham, Balavivek. . “Recommendation System Implementation With Deep Learning and PyTorch.” Medium, The Startup, 18 Aug. 2020, . medium.com/swlh/recommendation system-implementation-with-deep learning-and-pytorch-a03ee84a96f4. . [5] Karat, Sahar. “Co . Clustering.” Data Science . Made Simpler, 5 Mar. 2016, datasciencemadesimpler.wor dpress.com/tag/co . clustering/. . [6] Hug, Nicholas. “Welcome to Surprise’ Documentation.” Welcome to Surprise’ . Documentation! - Surprise 1 Documentation, 2015, . surprise.readthedocs.io/en/sta ble/. . [7] Mojdeh Saadati, Syed Shihab, Mohammed Shaiqur Rahman “Movie Recommender Systems: Implementation and Performance . Evaluation.” Semantic Scholar, 2019, . www.semanticscholar.org/paper/Mo vie-Recommender-Systems%3A Implementation-and-Saadati Shihab/01470f39285213e53f365ce0 1417b18d12467563#citing-papers. . [8] Xiangnan, He, et al. “Neural Collaborative Filtering.” . International World Wide Web Conference Committee, 3 Apr. 2017. . [9] Yedder, Hanene Ben, et al. “Modeling Prediction in . Recommender Systems Using Restricted Boltzmann Machine.” . IEEE Explore, IEEE, 5 Oct. 2017, ieeexplore.ieee.org/abstract/documen t/8122923. . [10] CooperUnion. “Anime . Recommendations Database.” . Kaggle, Kaggle, 21 Dec. 2016, . www.kaggle.com/CooperUnion/ani me-recommendations-database. . [11] Kampakis, Stylianos. “Performance Measures: RMSE and MAE.” The Data Scientist, The Data Scientist, 26 Nov. 2020, . thedatascientist.com/performance measures-rmse-mae/. .",
            "url": "https://ylu-1258.github.io/YLu-Blog/2022/11/16/An-Empirical-Study-on-Performance-Measures-of-Collaborative-Filtering-Recommendation-Algorithms.html",
            "relUrl": "/2022/11/16/An-Empirical-Study-on-Performance-Measures-of-Collaborative-Filtering-Recommendation-Algorithms.html",
            "date": " • Nov 16, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Alex Lu   Lab #1_ Measurements And Graphical Analysis",
            "content": "Lab #1: Measurements and Graphical Analysis . Alex Lu . Purpose: . Given disks of different radii, determine the relationship between the mass and the radius of the disks through graphical method and calculate the uncertainty associated with the measured value. We will learn about linearization and use it to create a mathematical model. . Materials and Equipment: . Balance . Meter stick . Circular disks (identical thickness and uniform density but different radii) . Graphic Calculator or online graphing tool . Procedure: . Substitute equations to get a relationship between mass and radius. . | Use the relationship between mass and radius to determine what variable should be processed to linearize the data . | Measure the radius (in cm) of the cylinder with the ruler with one end of the ruler at the center of the disk and the other on the edge. . | Zero out the balance and then measure the mass (in grams) of the cylinder. . | Stack the metal cylinders and measure the collective height of all cylinders. Divide by the total number of cylinders multiplied by 2 to the power of the number of folds to find the height of each individual cylinder. . | Organize the collected data into a data table . | Plot the original radius and mass . | Plot the processed radius and mass . | Plug height and other constants to create a relationship between mass and radius. . | Equations and calculations: . Let: . ( rho) be density . | (m) be mass . | (v) be volume . | (a) be the surface area of the cylinder . | (r) be the radii of the cylinder . | (h) be the height of the cylinder . | . Thus: . If ( rho = frac{m}{v}) and (v = ah) then . (m = rho v) . (m = rho text{ah}) . If (a = pi r^{2}), then . (m = rho pi r^{2}h) . Since both ( rho) and ( pi) are constants and (h) is negligible, ∴ (m propto r^{2}) . Since mass is directly proportional to radius squared, the graph could be linearized if (r^{2}) is plotted instead. . Precision and Uncertainty . The balanced used to measure the mass of the metal disks was accurate up to a hundredth of a gram, while the ruler used to measure the radius and height of the disks was accurate up to 1 millimeter (0.1 cm). As such, the radius was measured to the nearest hundredth of a centimeter or a tenth of a millimeter. . | Data Tables . m = mass . r = radii . H = height . Mass in Grams, Radius and height in cm, and Radius Squared of Each Metal Disk . Disk m (g) h (cm) r (cm)   r^2 (cm^2) . 1 | 0.07 | 0.0025 | 2.30 |   | 5.29 | . 2 | 0.14 | 0.0025 | 3.00 |   | 9.0 | . 3 | 0.25 | 0.0025 | 4.30 |   | 18.49 | . 4 | 0.37 | 0.0025 | 4.90 |   | 24.01 | . 5 | 0.73 | 0.0025 | 7.10 |   | 50.41 | . Graph of Non-Linearized Data: . . Equation: (y = 0.0133921x^{2} + 0.112825x - 0.0238674) . R2 : (0.9967) . This graph has non-linear data as it’s represented by a quadratic model. . Graph of Linearized Data: . . Line of the best fit equation: (y = 0.0145569x - 0.000100527) . r = (0.9983) . r2 = (0.9966 ) . This graph has linear data as it’s represented by a linear model. . Analysis Questions: . 1) What is the independent variable in your y = mx +b formula? . Considering the mathematic relationship between disk radius r and disk mass m, the independent variable from my line of best fit represents the radius of the disk squared. . 2) What does the slope represent in your y = mx +b formula? Show dimensionally that indeed that is what your slope represents and that the formula is valid dimensionally. . Using the (m = rho v) equation and the volume relationship, we can express the disk’s mass in terms of the radius and the height in this relationship: (m = rho pi r^{2}h). Since density (( rho)) and ( pi) are constants, and height ((h)) is negligible, we can essentially group these 3 constants into one value serving as the coefficient to the radius squared term. Thus, the slope is the product of the density, height, and ( pi). Since height has its units expressed in cm, density has units expressed in g/cm³, and ( pi) is a constant wth no units, the slope is a combination of all 3 constants and has units of g/cm2. . Because our slope has units of g/cm2, and x represents the radius squared expressed by cm2, and since our desired output ((y)) is mass in grams (g), the y-intercept ((b)) must have a unit of grams (g) in order for both sides of the equation to have the same units (grams). . (g = frac{g}{cm^{2}} cdot cm^{2} + g) . (g = g + g) . (g = g) . 3) Should the “b” in your y = mx + b formula be zero? Explain your answer. . The y-intercept ((b)) in my (y = mx + b) formula shouldn’t be zero unless the data naturally generates a line of best fit that passes through the origin. Forcing the y-intercept to pass through the origin would either alter the slope ((m)) or remove the y-intercept completely. This would result in an inaccurate interpretation of the model as the line is no longer set evenly between the data points. Thus we cannot guarantee the model’s integrity for future data points, rendering our model useless. . 4) Measure/estimate the “thickness” of your cylinders. Use that value to find the experimental density of your cylinders. Find a percent difference between your found density and the actual density. The actual material is aluminum. . (y = 0.0145569x - 0.000100527) . (slope = pi rho h) . ( rho = density) . ( rho = frac{ text{slope}}{h pi} = frac{0.0145569 frac{g}{cm^{2}}}{ pi(0.0025cm)} = 1.85 frac{g}{cm^{3}}) . The density of the metal disks according to my linear model is (1.85 frac{g}{cm^{3}}) , Since the metal used in the disk is Aluminum (density = (2.70 frac{g}{cm^{3}})) we can use the following calculations to determine our percent error . ( % Error = left | frac{Actual - Expected}{ text{Expected}} right | *100 % ) | . ( % Error = left | frac{1.85 frac{g}{cm^{3}} - 2.70 frac{g}{cm^{3}}}{2.70 frac{g}{cm^{3}}} right | *100 % = 31.5 % ) | . ∴ Percent difference = 31.5% . 5) Errors. Make sure you explain why your number is bigger or smaller than (if positive or negative difference.) . The number that I obtained is smaller than the actual value (31.5% error) because the instruments that we used to measure the disks’ dimensions had inaccuracies. The ruler that we used to measure the disks’ radius is only precise up to millimeters, and the balanced used to measure mass is only accurate up to the hundredth’s place, while actual instruments used to measure the actual value are much more precise. Additionally, the method I used to measure the height of the disks also had inherent inaccuracies, as folding the aluminum disks over each other may create tiny air pockets that add additional magnitude to my measured height, resulting in an inaccurate height measurement. The uncertainty of the ruler and also the inaccurate way of measuring height may have contributed to a greater height value than the actual value, resulting in a lower density value when I divided. Lastly, the disks weren’t in a perfectly circular shape, thus the direction that I measured the disk in might also have affected the end result, as measuring in different directions would produce different radii. . Synthesis Questions: . 1) In this experiment, if we had used disks with a greater thickness, would the slope of your best fit line have been different? Would your experimental value for density be the same? Explain. . Since slope represents the product of thickness, ( pi), and density, having a greater measured thickness would result in a much larger slope value. However, our experimental value of density would still be around the same, as an increase in thickness would also result in a proportionally large increase in mass, which will offset the thickness increase. . 2. How would your graph of m versus r2 be different if you had used disks of the same . thickness but made out of steel? Draw a second line on your m versus r2 plot that . represents disks made of steel. . The graph would be of m versus r2 would be different in that the slope of the graph would be much greater, this is because steel has a much higher density of (7.85 frac{g}{cm^{3}}) compared to aluminum’s (2.7 frac{g}{cm^{3}}) . . (Note: The blue line is a rough sketch of what the line of best fit would look like on the same scale as the aluminum graph is steel was used instead. This is not an accurate representation, just an approximation) . 3. Another group of students has acquired data for the exact same experiment; however, their disks are made of an unknown material that they are trying to determine. The group’s m versus r2 data produced a line of best fit with slope equal to 122 kg/m2. Each disk they measured had the same 0.5 cm thickness. Calculate the density of the unknown material and use the table below to help determine what material their disks are made of. . Work: . (Slope = 122kg/m^{2} , h = 0.5cm, slope = rho pi h) . (slope = 122 frac{ text{kg}}{m^{2}}( frac{1000g}{1kg})( frac{1m}{100cm})^{2} = 12.2 frac{g}{cm^{2}} = rho pi h) . ( rho = frac{ text{slope}}{ pi h} = frac{12.2 frac{g}{cm^{2}}}{ pi*0.5cm} = 7.77 frac{g}{cm^{3}}) . The closet material to a density of (7.77 frac{g}{cm^{3}}) is iron, which has a density of (7.8 frac{g}{cm^{3}}). The disks are most likely made out of iron. . Multiple Choice Questions: . 1. You perform the same experiment, but this time you plot a linear relationship between mass and the circumference of the disks rather than the radius. What is the slope of the linear plot? . Work: . (slope = rho pi h) . (m = rho v = rho pi r^{2}h) . (v = pi r^{2}h) . Let (c) be circumference . (c = 2 pi r) . (c^{2} = 4 pi^{2}r^{2}) . (r^{2} = frac{c^{2}}{4 pi^{2}}) . (m = rho pi h( frac{c^{2}}{4 pi^{2}}) = frac{ rho hc^{2}}{4 pi} = ( frac{ rho h}{4 pi})c^{2}) . () . Since ( rho) (density), h (height), and (4 pi) are all constants, these 3 values could be combined to form the slope for the linear plot, which is (( frac{ text{ph}}{4 pi})), Hence, E is the correct answer choice. . 2. Skipped . 3. Consider an experiment in which a student measures the mass and diameter of 10 . different-sized spheres, all made of the same material of uniform density ρ. For this . student to create a linear graph relating the mass of the sphere to its radius r, the . student would need to plot mass m versus which quantity: . ( text{let v be the volume of a sphere}) . (v = frac{4}{3} pi r^{3}) . (m = pv = ( frac{4p pi}{3})r^{3}) . Since (( frac{4p pi}{3})) is a constant, the student must plot a linear graph relating m versus the quality of radius cubed. Hence, option C is correct. .",
            "url": "https://ylu-1258.github.io/YLu-Blog/2022/11/16/Alex-Lu-Lab-1_-Measurements-and-Graphical-Analysis.html",
            "relUrl": "/2022/11/16/Alex-Lu-Lab-1_-Measurements-and-Graphical-Analysis.html",
            "date": " • Nov 16, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Binary Math",
            "content": "Binary Math with Conversions . Plus Binary Octal Hexadecimal Decimal Character Minus . +1 | 00000000 | 0 | 0 | 0 | NULL | -1 | . Turn on | Turn on | Turn on | Turn on | Turn on | Turn on | Turn on | Turn on | . | | | | | | | | . Plus Binary Hexadecimal RGB Minus . +1 | 00000000 | 0 | 0 | -1 | . +1 | 00000000 | 0 | 0 | -1 | . +1 | 00000000 | 0 | 0 | -1 | . Turn on | Turn on | Turn on | Turn on | Turn on | Turn on | Turn on | Turn on | . | | | | | | | | . Turn on | Turn on | Turn on | Turn on | Turn on | Turn on | Turn on | Turn on | . | | | | | | | | . Turn on | Turn on | Turn on | Turn on | Turn on | Turn on | Turn on | Turn on | . | | | | | | | | .",
            "url": "https://ylu-1258.github.io/YLu-Blog/markdown/apcsp/2022/11/15/Binary.html",
            "relUrl": "/markdown/apcsp/2022/11/15/Binary.html",
            "date": " • Nov 15, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "APCSP Final MC Analysis",
            "content": "Final MC results . Overall, the final was pretty easy and smooth. However, I think that there are definitely areas of improvement for me to not only increase my acccuracy but also my efficiency at these problems. . Incorrect Answers and Writeup . Writeup: Originally, I answered that the only strings of characters could be represented by digital binary sequences (i.e. the ASCII text encoding), and neglected the possibility of storing colors or audio recordings as binary. . However, after consulting some online resources and review the collegeboard 2.1 videos, I realized that almost everything in our world, weather it be abstract ideas or tangible objects, could be represented and abstracted into simple binary sequences. For instance, color may be encoded as a hex value or RGB code as we’ve experimented in our CSS style sheets for our project. In other cases, audio recordings may also be stored as binary sequence. For example, we can take the sound waves of a recording and simplify it into a numerical sequence. We could then parse that sequence through a computer, by transforming this numerical wave into sequences of binary bits instead. In fact, this is how most Machine Learning algorithms process sound for speech recognition and voice commands. . Overall conclusion and analysis . Overall, I feel like I have gained a strong grasp of the contents taught in this trimester about error checking, programming collaboration, computer networking, and etc. However I do think I could review some other topics on collegeboard such as data abstraction (binary representation), and some other topics to truly master my computer science knowledge and skills for trimester 2 of this course. .",
            "url": "https://ylu-1258.github.io/YLu-Blog/markdown/apcsp/2022/11/08/Final-MC.html",
            "relUrl": "/markdown/apcsp/2022/11/08/Final-MC.html",
            "date": " • Nov 8, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "APCSP N@tM",
            "content": "Goals . The Night at the Museum was an event coordinated by the visual arts and computer science departments to create an opportunity for students to observe and learn from the projects of others while benefiting from their gains. The projects displayed by the Tri 1 APCSP classes utilized two web applications with one designated for Frontend display and one for Backend processing. The two endpoints were linked up via an API developed on the backend, which allows for communication between backend and frontend and ultimately interactive webpages. . Our Project . We created a minesweeper game that boasts an ability that allows for individual users to log in into an account that they can create on the home page. This system is created by our backend API which stores users and their credentials and information in a 2 dimensional python dictionary. Ultimately, this allows for our user to play the minesweeper game while allowing them to freely store their high scores so long as the website is up. . Others . Alan, Steven, Ederick, Noor, Liav - Created a calendar that displays date and weather information. Allows users to input and store events on any date and incorporated city-wide weather data via an API. . Luke - Created a Quiz program that compiles random questions from a collection of geography, math and SAT prompts. Has a well-designed about page that links to everyone’s fastpage blogs. . Paaras, Samarth, AJ, Haoxuan - Developed an api that grabs some motivational qoutes from the internet and allows users to vote on their favorite quotes through number counter. . Peacock Justin, James, Shruti, Joslyn- Created a tetris game and used an astronomy api to create a table based on city names to provide location details such as longitude latitude and moon traits. . Advay, Krishiv, Shivansh, Dhruva, Prasith - Another motivational quotes generator with added motivational features for fitness and atheletic goals. . Jonathan, Martin, Abdulla, Leonard - Created a program to return basketball stats of NBA teams, and used an API to get the stats for each team. . Lyntax Aniket, Soham, Ryan, Lucas - One api with a dictionary hosted on a flask server that has words and definitions so users can imput it against a word. Which serves to play a hangman game. Which also eliminates letters and dynamically renders the charector. . Ananya, Sreeja, Aliya, Clair - Created a Wordle game that uses a custom api and randomly generates a game. Also checks if words are valid are not and informs the user if they are incorrect or not. . Raunak, Tanay, Yuri, Sachit, Harsha - Created a clock alarm and stopwatch, using a custom api to request data from a variety of time of timezones to display times. . Lily, Ekam, Ishi, Shreyas - Uses a custom dictionary api of different words and their definitions to allow users to find them and a word of the day that changes. Along with a feedback to adapt the user experience. . Aiden, ahad, dash, sabine - Innovative api that has games such as tetris, blackjack, cookie clicker, and a pokemon game with custom to log losses wins, correct answers wrong and other game data. . Keira, Zeen, Ellie, Giannina - Made a custom custmoer survey along with a quiz that has three different quizzes for calculus, physics, and satstics, with solutions. Display is randomized with a backend database along with a quiz summary and a customer service api that prompts users with questions to save them. . Safin, Navan, Alex, Kalani - Made a motivational quotes website that allows users to like and dislike posts and also giving them the ability to comment on each individual quote .",
            "url": "https://ylu-1258.github.io/YLu-Blog/markdown/apcsp/2022/11/07/N@tM.html",
            "relUrl": "/markdown/apcsp/2022/11/07/N@tM.html",
            "date": " • Nov 7, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "Python Web API Endpoints using Jokes",
            "content": "Model for API . We will begin our journey into APIs by creating and thinking about data. We have learned about Python Lists and dictionaries. In this data example, we are going to make &quot;the best computer jokes ever ;)&quot; and serve them over the Internet. The ultimate objective is to allow our viewers to provide a like or dislike on each of our jokes. . This code planning begins by coming up with some jokes and defining a data &quot;model&quot; to keep and manage the jokes. . jokes_data contains a list of dictionary records containing joke and reactions:haha or boohoo - joke_list contains collection of jokes we will put into jokes_data | . | Next comes some functions to interact with our jokes . def initJokes(): initializes jokes_data | def getJokes(): returns the complete list of jokes | def getJoke(): returns a single joke from our list | ... many more function can be examined by reading comments below ... | . | . import random jokes_data = [] joke_list = [ &quot;If you give someone a program... you will frustrate them for a day; if you teach them how to program... you will &quot; &quot;frustrate them for a lifetime.&quot;, &quot;Q: Why did I divide sin by tan? A: Just cos.&quot;, &quot;UNIX is basically a simple operating system... but you have to be a genius to understand the simplicity.&quot;, &quot;Enter any 11-digit prime number to continue.&quot;, &quot;If at first you don&#39;t succeed; call it version 1.0.&quot;, &quot;Java programmers are some of the most materialistic people I know, very object-oriented&quot;, &quot;The oldest computer can be traced back to Adam and Eve. It was an apple but with extremely limited memory. Just &quot; &quot;1 byte. And then everything crashed.&quot;, &quot;Q: Why did Wi-Fi and the computer get married? A: Because they had a connection&quot;, &quot;Bill Gates teaches a kindergarten class to count to ten. 1, 2, 3, 3.1, 95, 98, ME, 2000, XP, Vista, 7, 8, 10.&quot;, &quot;Q: What’s a aliens favorite computer key? A: the space bar!&quot;, &quot;There are 10 types of people in the world: those who understand binary, and those who don’t.&quot;, &quot;If it wasn&#39;t for C, we’d all be programming in BASI and OBOL.&quot;, &quot;Computers make very fast, very accurate mistakes.&quot;, &quot;Q: Why is it that programmers always confuse Halloween with Christmas? A: Because 31 OCT = 25 DEC.&quot;, &quot;Q: How many programmers does it take to change a light bulb? A: None. It’s a hardware problem.&quot;, &quot;The programmer got stuck in the shower because the instructions on the shampoo bottle said: Lather, Rinse, Repeat.&quot;, &quot;Q: What is the biggest lie in the entire universe? A: I have read and agree to the Terms and Conditions.&quot;, &#39;An SQL statement walks into a bar and sees two tables. It approaches, and asks may I join you?&#39; ] # Initialize jokes def initJokes(): # setup jokes into a dictionary with id, joke, haha, boohoo item_id = 0 for item in joke_list: jokes_data.append({&quot;id&quot;: item_id, &quot;joke&quot;: item, &quot;haha&quot;: 0, &quot;boohoo&quot;: 0}) item_id += 1 # prime some haha responses for i in range(200): id = getRandomJoke()[&#39;id&#39;] addJokeHaHa(id) # prime some haha responses for i in range(50): id = getRandomJoke()[&#39;id&#39;] addJokeBooHoo(id) # Return all jokes from jokes_data def getJokes(): return(jokes_data) # Joke getter def getJoke(id): return(jokes_data[id]) # Return random joke from jokes_data def getRandomJoke(): return(random.choice(jokes_data)) # Liked joke def favoriteJoke(): best = 0 bestID = -1 for joke in getJokes(): if joke[&#39;haha&#39;] &gt; best: best = joke[&#39;haha&#39;] bestID = joke[&#39;id&#39;] return jokes_data[bestID] # Jeered joke def jeeredJoke(): worst = 0 worstID = -1 for joke in getJokes(): if joke[&#39;boohoo&#39;] &gt; worst: worst = joke[&#39;boohoo&#39;] worstID = joke[&#39;id&#39;] return jokes_data[worstID] # Add to haha for requested id def addJokeHaHa(id): jokes_data[id][&#39;haha&#39;] = jokes_data[id][&#39;haha&#39;] + 1 return jokes_data[id][&#39;haha&#39;] # Add to boohoo for requested id def addJokeBooHoo(id): jokes_data[id][&#39;boohoo&#39;] = jokes_data[id][&#39;boohoo&#39;] + 1 return jokes_data[id][&#39;boohoo&#39;] # Pretty Print joke def printJoke(joke): print(joke[&#39;id&#39;], joke[&#39;joke&#39;], &quot; n&quot;, &quot;haha:&quot;, joke[&#39;haha&#39;], &quot; n&quot;, &quot;boohoo:&quot;, joke[&#39;boohoo&#39;], &quot; n&quot;) # Number of jokes def countJokes(): return len(jokes_data) # Test Joke Model if __name__ == &quot;__main__&quot;: initJokes() # initialize jokes # Most likes and most jeered best = favoriteJoke() print(&quot;Most liked&quot;, best[&#39;haha&#39;]) printJoke(best) worst = jeeredJoke() print(&quot;Most jeered&quot;, worst[&#39;boohoo&#39;]) printJoke(worst) # Random joke print(&quot;Random joke&quot;) printJoke(getRandomJoke()) # Count of Jokes print(&quot;Jokes Count: &quot; + str(countJokes())) . Backend Interface for Web API (Control) . An application programming interface (API) is the medium by which different systems of software interact. In our applications we have two big systems:1. Python Backend that stores data beyond a single Web page2. GH Pages/Fastpages Frontend that is responsible for presenting data . To communicate data between Frontend and Backend, this section Backend code provides and interface to the Frontend using a Web Service Endpoint. Examples of endpoints are listed below and can be typed within a browser, which will return JSON data: . https://flask.nighthawkcodingsociety.com/api/jokes | https://flask.nighthawkcodingsociety.com/api/jokes/2 | https://flask.nighthawkcodingsociety.com/api/jokes/random | . As you can see, these Endpoints return JSON. They are NOT that readable by normal humans. However, they are very effective in passing requested data across the Internet. The Frontend code is responsible for formatting and presenting and interface that allows the typical computer user to interact with this data. . The next cell of code is Creating Endpoints that return JSON. This allows developers in the Frontend to interact with Backend data. API is a contract between the Frontend and Backend on how to share data. . FYI, there is NO output from this section . . from flask import Blueprint, jsonify # jsonify creates an endpoint response object from flask_restful import Api, Resource # used for REST API building import requests # used for testing import random # Blueprints allow this code to be procedurally abstracted from main.py, meaning code is not all in one place app_api = Blueprint(&#39;api&#39;, __name__, url_prefix=&#39;/api/jokes&#39;) # endpoint prefix avoid redundantly typing /api/jokes over and over # API generator https://flask-restful.readthedocs.io/en/latest/api.html#id1 api = Api(app_api) class JokesAPI: # not implemented, this would be where we would allow creation of a new Joke class _Create(Resource): def post(self, joke): pass # getJokes() class _Read(Resource): def get(self): return jsonify(getJokes()) # getJoke(id) class _ReadID(Resource): def get(self, id): return jsonify(getJoke(id)) # getRandomJoke() class _ReadRandom(Resource): def get(self): return jsonify(getRandomJoke()) # getRandomJoke() class _ReadCount(Resource): def get(self): count = countJokes() countMsg = {&#39;count&#39;: count} return jsonify(countMsg) # put method: addJokeHaHa class _UpdateLike(Resource): def put(self, id): addJokeHaHa(id) return jsonify(getJoke(id)) # put method: addJokeBooHoo class _UpdateJeer(Resource): def put(self, id): addJokeBooHoo(id) return jsonify(getJoke(id)) # building RESTapi interfaces, these routes are added to Web Server http://&lt;server&lt;/api/jokes api.add_resource(_Create, &#39;/create/&lt;string:joke&gt;&#39;) api.add_resource(_Read, &#39;/&#39;) # default, which returns all jokes api.add_resource(_ReadID, &#39;/&lt;int:id&gt;&#39;) api.add_resource(_ReadRandom, &#39;/random&#39;) api.add_resource(_ReadCount, &#39;/count&#39;) api.add_resource(_UpdateLike, &#39;/like/&lt;int:id&gt;/&#39;) api.add_resource(_UpdateJeer, &#39;/jeer/&lt;int:id&gt;/&#39;) . Frontend (View Simulation) and Hacks . This python codes tests endpoints on a server. This can be handy for development and testing when making modifications to the Jokes Web APIs. This code works off of the server endpoint/url, not from code cells above it in this notebook. . To work with this code and make observation for learning... . Run a local server from flask_portfolio project and the change server variable to be local | Observe the requests endpoints and the output, see if you can observe what is happening/changing on put requests | The &quot;requests&quot; are captured into a List, the List is used in the for loop to extract from RESTful API format. | Try running this with Debugging and observe what data is being created at each step (Required) | Try to format this data in Python print statements to be more readable (Required) | Start and stop local server and observe errors | . # server = &quot;http://127.0.0.1:5000/&quot; # run local server = &#39;https://flask.nighthawkcodingsociety.com/&#39; # run from web server url = server + &quot;api/jokes/&quot; responses = [] # responses list # Get the count of jokes on server count_response = requests.get(url+&quot;count&quot;) count_json = count_response.json() count = count_json[&#39;count&#39;] # Update likes/dislikes test sequence using random joke num = str(random.randint(0, count-1)) # test a random record responses.append( requests.get(url+num) # Get/read joke by id ) responses.append( requests.put(url+&quot;like/&quot;+num) # Put/add to like count ) responses.append( requests.put(url+&quot;jeer/&quot;+num) # Put/add to jeer count ) # Get a random joke responses.append( requests.get(url+&quot;random&quot;) # Get/read a random joke ) # Cycle through and print responses for response in responses: print(response) try: print(response.json()) except: print(&quot;data error&quot;) .",
            "url": "https://ylu-1258.github.io/YLu-Blog/techtalk/webapi",
            "relUrl": "/techtalk/webapi",
            "date": " • Oct 17, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "Javascript Web Page using an API",
            "content": "The code below holds the info that is being generated into an HTML table. . Key things to know: . &lt; table &gt; creates a TABLE | &lt; tr &gt; creates a ROW | &lt; th &gt; makes the text a column HEADER | &lt; tbody id = &quot;results&quot; &gt; defines an element id, to be used within JavaScript | . &lt;!-- HTML table fragment for page --&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;Joke&lt;/th&gt; &lt;th&gt;HaHa&lt;/th&gt; &lt;th&gt;Boohoo&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody id=&quot;result&quot;&gt; &lt;!-- javascript generated data --&gt; &lt;/tbody&gt; &lt;/table&gt; . Constant variables are declared here with keyword const . Key things to know: . The document object &quot;result&quot; represents table body in the HTML above. | If you want to access any element in an HTML page in JavaScript, you always start by accessing the document object. In this case, we are accessing &quot;result&quot; and defining a &quot;resultContainer&quot; | In the code, in following cells, document elements are created and organized for each Joke, each is added to the &quot;resultContainer&quot; as a row in the table body. | Accessing the api is done using the variables url and options, this is setup to fetch the Jokes from the backend | . // prepare HTML defined &quot;result&quot; container for new output const resultContainer = document.getElementById(&quot;result&quot;); // keys for joke reactions const HAHA = &quot;haha&quot;; const BOOHOO = &quot;boohoo&quot;; // prepare fetch urls const url = &quot;https://flask.nighthawkcodingsociety.com/api/jokes&quot;; const like_url = url + &quot;/like/&quot;; // haha reaction const jeer_url = url + &quot;/jeer/&quot;; // boohoo reaction // prepare fetch GET options const options = { method: &#39;GET&#39;, // *GET, POST, PUT, DELETE, etc. mode: &#39;cors&#39;, // no-cors, *cors, same-origin cache: &#39;default&#39;, // *default, no-cache, reload, force-cache, only-if-cached credentials: &#39;omit&#39;, // include, *same-origin, omit headers: { &#39;Content-Type&#39;: &#39;application/json&#39; // &#39;Content-Type&#39;: &#39;application/x-www-form-urlencoded&#39;, }, }; // prepare fetch PUT options, clones with JS Spread Operator (...) const put_options = {...options, method: &#39;PUT&#39;}; // clones and replaces method . The below code uses a function called fetch to gather the data from the backend. . Key things to understand: . The &quot;url&quot; &quot;response&quot; is checked in case the site is down and returns an error | On successful fetch, the code places each Joke in the HTML table body using a &quot;for&quot; loop and creating document elements from each &quot;row&quot; of the fetched &quot;data&quot;. | The creation of each Haha and Boohoo &quot;onclick&quot; &quot;button&quot; is also done in the same loop. | Updates to backend are setup to occur with each onclick, each click calls the &quot;reaction&quot; function | . // fetch the API fetch(url, options) // response is a RESTful &quot;promise&quot; on any successful fetch .then(response =&gt; { // check for response errors if (response.status !== 200) { error(&#39;GET API response failure: &#39; + response.status); return; } // valid response will have JSON data response.json().then(data =&gt; { console.log(data); for (const row of data) { // make &quot;tr element&quot; for each &quot;row of data&quot; const tr = document.createElement(&quot;tr&quot;); // td for joke cell const joke = document.createElement(&quot;td&quot;); joke.innerHTML = row.id + &quot;. &quot; + row.joke; // add fetched data to innerHTML // td for haha cell with onclick actions const haha = document.createElement(&quot;td&quot;); const haha_but = document.createElement(&#39;button&#39;); haha_but.id = HAHA+row.id // establishes a HAHA JS id for cell haha_but.innerHTML = row.haha; // add fetched &quot;haha count&quot; to innerHTML haha_but.onclick = function () { // onclick function call with &quot;like parameters&quot; reaction(HAHA, like_url+row.id, haha_but.id); }; haha.appendChild(haha_but); // add &quot;haha button&quot; to haha cell // td for boohoo cell with onclick actions const boohoo = document.createElement(&quot;td&quot;); const boohoo_but = document.createElement(&#39;button&#39;); boohoo_but.id = BOOHOO+row.id // establishes a BOOHOO JS id for cell boohoo_but.innerHTML = row.boohoo; // add fetched &quot;boohoo count&quot; to innerHTML boohoo_but.onclick = function () { // onclick function call with &quot;jeer parameters&quot; reaction(BOOHOO, jeer_url+row.id, boohoo_but.id); }; boohoo.appendChild(boohoo_but); // add &quot;boohoo button&quot; to boohoo cell // this builds ALL td&#39;s (cells) into tr (row) element tr.appendChild(joke); tr.appendChild(haha); tr.appendChild(boohoo); // this adds all the tr (row) work above to the HTML &quot;result&quot; container resultContainer.appendChild(tr); } }) }) // catch fetch errors (ie Nginx ACCESS to server blocked) .catch(err =&gt; { error(err + &quot; &quot; + url); }); . The below code uses fetch to update backend data using &quot;put_options&quot;. The purpose is to update Hahaa and Bohoo counters in backend. . Key things to understand: . The &quot;url&quot; &quot;response&quot; is checked to verify update occurred | The element id of button clicked is updated with the data returned from the API. | Note, the elemID is received as parameter. This data was setup when the button was created in former cell. | . // Reaction function to likes or jeers user actions function reaction(type, put_url, elemID) { // fetch the API fetch(put_url, put_options) // response is a RESTful &quot;promise&quot; on any successful fetch .then(response =&gt; { // check for response errors if (response.status !== 200) { error(&quot;PUT API response failure: &quot; + response.status) return; // api failure } // valid response will have JSON data response.json().then(data =&gt; { console.log(data); // Likes or Jeers updated/incremented if (type === HAHA) // like data element document.getElementById(elemID).innerHTML = data.haha; // fetched haha data assigned to haha Document Object Model (DOM) else if (type === BOOHOO) // jeer data element document.getElementById(elemID).innerHTML = data.boohoo; // fetched boohoo data assigned to boohoo Document Object Model (DOM) else error(&quot;unknown type: &quot; + type); // should never occur }) }) // catch fetch errors (ie Nginx ACCESS to server blocked) .catch(err =&gt; { error(err + &quot; &quot; + put_url); }); } // Something went wrong with actions or responses function error(err) { // log as Error in console console.error(err); // append error to resultContainer const tr = document.createElement(&quot;tr&quot;); const td = document.createElement(&quot;td&quot;); td.innerHTML = err; tr.appendChild(td); resultContainer.appendChild(tr); } . Hacks . The code below relates to the rapidapi you worked with last week. . What are some similarities you see with the javascript for the jokes api? | In a blogpost, break up the code in cells like done above and try to describe what this code is doing. | . &lt;!-- HTML table fragment for page --&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;Time&lt;/th&gt; &lt;th&gt;All-time Cases&lt;/th&gt; &lt;th&gt;Recorded Deaths&lt;/th&gt; &lt;th&gt;Active Cases&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;td id=&quot;time&quot;&gt;&lt;/td&gt; &lt;td id=&quot;total_cases&quot;&gt;&lt;/td&gt; &lt;td id=&quot;total_deaths&quot;&gt;&lt;/td&gt; &lt;td id=&quot;active_cases&quot;&gt;&lt;/td&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;Country&lt;/th&gt; &lt;th&gt;All-time Cases&lt;/th&gt; &lt;th&gt;Recorded Deaths&lt;/th&gt; &lt;th&gt;Active Cases&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody id=&quot;result&quot;&gt; &lt;!-- generated rows --&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;!-- Script is layed out in a sequence (no function) and will execute when page is loaded --&gt; &lt;script&gt; // prepare HTML result container for new output const resultContainer = document.getElementById(&quot;result&quot;); // prepare fetch options const url = &quot;https://flask.nighthawkcodingsociety.com/api/covid/&quot;; const headers = { method: &#39;GET&#39;, // *GET, POST, PUT, DELETE, etc. mode: &#39;cors&#39;, // no-cors, *cors, same-origin cache: &#39;default&#39;, // *default, no-cache, reload, force-cache, only-if-cached credentials: &#39;omit&#39;, // include, *same-origin, omit headers: { &#39;Content-Type&#39;: &#39;application/json&#39; // &#39;Content-Type&#39;: &#39;application/x-www-form-urlencoded&#39;, }, }; // fetch the API fetch(url, headers) // response is a RESTful &quot;promise&quot; on any successful fetch .then(response =&gt; { // check for response errors if (response.status !== 200) { const errorMsg = &#39;Database response error: &#39; + response.status; console.log(errorMsg); const tr = document.createElement(&quot;tr&quot;); const td = document.createElement(&quot;td&quot;); td.innerHTML = errorMsg; tr.appendChild(td); resultContainer.appendChild(tr); return; } // valid response will have json data response.json().then(data =&gt; { console.log(data); console.log(data.world_total) // World Data document.getElementById(&quot;time&quot;).innerHTML = data.world_total.statistic_taken_at; document.getElementById(&quot;total_cases&quot;).innerHTML = data.world_total.total_cases; document.getElementById(&quot;total_deaths&quot;).innerHTML = data.world_total.total_deaths; document.getElementById(&quot;active_cases&quot;).innerHTML = data.world_total.active_cases; // Country data for (const row of data.countries_stat) { console.log(row); // tr for each row const tr = document.createElement(&quot;tr&quot;); // td for each column const name = document.createElement(&quot;td&quot;); const cases = document.createElement(&quot;td&quot;); const deaths = document.createElement(&quot;td&quot;); const active = document.createElement(&quot;td&quot;); // data is specific to the API name.innerHTML = row.country_name; cases.innerHTML = row.cases; deaths.innerHTML = row.deaths; active.innerHTML = row.active_cases; // this builds td&#39;s into tr tr.appendChild(name); tr.appendChild(cases); tr.appendChild(deaths); tr.appendChild(active); // add HTML to container resultContainer.appendChild(tr); } }) }) // catch fetch errors (ie ACCESS to server blocked) .catch(err =&gt; { console.error(err); const tr = document.createElement(&quot;tr&quot;); const td = document.createElement(&quot;td&quot;); td.innerHTML = err; tr.appendChild(td); resultContainer.appendChild(tr); }); &lt;/script&gt; .",
            "url": "https://ylu-1258.github.io/YLu-Blog/techtalk/webfrontend",
            "relUrl": "/techtalk/webfrontend",
            "date": " • Oct 17, 2022"
        }
        
    
  
    
        ,"post8": {
            "title": "6 Collegeboard Criterias",
            "content": "Collegeboard’s 6 Criterias . What are we aiming to accomplish? . Program Purpose &amp; Function . An online single-player minesweeper game that can be played anywhere, anytime | Users could choose to store data such as high scores (minimum number of moves or time) if they wish. | . Data Abstraction . Using the sqlite3 library on python to store user names and each user’s corresponding information, such as high-score | Using python lists to store the state of the game, what nodes or cells are revealed, flagged, hidden, etc. | Creating a main class for the game and then establishing a constructor that initializes other object attributes for the main game to access or manipulate. | . Managing Complexity . Maintaining user data in a organized manner with software like databases (SQL). | Using loops to reduce repetitive code / allow for a dynamic number of times for the execution of our code. | . Procedural Abstraction . Establishing a set of enumerated types to link certain values with constants. Used to established the different states of a node or the particular status of that cell. | . | Using a main class to cover the Minesweeper Game: The class will contain a constructor, attributes, and methods to ensure smooth gameplay. | This will simplify the programming process too as each individual method under the class will “inherit” the attributes of the class. This expands the scope of our functions sort of “links” them together. (Methods work in harmony with each other) | . | Creating templates for SQL queries, JSON queries, and also html pages to reduce the amount of manual programing required | . Algorithm Implementation . Maintain a clear-board at the begining of the game. On the first input (click) from the user, start randomly generating N=(M//3) number of mines, where M is the total number of mines. Use the random module to generate a set of integers within the bounds of the array, generate two integers from each mine to indicate the row or coloumn coordinate. Finally, add both generated values into a tuple which will then be appended to a list containing all of the mine coordinates. | AS OF 10/10 Consider using the RapidAPI minesweeper API to generate a board. (However this limits our flexibility as the first mine clicked might be an actual mine, leading to an inevitable loss). | . | Use an overall gameloop that checks the status of the game on each “turn of the game” (iteration of the game). If gameOver == true, signify that the game is over Use control flow statements such as if-else-elif to determine wether or not a user has clicked on a mine. If yes, set gameOver to true, if not, continue with the current iteration of the loop. | Based on the current state of the board, if there are no mines left, return a win screen. If there are mines left, assume that the player clicked on a mine and return a lose screen. | . | . | Within each iteration, prompt the user to input a coordinate point (When testing, use terminal input with standard (x,y) input, for the final product, clicking each cell should return a coordinate automatically to the python backend). Hard part is detecting a nearby “safe grid” for each mine clicked, while subsequently revealing all the mines in the safe grid | We have decided to use a recursive approach to resolve this problem. The recursive approach will store a list of safe mines. It will first append the mine clicked by the user (assuming that the mine passes the safe check). Following this, the algorithm will then check the (r+1,c), (r+1,c-1), (r+1,c+1), (r,c-1), (r,c+1), (r-1,c), (r-1,c-1), and (r-1,c+1) coordinates to determine wether or not surrouding cells are safe or not. Recursively execute the same process for each the 8 mines adjacent to the initial cell and stop until an “edge-mine” is reached. | . | Communiate user input and program output via JSON queries between the python backend and the HTML/CSS/JS frontend. | . Testing Code . For testing our code, we will primarily experiment with our code either in linux for python programs, or in the browser for frontend testing. | We will make use of print() and Console.log() statements to help with debugging. | Certain events in python could be made easier if we used try/except exceptions to catch errors. | .",
            "url": "https://ylu-1258.github.io/YLu-Blog/markdown/apcsp/week%207/2022/10/09/Final-Project-Criterias.html",
            "relUrl": "/markdown/apcsp/week%207/2022/10/09/Final-Project-Criterias.html",
            "date": " • Oct 9, 2022"
        }
        
    
  
    
        ,"post9": {
            "title": "API Usage",
            "content": "| API Demonstration | . . What is an API? . An API (Application Programming Interface) is used as a intermediary bridge between two software applications that stardizes protocol and conventions. In simple words an API could help us to send and receive data. . import requests # Set number of rows to 10 r=10 # Set number of columns to 10 c=10 # Set number of mines to r*c/4 b = (r*c)//4 url = &quot;https://minesweeper1.p.rapidapi.com/boards/new&quot; querystring = {&quot;r&quot;:r,&quot;c&quot;:c,&quot;bombs&quot;:b} headers = { &quot;X-RapidAPI-Key&quot;: &quot;9443d1725fmsha0cfacd48534f3cp1802cfjsn326b60d92795&quot;, &quot;X-RapidAPI-Host&quot;: &quot;minesweeper1.p.rapidapi.com&quot; } response = requests.request(&quot;GET&quot;, url, headers=headers, params=querystring) board = response.json().get(&quot;board&quot;) for row in board: temp = [&quot;M&quot; if k == &quot;-1&quot; else k for k in [str(i) for i in row]] print(&quot; &quot;.join(temp)) #print(board) . 1 1 2 M M 1 1 M M 2 3 M 3 2 2 1 1 3 M 2 M M 3 0 0 1 1 3 2 2 M M 2 0 0 1 M 3 M 3 3 4 3 1 0 1 2 5 M M 2 M M 3 1 0 1 M M 3 2 M M M 1 0 1 2 2 1 1 2 3 3 2 1 0 0 1 1 0 0 0 2 M 2 1 1 2 M 0 0 0 2 M 2 1 M 2 1 . import requests url = &quot;https://stock-and-options-trading-data-provider.p.rapidapi.com/options/aapl&quot; headers = { &quot;X-RapidAPI-Proxy-Secret&quot;: &quot;a755b180-f5a9-11e9-9f69-7bf51e845926&quot;, &quot;X-RapidAPI-Key&quot;: &quot;9443d1725fmsha0cfacd48534f3cp1802cfjsn326b60d92795&quot;, &quot;X-RapidAPI-Host&quot;: &quot;stock-and-options-trading-data-provider.p.rapidapi.com&quot; } response = requests.request(&quot;GET&quot;, url, headers=headers) json = response.json() print(response.text) print(json[&quot;stock&quot;]) . def print_stock(json): stock_info = json[&quot;stock&quot;] blacklist = [&quot;longBusinessSummary&quot;, &quot;coinMarketCapLink&quot;, &quot;ebitda&quot;, &quot;maxAge&quot;, &quot;ebitdaMargins&quot;, &quot;logo_url&quot;, &quot;strikePrice&quot;, &quot;volumeAllCurrencies&quot;, &quot;startDate&quot;, &quot;circulatingSupply&quot;, &quot;navPrice&quot;] print(&quot;Current Stock Information for Ticker: &quot;, stock_info[&quot;symbol&quot;]) for key, value in stock_info.items(): if key not in blacklist: print(key + &quot;: &quot; + str(value)) print_stock(json) . Current Stock Information for Ticker: AAPL zip: 95014 sector: Technology fullTimeEmployees: 154000 city: Cupertino phone: 408 996 1010 state: CA country: United States companyOfficers: [] website: https://www.apple.com address1: One Apple Park Way industry: Consumer Electronics profitMargins: 0.25709 grossMargins: 0.43313998 operatingCashflow: 118224003072 revenueGrowth: 0.019 operatingMargins: 0.30533 targetLowPrice: 130 recommendationKey: buy grossProfits: 152836000000 freeCashflow: 83344621568 targetMedianPrice: 185 currentPrice: 140.42 earningsGrowth: -0.077 currentRatio: 0.865 returnOnAssets: 0.22204 numberOfAnalystOpinions: 41 targetMeanPrice: 183.5 debtToEquity: 205.984 returnOnEquity: 1.62816 targetHighPrice: 220 totalCash: 48230998016 totalDebt: 119691001856 totalRevenue: 387541991424 totalCashPerShare: 3.001 financialCurrency: USD revenuePerShare: 23.732 quickRatio: 0.697 recommendationMean: 1.9 exchange: NMS shortName: Apple Inc. longName: Apple Inc. exchangeTimezoneName: America/New_York exchangeTimezoneShortName: EDT isEsgPopulated: False gmtOffSetMilliseconds: -14400000 quoteType: EQUITY symbol: AAPL messageBoardId: finmb_24937 market: us_market annualHoldingsTurnover: None enterpriseToRevenue: 5.994 beta3Year: None enterpriseToEbitda: 17.929 52WeekChange: -0.019046307 morningStarRiskRating: None forwardEps: 6.46 revenueQuarterlyGrowth: None sharesOutstanding: 16070800384 fundInceptionDate: None annualReportExpenseRatio: None totalAssets: None bookValue: 3.61 sharesShort: 116076947 sharesPercentSharesOut: 0.0072000003 fundFamily: None lastFiscalYearEnd: 1632528000 heldPercentInstitutions: 0.59582 netIncomeToCommon: 99632996352 trailingEps: 6.05 lastDividendValue: 0.23 SandP52WeekChange: -0.16544336 priceToBook: 38.897507 heldPercentInsiders: 0.00071000005 nextFiscalYearEnd: 1695600000 yield: None mostRecentQuarter: 1656115200 shortRatio: 1.53 sharesShortPreviousMonthDate: 1660521600 floatShares: 16053877710 beta: 1.249815 enterpriseValue: 2322811584512 priceHint: 2 threeYearAverageReturn: None lastSplitDate: 1598832000 lastSplitFactor: 4:1 legalType: None lastDividendDate: 1659657600 morningStarOverallRating: None earningsQuarterlyGrowth: -0.106 priceToSalesTrailing12Months: 5.823012 dateShortInterest: 1663200000 pegRatio: 2.42 ytdReturn: None forwardPE: 21.736841 lastCapGain: None shortPercentOfFloat: 0.0072000003 sharesShortPriorMonth: 115400891 impliedSharesOutstanding: 0 category: None fiveYearAverageReturn: None previousClose: 140.09 regularMarketOpen: 140.42 twoHundredDayAverage: 159.1368 trailingAnnualDividendYield: 0.006353059 payoutRatio: 0.1471 volume24Hr: None regularMarketDayHigh: 141.89 averageDailyVolume10Day: 101319120 regularMarketPreviousClose: 140.09 fiftyDayAverage: 158.688 trailingAnnualDividendRate: 0.89 open: 140.42 toCurrency: None averageVolume10days: 101319120 expireDate: None algorithm: None dividendRate: 0.92 exDividendDate: 1659657600 regularMarketDayLow: 138.5729 currency: USD trailingPE: 23.209917 regularMarketVolume: 73717951 lastMarket: None maxSupply: None openInterest: None marketCap: 2256661643264 averageVolume: 80128214 dayLow: 138.5729 ask: 141.03 askSize: 1000 volume: 73717951 fiftyTwoWeekHigh: 182.94 fromCurrency: None fiveYearAvgDividendYield: 1.02 fiftyTwoWeekLow: 129.04 bid: 141.03 tradeable: False dividendYield: 0.0066000004 bidSize: 1800 dayHigh: 141.89 regularMarketPrice: 140.42 lastUpdated: 10/10/2022 .",
            "url": "https://ylu-1258.github.io/YLu-Blog/jupyter/apcsp/week%207/2022/10/07/Minesweeper-Stocks-api.html",
            "relUrl": "/jupyter/apcsp/week%207/2022/10/07/Minesweeper-Stocks-api.html",
            "date": " • Oct 7, 2022"
        }
        
    
  
    
        ,"post10": {
            "title": "Python RapidAPI",
            "content": "Python, RapidAPI Terms . APIs and tooling like Jupyter docs allows many opportunities in fields like Data Science. As more and more developers use APIs, they build standards in how you setup a client, send requests and receive information... . Covid19 RapidAPI Example . To begin the API journey. You need to find an API provider. . RapidAPI is a great option. You must setup and account, but there are many free options. | Goto this page for starters, the Corona virus World and India data- Under Code Snippets pick Python - Requests | . RapidAPI, you will select Python Requests type of code to work with you Notebook. . The url is the endpoint to which the API is directed | The headers is a dictionary data structure to send special messaging to the endpoint | The requests.request() python function is used to send a request and retrieve their responses | The response variable receives result of of the request in JSON text | . Next step, is to format the response according to your data science needs . &quot;&quot;&quot; Requests is a HTTP library for the Python programming language. The goal of the project is to make HTTP requests simpler and more human-friendly. &quot;&quot;&quot; import requests &quot;&quot;&quot; RapidAPI is the world&#39;s largest API Marketplace. Developers use Rapid API to discover and connect to thousands of APIs. &quot;&quot;&quot; url = &quot;https://corona-virus-world-and-india-data.p.rapidapi.com/api&quot; headers = { &#39;x-rapidapi-key&#39;: &quot;9443d1725fmsha0cfacd48534f3cp1802cfjsn326b60d92795&quot;, &#39;x-rapidapi-host&#39;: &quot;corona-virus-world-and-india-data.p.rapidapi.com&quot; } # Request Covid Data response = requests.request(&quot;GET&quot;, url, headers=headers) # print(response.text) # uncomment this line to see raw data # This code looks for &quot;world data&quot; print(&quot;World Totals&quot;) world = response.json().get(&#39;world_total&#39;) # turn response to json() so we can extract &quot;world_total&quot; for key, value in world.items(): # this finds key, value pairs in country print(key, value) print() # This code looks for USA in &quot;countries_stats&quot; print(&quot;Country Totals&quot;) countries = response.json().get(&#39;countries_stat&#39;) for country in countries: # countries is a list if country[&quot;country_name&quot;] == &quot;USA&quot;: # this filters for USA for key, value in country.items(): # this finds key, value pairs in country print(key, value) . World Totals total_cases 509,268,964 new_cases 204,268 total_deaths 6,242,509 new_deaths 630 total_recovered 461,827,849 active_cases 41,198,606 serious_critical 42,510 total_cases_per_1m_population 65,334 deaths_per_1m_population 800.9 statistic_taken_at 2022-04-24 11:18:01 Country Totals country_name USA cases 82,649,779 deaths 1,018,316 region total_recovered 80,434,925 new_deaths 0 new_cases 0 serious_critical 1,465 active_cases 1,196,538 total_cases_per_1m_population 247,080 deaths_per_1m_population 3,044 total_tests 1,000,275,726 tests_per_1m_population 2,990,303 . Digital Coin Example . This example provides digital coin feedback (ie Bitcoin). It include popularity, price, symbols, etc. . A valid X-RapidAPI-Key is required. Look in code for link to RapidAPI page | Read all comments in code for further guidance | . # RapidAPI page https://rapidapi.com/Coinranking/api/coinranking1/ # Begin Rapid API Code import requests url = &quot;https://coinranking1.p.rapidapi.com/coins&quot; querystring = {&quot;referenceCurrencyUuid&quot;:&quot;yhjMzLPhuIDl&quot;,&quot;timePeriod&quot;:&quot;24h&quot;,&quot;tiers[0]&quot;:&quot;1&quot;,&quot;orderBy&quot;:&quot;marketCap&quot;,&quot;orderDirection&quot;:&quot;desc&quot;,&quot;limit&quot;:&quot;50&quot;,&quot;offset&quot;:&quot;0&quot;} headers = { &quot;X-RapidAPI-Key&quot;: &quot;9443d1725fmsha0cfacd48534f3cp1802cfjsn326b60d92795&quot;, # place your key here &quot;X-RapidAPI-Host&quot;: &quot;coinranking1.p.rapidapi.com&quot; } response = requests.request(&quot;GET&quot;, url, headers=headers, params=querystring) print(response.text) # End Rapid API Code json = response.json() # convert response to python json object # Observe data from an API. This is how data transports over the internet in a &quot;JSON&quot; text form # - The JSON &quot;text&quot; is formed in dictionary {} and list [] divisions # - To read the result, Data Scientist of Developer converts JSON into human readable form # - Review the first line, look for the keys -- &quot;status&quot; and &quot;data&quot; . Formatting Digital Coin example . JSON text transferred from the API in the previous cell was converted to a Python Dictionary called json. The &quot;coins&quot; in the dictionary contain a list of the most relevant data. Look at the code and comments to see how the original text is turned into something understandable. Additionally, there are error check to make sure we are starting the code with the expectation that the API was run correctly. . &quot;&quot;&quot; This cell is dependent on valid run of API above. - try and except code is making sure &quot;json&quot; was properly run above - inside second try is code that is used to process Coin API data Note. Run this cell repeatedly to format data without re-activating API &quot;&quot;&quot; try: print(&quot;JSON data is Python type: &quot; + str(type(json))) try: # Extracting Coins JSON status, if the API worked status = json.get(&#39;status&#39;) print(&quot;API status: &quot; + status) print() # Extracting Coins JSON data, data about the coins data = json.get(&#39;data&#39;) # Procedural abstraction of Print code for coins def print_coin(c): print(c[&quot;symbol&quot;], c[&quot;price&quot;]) print(&quot;Icon Url: &quot; + c[&quot;iconUrl&quot;]) print(&quot;Rank Url: &quot; + c[&quot;coinrankingUrl&quot;]) # Coins data was observed to be a list for coin in data[&#39;coins&#39;]: print_coin(coin) print() except: print(&quot;Did you insert a valid key in X-RapidAPI-Key of API cell above?&quot;) print(json) except: print(&quot;This cell is dependent on running API call in cell above!&quot;) . This cell is dependent on running API call in cell above! . Go deeper into APIs . Web Development vs Jupyter Notebook. A notebook is certainly a great place to start. But, for your end of Trimester project we want you to build the skill to reference and use APIs within your Project. Here are some resources to get you started with this journey. . In the Nighthawk Coders APCSP you can find an Overview and Examples using APIs:APCSP APIs menu- Using Covid RapidAPI JavaScript frontend API code in APCSP Fastpages GitHub repo: https://github.com/nighthawkcoders/APCSP/blob/master/_posts/2022-07-10-PBL-rapidapi.md | . | Making a Jokes API (this will next API tech talk) Frontend. JavaScript frontend code in APCSP fastpages GitHub repo: https://github.com/nighthawkcoders/APCSP/blob/master/_posts/2022-07-10-PBL-jokes.md | Backend Endpoints. Python code that allows Frontend access: https://github.com/nighthawkcoders/flask_portfolio/blob/main/api.py | Backend Jokes Management. Python code that support Create, Read, Update, Delete (CRUD): https://github.com/nighthawkcoders/flask_portfolio/blob/main/model_jokes.py | . | . Hacks . Find and use an API as part of your project. An API and a little coding logic will be a big step toward getting meaningful data for a project. There are many API providers, find one that might work for your project to complete this hack. When picking an API you are looking for something that will work with either JavaScript Fetch or Python Request. Here are some samples, these are not qualified in any way. . RapidAPI- GitHub Project | No Key APIs Article | Twitter Developer | Google Developer | Reddit Developer | . Show API and format results in either Web Page or Jupyter Notebook. Ultimately, I will expect that we do APIs in backend (Python/Flask). However, for this Hack you can pick your preference. We will discuss pros and cons in next API tech talk. .",
            "url": "https://ylu-1258.github.io/YLu-Blog/week%207/jupyter/apcsp/2022/10/03/PBL-python_rapidapi.html",
            "relUrl": "/week%207/jupyter/apcsp/2022/10/03/PBL-python_rapidapi.html",
            "date": " • Oct 3, 2022"
        }
        
    
  
    
        ,"post11": {
            "title": "1.4 Identifying and Correcting Errors",
            "content": "College Board Big Idea 1 . Identifying and Correcting Errors (Unit 1.4) . Become familiar with types of errors and strategies to fixing them . Lightly Review Videos and take notes on topics with Blog | Complete assigned MCQ questions | . Here are some code segments you can practice fixing: . alphabet = &quot;abcdefghijklmnopqrstuvwxyz&quot; alphabetList = [] for i in alphabet: alphabetList.append(i) print(alphabetList) . The intended outcome is to determine where the letter is in the alphabet using a while loop . What is a good test case to check the current outcome? Why? | Make changes to get the intended outcome. | . letter = input(&quot;What letter would you like to check?&quot;) i = 0 while i &lt; 26: if alphabetList[i] == letter: print(&quot;The letter &quot; + letter + &quot; is the &quot; + str(i+1) + &quot; letter in the alphabet&quot;) i += 1 . The intended outcome is to determine where the letter is in the alphabet using a for loop . What is a good test case to check the current outcome? Why? | Make changes to get the intended outcome. | . letter = input(&quot;What letter would you like to check?&quot;) count = 1 for i in alphabetList: if i == letter: print(&quot;The letter &quot; + letter + &quot; is the &quot; + str(count) + &quot; letter in the alphabet&quot;) count += 1 . This code outputs the even numbers from 0 - 10 using a while loop. . Analyze this code to determine what can be changed to get the outcome to be odd numbers. (Code block below) | . evens = [] i = 0 while i &lt;= 10: evens.append(i) i += 2 print(evens) . This code should output the odd numbers from 0 - 10 using a while loop. . odds = [] i = 1 while i &lt;= 10: odds.append(i) i += 2 print(odds) . This code outputs the even numbers from 0 - 10 using a for loop. . Analyze this code to determine what can be changed to get the outcome to be odd numbers. (Code block below) | . numbers = [0,1,2,3,4,5,6,7,8,9,10] evens = [] for i in numbers: if (numbers[i] % 2 == 0): evens.append(numbers[i]) print(evens) . This code should output the odd numbers from 0 - 10 using a for loop. . numbers = [0,1,2,3,4,5,6,7,8,9,10] odds = [] for i in numbers: if (numbers[i] % 2 != 0): odds.append(numbers[i]) print(odds) . The intended outcome is printing a number between 1 and 100 once, if it is a multiple of 2 or 5 . What values are outputted incorrectly. Why? | Make changes to get the intended outcome. | . numbers = [] newNumbers = [] i = 0 while i &lt;= 100: numbers.append(i) i += 1 for i in numbers: if numbers[i] == 0: continue if numbers[i] % 5 == 0: newNumbers.append(numbers[i]) elif numbers[i] % 2 == 0: newNumbers.append(numbers[i]) print(newNumbers) . [2, 4, 5, 6, 8, 10, 12, 14, 15, 16, 18, 20, 22, 24, 25, 26, 28, 30, 32, 34, 35, 36, 38, 40, 42, 44, 45, 46, 48, 50, 52, 54, 55, 56, 58, 60, 62, 64, 65, 66, 68, 70, 72, 74, 75, 76, 78, 80, 82, 84, 85, 86, 88, 90, 92, 94, 95, 96, 98, 100] . Challenge . This code segment is at a very early stage of implementation. . What are some ways to (user) error proof this code? | The code should be able to calculate the cost of the meal of the user | . Hint: . write a “single” test describing an expectation of the program of the program | test - input burger, expect output of burger price | run the test, which should fail because the program lacks that feature | write “just enough” code, the simplest possible, to make the test pass | . Then repeat this process until you get program working like you want it to work. . menu = {&quot;burger&quot;: 3.99, &quot;fries&quot;: 1.99, &quot;drink&quot;: 0.99} total = 0 #shows the user the menu and prompts them to select an item print(&quot;Menu&quot;) for k,v in menu.items(): print(k + &quot; $&quot; + str(v)) #why does v have &quot;str&quot; in front of it? Because each key value in the menu is actually an integer type. #ideally the code should prompt the user multiple times num_of_items = int(input(&quot;How many items do you wish to buy? &quot;)) print(&quot;Now ordering {0} items&quot;.format(num_of_items)) counter = 0 shopping_cart = {&quot;burger&quot;:0,&quot;fries&quot;:0,&quot;drink&quot;:0} while counter &lt; num_of_items: item = input(&quot;Please select an item from the menu&quot;) try: total+=menu[item] shopping_cart[item]+=1 print(&quot;Processing order for {0}&quot;.format(item)) counter+=1 except: print(&quot;Product &quot;{0} &quot; is not on our menu, please try again...&quot;.format(item)) #code should add the price of the menu items selected by the user print(&quot;You bought {0} burgers, {1} fries, and {2} drinks for a total of {3}$&quot;.format(shopping_cart[&quot;burger&quot;],shopping_cart[&quot;fries&quot;],shopping_cart[&quot;drink&quot;],round(total,2))) . Menu burger $3.99 fries $1.99 drink $0.99 Now ordering 5 items Processing order for burger Processing order for burger Processing order for fries Processing order for drink Product &#34;ice cream&#34; is not on our menu, please try again... Processing order for fries You bought 2 burgers, 2 fries, and 1 drinks for a total of 12.95$ . Hacks . Now is a good time to think about Testing of your teams final project... . What errors may arise in your project? | What are some test cases that can be used? | Make sure to document any bugs you encounter and how you solved the problem. | What are “single” tests that you will perform on your project? Or, your part of the project? As Hack Design and Test plan action … Divide these “single” tests into Issues for Scrum Board prior to coding. FYI, related tests could be in same Issue by using markdown checkboxes to separate tests. | . | .",
            "url": "https://ylu-1258.github.io/YLu-Blog/week%207/jupyter/apcsp/2022/10/03/AP-error_testing.html",
            "relUrl": "/week%207/jupyter/apcsp/2022/10/03/AP-error_testing.html",
            "date": " • Oct 3, 2022"
        }
        
    
  
    
        ,"post12": {
            "title": "Collegeboard 1.1 - 1.3",
            "content": "Screenshots (Completion of Quiz) . Quiz 1.1 Collaboration: . Quiz 1.2 Program Function and Purpose: . Quiz 1.3 Program Design and Development: . Quiz 1.4 Identifying and Correcting Errors: . Notes from Collegeboard videos . 1.1 Collaboration . Learning Objectives CRD - 1.A Explain how innovations are improved through collaboration. CRD - 1.A.1 A computing innovation includes a program as an integral part of its function. CRD - 1.A.2 A computer innovation can be physica (i.e., self-driving car), non physical computing software (e.g., picture editing software), or a nonphysical computng concept (e.g., e-commerce). CRD - 1.A.3 Effective collaboration produces a computing innovation that reflects the diversity of talents and perspectives of those who designed it. CRD - 1.A.4 Collaboration that incudes diverse perspectives helps avoid bias in the development of computing innovations CRD - 1.C Demonstrate effective interpersonal skills during collaboration. CRD - 1.C.1 Effective collaborative teams practice interpersonal skills, including, but not limited to: Communication, Consesus Building, Conflict Resolution, Negotiation . Notes Different people in different departments work on different subjects and interests. . A program always start up with an idea, purpose and plan. A program must have requirements of the that meet the necessary constraints it must meet. . Developers are able to practice better interpersonal skills in a diverse team. . Pair Programming: One programmer types the code, while the other reviews each line of code. . Think-Pair-Share: Students think through a problem alone, pair with a partner to share ideas, and then share results with the class . 1.2: Program Function and Purpose . Learning Objectives CRD - 2.A - Describe the purpose of a computing innovation. CRD - 2.A.1 - The purpose of computing innovations is to solve problems or to pursue interests through creative expression. CRD - 2.A.2 - An understanding of the purpose of a computing innovation provides developers with an improved abiltiy to develop that essential knowledge on. . Notes Different innovations are contained under different categories of innovations . Program inputs are pecies of data that are sent to computers for processing and interpretation. The computer will then perform operations and manipulate teh data in order to produce a desirable. Inputs can either come from the user themselves, or from another program. . Every event in a program is associated with an action that supplies an input to the program. . Event Driven Programming: Program segments and code fragments are executed based on events that trigger specific control flow structures rather an a smooth, sequential flow. . A program is a collection of software statements that collectively serves a specific use and performs a certain task. . The program works for a variety of inputs and situations. (Think of it like a function) . 1.3: Program Design and Development . Learning Objectives CRD - 2.E - Develop a program using a development process. CRD - 2.F - Design a program and its user interface CRD - 2.E.1 - A development process can be ordered and intentional, or exploratory in nature. CRD - 2.E.2 - There are multiple development processes. The following phases are commonly used when developing a program: . Investigating and reflecting | Designing | Prototyping | Testing | . CRD - 2.E.3 - A development process that is iterative requires refinement and revision based on feedback, testing, or reflection throughout the process. This may require revisiting earlier phases of the process. CRD - 2.F.1 - The design of a program incorporates investigation to determine its requirements. CRD - 2.F.2 - Investigation in a development process is useful for understanding and identifying the program constraints, as well as the concerns and interests of the people who will use the program. CRD - 2.F.3 - Some ways investigation can be performed are as follows: . Collecting data through surveys | User testing | Interviews | . CRD - 2.F.4 - Program requirements describe how a program functions and may include a description of user interactions that a program must provide. CRD - 2.F.5 - A program’s specification defines the requirements for the program. CRD - 2.F.6 - In a development process, the design phase outlines how to accomplish a given program specification. CRD - 2.F.7 - The design phase of a program may include: . Brainstorming | Planning and storyboarding | Organizing the program into modules and functional components | Creation of diagrams that represent the layouts of the user interface | Development of a testing strategy for the program | . 1.4: Identifying and Correcting Errors . Learning Objectives CRD-2.I.1 - A logic error is a mistake in the algorithm or program that causes it to behave incorrectly or unexpectedly. CRD-2.I.2 - A syntax error is a mistake in the program where the rules of the programming language are not followed. CRD-2.I.3 - A run-time error is a mistake in the program that occurs during the execution of a program. Programming languages define their own runtime errors. CRD-2.I.4 - An overflow error is an error that occurs when a computer attempts to handle a number that is outside of the defined range of values. CRD-2.I.5 - The following are effective ways to find and correct errors: . test cases | hand tracing | visualizations | debuggers | adding extra output statement(s) CRD-2.J.1 - In the development process, testing uses defined inputs to ensure that an algorithm or program is producing the expected outcomes. Programmers use the results from testing to revise their algorithms or programs. CRD-2.J.2 - Defined inputs used to test a program should demonstrate the different expected outcomes that are at or just beyond the extremes (minimum and maximum) of input data. CRD-2.J.3 - Program requirements are needed to identify appropriate defined inputs for testing. | .",
            "url": "https://ylu-1258.github.io/YLu-Blog/markdown/apcsp/week%206/2022/10/02/Collegeboard-notes-unit-1.html",
            "relUrl": "/markdown/apcsp/week%206/2022/10/02/Collegeboard-notes-unit-1.html",
            "date": " • Oct 2, 2022"
        }
        
    
  
    
        ,"post13": {
            "title": "Coinflip",
            "content": "| Javascript Kernel Usage | Javascript Table Generator | Coin Flip | . Ayo what is this . Having trouble deciding on something? Why not try this coin flip! . Flip A Coin! . Heads: Tails: . | | &lt;/table&gt; .",
            "url": "https://ylu-1258.github.io/YLu-Blog/markdown/apcsp/week%205/2022/09/28/coinflip.html",
            "relUrl": "/markdown/apcsp/week%205/2022/09/28/coinflip.html",
            "date": " • Sep 28, 2022"
        }
        
    
  
    
        ,"post14": {
            "title": "Project Purpoes and Scrum Methodology",
            "content": "The Idea . Our team wishes to build a minesweeper game with a python backend that could consistently communicate with our front-end website to bring a new degree of sophistication and aesthetic to this classic Game. This project would be split into two main portions each with it’s own challenge. The first portion would be making the game engine in python, while the latter portion would be to design and create an aesthetically pleasing and functional frontend to display the Game Status . . The Frontend . The frontend aspect of this project should preferably display a grid of the game and incorporate some aspect of user input. Preliminary testing and experimentation could be done through textbox inputs of coordinate values, while prospective functions could include individual buttons on the Grid itself to provide a GUI for the user to interact with. In both cases, the frontend should be able to communicate with the python backend through JSON data to send and receive data. . More aspects about the Frontend aspects will be added with additional planning in the future. . The Backend . Arguably the more challenging half of this project, the backend would primarily serve to be the game engine of the minesweeper game. Additionally, the backend would also control and organize the various pages and menus on the website. In this writeup, we will only be focusing on the main goals and challenges faced by the Backend team. . Using Object Oriented Programming to create individual objects for the game. Some aspects that could be represented by such objects would be the gameboard and also the individual cells and nodes present within the game board. Doing so would simplify the algorithmic aspect of the game, as the board itself could have methods and attributes to help control the game logic. | Morever, using an object to represent an individual cell in the board would provide greater functionalities than just using a single variable | | Use Enumerated types with the python enum module to create different values for the type and status of each node. Each of these types could then be bound to a constant value which could then be printed on the screen . | Use Recursion to create an algorithm to recursively detect adjacent cells that are safe (AKA not mines). This algorithm would work in the following format Maintain a list of current cells already determined to be safe | Verify if the four adjacent (up, down, left, right) cells next to the selected cell are safe or not, if safe, store the coordinate point in the list, if not, record the cell as a “border cell”, terminate the recursive process, and run another helper function to determine the precise number of mines surrounding the cell | Re-call the function for each of the surrounding adjacent cells to identify other consecutive cells who are safe. | Return a list of the coordinates of a contiguous block of safe cells | Mark the cells to be safe and calculate number of surrounding mines for border cells. | | Verify the Game status, Game is won if: All mines are flagged | All safe cells are cleared Game is lost if: | A mine was dug by the user | | Return the final result back to the front end. If a safe cell was dug, send out a JSON containing an array of the coordinates of the cell and it’s safe neighbors. If a mine was dug, send a JSON containing a boolean value to signify the end of the game. | The Scrum Process . .",
            "url": "https://ylu-1258.github.io/YLu-Blog/markdown/apcsp/week%205/2022/09/26/Project-and-Scrum.html",
            "relUrl": "/markdown/apcsp/week%205/2022/09/26/Project-and-Scrum.html",
            "date": " • Sep 26, 2022"
        }
        
    
  
    
        ,"post15": {
            "title": "Project Details",
            "content": "The Idea . Our team wishes to build a minesweeper game with a python backend that could consistently communicate with our front-end website to bring a new degree of sophistication and aesthetic to this classic Game. This project would be split into two main portions each with it&#39;s own challenge. The first portion would be making the game engine in python, while the latter portion would be to design and create an aesthetically pleasing and functional frontend to display the Game Status . The Frontend . The frontend aspect of this project should preferably display a grid of the game and incorporate some aspect of user input. Preliminary testing and experimentation could be done through textbox inputs of coordinate values, while prospective functions could include individual buttons on the Grid itself to provide a GUI for the user to interact with. In both cases, the frontend should be able to communicate with the python backend through JSON data to send and receive data. . More aspects about the Frontend aspects will be added with additional planning in the future. . The Backend . Arguably the more challenging half of this project, the backend would primarily serve to be the game engine of the minesweeper game. Additionally, the backend would also control and organize the various pages and menus on the website. In this writeup, we will only be focusing on the main goals and challenges faced by the Backend team. . Using Object Oriented Programming to create individual objects for the game. Some aspects that could be represented by such objects would be the gameboard and also the individual cells and nodes present within the game board. Doing so would simplify the algorithmic aspect of the game, as the board itself could have methods and attributes to help control the game logic. | Morever, using an object to represent an individual cell in the board would provide greater functionalities than just using a single variable | | Use Enumerated types with the python enum module to create different values for the type and status of each node. Each of these types could then be bound to a constant value which could then be printed on the screen . | Use Recursion to create an algorithm to recursively detect adjacent cells that are safe (AKA not mines). This algorithm would work in the following format . Maintain a list of current cells already determined to be safe | Verify if the four adjacent (up, down, left, right) cells next to the selected cell are safe or not, if safe, store the coordinate point in the list, if not, record the cell as a &quot;border cell&quot;, terminate the recursive process, and run another helper function to determine the precise number of mines surrounding the cell | Re-call the function for each of the surrounding adjacent cells to identify other consecutive cells who are safe. | Return a list of the coordinates of a contiguous block of safe cells | Mark the cells to be safe and calculate number of surrounding mines for border cells. | | Verify the Game status, Game is won if: . All mines are flagged | All safe cells are cleared Game is lost if: | A mine was dug by the user | | Return the final result back to the front end. If a safe cell was dug, send out a JSON containing an array of the coordinates of the cell and it&#39;s safe neighbors. If a mine was dug, send a JSON containing a boolean value to signify the end of the game. . | CHALLENGE 1: OOP . Below is the currently implemented code for the gameboard and each individual node. . Here is the code for each individual node in the minesweeper: . from enum import Enum from random import randint . class MType(Enum): SAFE = 1 MINE = 2 class MStatus(Enum): UNKNOWN = 1 DUG = 2 FLAG = 3 class Node: def __init__(self): self.type = MType.SAFE self.status = MStatus.UNKNOWN self.value = &quot;-&quot; def setInitialValue(self): if self.type == MType.MINE: self.value=&quot;M&quot; . Here is the Code for the Minesweeper gameboard and its methods. . class NotInBoard(Exception): pass class Minesweeper: # Class constructor, containing game data def __init__(self): self.mine_coords = [] self.board = [] self.rows = 0 self.cols = 0 self.gameWin = False self.gameOver = False # Create the gameboard on a given input of rows and columns def generateBoard(self,r,c): self.rows = r self.cols = c self.board = [[Node() for i in range(self.rows)] for i in range(self.cols)] return self.board # Print the current board def printBoard(self): res = &quot;&quot; for row in self.board: concat = &quot;&quot; for column in row: concat = concat + column.value + &quot; &quot; concat = concat.strip() + &quot; n&quot; res = res+concat print(res) return 0 # Generate the coordinates of the mines AFTER the first user input # THE FIRST ACTION ON A CELL WOULD NEVER BE A MINE! def setMines(self): num_of_mines = (self.rows * self.cols)//4 counter = 0 while counter &lt;= num_of_mines: mine_coord = (randint(0,self.rows-1),randint(0,self.cols-1)) print(&quot;Mine coord: &quot;, mine_coord) if mine_coord in self.mine_coords: continue self.board[mine_coord[0]][mine_coord[1]].type = MType.MINE self.mine_coords.append(mine_coord) counter +=1 for i in range(self.rows): for j in range(self.cols): self.board[i][j].setInitialValue() return 0 # The first click of the game def firstClick(self): self.printBoard() try: ipt = input(&quot;What row and column? (format in row,column): &quot;) row, col = int(ipt.split(&quot;,&quot;)[0]), int(ipt.split(&quot;,&quot;)[1]) if row not in range(1,self.rows+1) or col not in range(1,self.cols+1): print(&quot;Values not in bound, try again&quot;) raise NotInBoard self.setMines() except NotInBoard: self.firstClick() # Following actions after the first-dig # checkAdkacent function will be created soon def digMine(self, r ,c): if self.board[r][c].type == MType.MINE: self.gameOver = True else: safe_mines = self.checkAdjacent(r,c) . CHALLENGE 2: Enumerated types . This part is kinda free tho ngl . from enum import Enum class MType(Enum): SAFE = 1 MINE = 2 class MStatus(Enum): UNKNOWN = 1 DUG = 2 FLAG = 3 . CHALLENGE 3: Recursive algorithm . Yet to be implemented . CHALLENGE 4: Yet to be implemented . Yet to be implemented . CHALLENGE 5: JSON communication . Yet to be implemented . Current Game Implementation . def main(): MS = Minesweeper() rows = int(input(&quot;How many rows do you want? &quot;)) cols = int(input(&quot;How many columns do you want? &quot;)) MS.generateBoard(cols, rows) MS.firstClick() MS.printBoard() # Define a while lop here to loop game. print(&quot;List of generated mine coordinates: &quot;, MS.mine_coords) main() . - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Mine coord: (3, 8) Mine coord: (9, 2) Mine coord: (4, 7) Mine coord: (7, 3) Mine coord: (9, 6) Mine coord: (8, 1) Mine coord: (5, 1) Mine coord: (7, 6) Mine coord: (5, 5) Mine coord: (8, 4) Mine coord: (7, 2) Mine coord: (4, 7) Mine coord: (8, 3) Mine coord: (2, 9) Mine coord: (0, 9) Mine coord: (5, 4) Mine coord: (7, 9) Mine coord: (2, 0) Mine coord: (0, 1) Mine coord: (4, 4) Mine coord: (4, 2) Mine coord: (6, 3) Mine coord: (0, 6) Mine coord: (3, 0) Mine coord: (9, 7) Mine coord: (8, 3) Mine coord: (4, 9) Mine coord: (9, 6) Mine coord: (3, 9) - M - - - - M - - M - - - - - - - - - - M - - - - - - - - M M - - - - - - - M M - - M - M - - M - M - M - - M M - - - - - - - M - - - - - - - - M M - - M - - M - M - M M - - - - - - - M - - - M M - - List of generated mine coordinates: [(3, 8), (9, 2), (4, 7), (7, 3), (9, 6), (8, 1), (5, 1), (7, 6), (5, 5), (8, 4), (7, 2), (8, 3), (2, 9), (0, 9), (5, 4), (7, 9), (2, 0), (0, 1), (4, 4), (4, 2), (6, 3), (0, 6), (3, 0), (9, 7), (4, 9), (3, 9)] .",
            "url": "https://ylu-1258.github.io/YLu-Blog/jupyter/apcsp/week%205/2022/09/25/Project-Purpose.html",
            "relUrl": "/jupyter/apcsp/week%205/2022/09/25/Project-Purpose.html",
            "date": " • Sep 25, 2022"
        }
        
    
  
    
        ,"post16": {
            "title": "Javascript Table Generator.",
            "content": "| Javascript Kernel Usage | Javascript Table Generator | Coin Flip | . . Defining the Class Object and Classes array . function Class(name, period, timeStart, timeEnd, teacher) { this.name = name; this.period = period; this.timeStart = timeStart; this.timeEnd = timeEnd; this.teacher = teacher; this.grade = &quot;&quot;; } Class.prototype.set_grade = function(grade) { this.grade = grade } var Classes = [new Class(&quot;AP Physics C: Mechanics&quot;, &quot;1&quot;, &quot;8:35&quot;, &quot;9:44&quot;, &quot;Mr. Liao&quot;), new Class(&quot;AP Calculus BC&quot;, &quot;2&quot;, &quot;9:49&quot;, &quot;10:58&quot;, &quot;Mrs. Lanzi&quot;), new Class(&quot;US History&quot;, &quot;3&quot;, &quot;11:13&quot;, &quot;12:22&quot;, &quot;Mr. Smith&quot;), new Class(&quot;AP Computer Science: Principles&quot;, &quot;4&quot;, &quot;12:57&quot;, &quot;2:06&quot;, &quot;Mr. Mort (the goat)&quot;), new Class(&quot;AP Physics C: Mechanics&quot;, &quot;5&quot;, &quot;2:36&quot;, &quot;3:45&quot;, &quot;Mrs. Dafoe&quot;)]; Classes[0].set_grade(&quot;A&quot;) Classes[1].set_grade(&quot;B+&quot;) Classes[2].set_grade(&quot;A+&quot;) Classes[3].set_grade(&quot;A+&quot;) Classes[4].set_grade(&quot;A&quot;) console.log(Classes) . [ Class { name: &#39;AP Physics C: Mechanics&#39;, period: &#39;1&#39;, timeStart: &#39;8:35&#39;, timeEnd: &#39;9:44&#39;, teacher: &#39;Mr. Liao&#39;, grade: &#39;A&#39; }, Class { name: &#39;AP Calculus BC&#39;, period: &#39;2&#39;, timeStart: &#39;9:49&#39;, timeEnd: &#39;10:58&#39;, teacher: &#39;Mrs. Lanzi&#39;, grade: &#39;B+&#39; }, Class { name: &#39;US History&#39;, period: &#39;3&#39;, timeStart: &#39;11:13&#39;, timeEnd: &#39;12:22&#39;, teacher: &#39;Mr. Smith&#39;, grade: &#39;A+&#39; }, Class { name: &#39;AP Computer Science: Principles&#39;, period: &#39;4&#39;, timeStart: &#39;12:57&#39;, timeEnd: &#39;2:06&#39;, teacher: &#39;Mr. Mort (the goat)&#39;, grade: &#39;A+&#39; }, Class { name: &#39;AP Physics C: Mechanics&#39;, period: &#39;5&#39;, timeStart: &#39;2:36&#39;, timeEnd: &#39;3:45&#39;, teacher: &#39;Mrs. Dafoe&#39;, grade: &#39;A&#39; } ] . Constructing a table from the Classes Array . function toHtml (Classes) { // HTML Style is build using inline structure var style = ( &quot;background-color: rgb(8, 0, 124);&quot; + &quot;border-color: rgb(220, 220, 220);&quot; + &quot;color: rgb(220, 220, 220);&quot; ); // HTML Body of Table is build as a series of concatenations (+=) var body = &quot;&quot;; // Heading for Array Columns body += &quot;&lt;tr&gt;&quot;; body += &quot;&lt;th&gt;&lt;strong&gt;&quot; + &quot;Class&quot; + &quot;&lt;/strong&gt;&lt;/th&gt;&quot;; body += &quot;&lt;th&gt;&lt;strong&gt;&quot; + &quot;Period #&quot; + &quot;&lt;/strong&gt;&lt;/th&gt;&quot;; body += &quot;&lt;th&gt;&lt;strong&gt;&quot; + &quot;Starting Time&quot; + &quot;&lt;/strong&gt;&lt;/th&gt;&quot;; body += &quot;&lt;th&gt;&lt;strong&gt;&quot; + &quot;Ending Time&quot; + &quot;&lt;/strong&gt;&lt;/th&gt;&quot;; body += &quot;&lt;th&gt;&lt;strong&gt;&quot; + &quot;Teacher&quot; + &quot;&lt;/strong&gt;&lt;/th&gt;&quot;; body += &quot;&lt;th&gt;&lt;strong&gt;&quot; + &quot;Grade&quot; + &quot;&lt;/strong&gt;&lt;/th&gt;&quot;; body += &quot;&lt;/tr&gt;&quot;; // Data of Array, iterate through each row of compsci.classroom for (var row of Classes) { // tr for each row, a new line body += &quot;&lt;tr&gt;&quot;; // td for each column of data body += &quot;&lt;td&gt;&quot; + row.name + &quot;&lt;/td&gt;&quot;; body += &quot;&lt;td&gt;&quot; + row.period + &quot;&lt;/td&gt;&quot;; body += &quot;&lt;td&gt;&quot; + row.timeStart + &quot;&lt;/td&gt;&quot;; body += &quot;&lt;td&gt;&quot; + row.timeEnd + &quot;&lt;/td&gt;&quot;; body += &quot;&lt;td&gt;&quot; + row.teacher + &quot;&lt;/td&gt;&quot;; body += &quot;&lt;td&gt;&quot; + row.grade + &quot;&lt;/td&gt;&quot;; // tr to end line body += &quot;&lt;tr&gt;&quot;; } // Build and HTML fragment of div, table, table body return ( &quot;&lt;div style=&#39;&quot; + style + &quot;&#39;&gt;&quot; + &quot;&lt;table&gt;&quot; + body + &quot;&lt;/table&gt;&quot; + &quot;&lt;/div&gt;&quot; ); }; // IJavaScript HTML processor receive parameter of defined HTML fragment $$.html(toHtml(Classes)); . ClassPeriod #Starting TimeEnding TimeTeacherGrade . AP Physics C: Mechanics | 1 | 8:35 | 9:44 | Mr. Liao | A | AP Calculus BC | 2 | 9:49 | 10:58 | Mrs. Lanzi | B+ | US History | 3 | 11:13 | 12:22 | Mr. Smith | A+ | AP Computer Science: Principles | 4 | 12:57 | 2:06 | Mr. Mort (the goat) | A+ | AP Physics C: Mechanics | 5 | 2:36 | 3:45 | Mrs. Dafoe | A | &lt;/table&gt;&lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; . .",
            "url": "https://ylu-1258.github.io/YLu-Blog/jupyter/apcsp/week%205/2022/09/25/JavaScript-Table.html",
            "relUrl": "/jupyter/apcsp/week%205/2022/09/25/JavaScript-Table.html",
            "date": " • Sep 25, 2022"
        }
        
    
  
    
        ,"post17": {
            "title": "JavaScript Writeup",
            "content": "| Javascript Kernel Usage | Javascript Table Generator | Coin Flip | . . What is Javascript? . Javascript is a fronend programming language that drives the web and internet. Every interaction on a website could be traced back to javascript and its functionalities. . Resources: W3Schools feecodecamp.org javascript30.com . Console.log output . Console.log is similar to the print() function in python. However, running this command does not alter the content of the webpgae in anyway. The output of this command actually outputs to a terminal &quot;Console&quot; on the webpage. This could be useful for debugging or reviewing our code. . console.log(&quot;Hello World!&quot;) . Hello World! . Variables . Variables exist in Javascript much similar to variables in python. We could define a variable in our javascript with the var keyword. . var msg = &quot;Hello World!&quot;; console.log(msg); . Hello World! . Functions . Functions could be defined in Javascript with the function keyword, a function name, and arguments to the function. Moreover, the code within the function would need to be wrapped in curly braces (squigglies). . function logIt(ipt) { console.log(ipt) } logIt(&quot;Hello World&quot;); logIt(&quot;Foo Bar!&quot;) logIt(1337) . Hello World Foo Bar! 1337 . Dynamically and loosely typed language (string, number) . Javascript is a loosely typed language which means that the type of information to be stored does not need to be specified in advance. . function logItType(output) { console.log(typeof output, &quot;:&quot;, output); } console.log(&quot;Looking at dynamic nature of types in JavaScript&quot;) logItType(&quot;hello&quot;); // String logItType(2020); // Number logItType([1, 2, 3]); // Object is generic for this Array, which similar to Python List . Looking at dynamic nature of types in JavaScript string : hello number : 2020 object : [ 1, 2, 3 ] . Building a function class . The this keyword refers to the function itself, similar to self in python. The variables defined in this way are isolated within their own scope. . Moreover, the JSON string created in the javascript file could be used as a message to communicate between frontend and backend. The JSON is a way to serialize data to transfer it over a network. Meaning we could use JSON to communicate between javascript programs and python programs. . // define a function to hold data for a Person function Person(name, ghID, classOf) { this.name = name; this.ghID = ghID; this.classOf = classOf; this.role = &quot;&quot;; } // define a setter for role in Person data Person.prototype.setRole = function(role) { this.role = role; } // define a JSON conversion &quot;method&quot; associated with Person Person.prototype.toJSON = function() { const obj = {name: this.name, ghID: this.ghID, classOf: this.classOf, role: this.role}; const json = JSON.stringify(obj); return json; } // make a new Person and assign to variable teacher var teacher = new Person(&quot;Mr M&quot;, &quot;jm1021&quot;, 1977); // object type is easy to work with in JavaScript logItType(teacher); // before role logItType(teacher.toJSON()); // ok to do this even though role is not yet defined // output of Object and JSON/string associated with Teacher teacher.setRole(&quot;Teacher&quot;); // set the role logItType(teacher); logItType(teacher.toJSON()); . object : Person { name: &#39;Mr M&#39;, ghID: &#39;jm1021&#39;, classOf: 1977, role: &#39;&#39; } string : {&#34;name&#34;:&#34;Mr M&#34;,&#34;ghID&#34;:&#34;jm1021&#34;,&#34;classOf&#34;:1977,&#34;role&#34;:&#34;&#34;} object : Person { name: &#39;Mr M&#39;, ghID: &#39;jm1021&#39;, classOf: 1977, role: &#39;Teacher&#39; } string : {&#34;name&#34;:&#34;Mr M&#34;,&#34;ghID&#34;:&#34;jm1021&#34;,&#34;classOf&#34;:1977,&#34;role&#34;:&#34;Teacher&#34;} .",
            "url": "https://ylu-1258.github.io/YLu-Blog/jupyter/apcsp/week%204/2022/09/21/JavaScript-Tutorial.html",
            "relUrl": "/jupyter/apcsp/week%204/2022/09/21/JavaScript-Tutorial.html",
            "date": " • Sep 21, 2022"
        }
        
    
  
    
        ,"post18": {
            "title": "Psychology and a Good Life",
            "content": "BIG IDEA 1: This enterprise is possible. . We can control and improve our happiness a lot more than we think. . Happiness may be somewhat herritable, but it is still mostly in our control and it’s our decisions that dictate our happiness. | If our genes control 50% of our happiness, then it stands to reason that we have control over the other 50%. | . BIG IDEA 2: We think life’s circumstances matter a ton, but they don’t matter nearly as much as we think . Rent, work, and School may be important, but are they more important to us than happiness? . Lottery winners are happy on the day they win the lottery But months later, lottery winners have the same happiness as they did before winning | . | The opposite applies to bad scenarios Unfortunate events may cause frustration and sadness in the instant they happen, but they will dissipate quickly over time too | We as humans get used to it | . | Individuals who experience catastrophic events are more likely to view life with a positive attitude, viewing their life as more precious | . BIG IDEA 3: Taking control of it, is kind of hard . Just how hard is it to be happy? . We can control our happiness through our behaviors | It takes daily work and effort to be happy Unhappy people can do happy things everyday and persevere that way to feel happy | . | Any big event in life requires effort, happiness is the same | A change in happiness requires permenant changes that persist every day of our life | training happiness is similar to exercising | The GI Joe fallacy: Knowing is half the battle. Just listening to something isn’t enough, you must apply it. | . BIG IDEA 4: One of the hard you have to do when you begin this enterprise, is recognizing your intuition don’t work . Our mind is lying to us . Our mind forcasts things that makes us happy, but in reality it does not | People correlate salary (money) with happiness People at different salary levels have different expectations, 30k -&gt; 50k -&gt; 100k … etc | expectations aren’t linear, it grows exponentially | . | Salary vs happiness levels off quickly | People who seek out material posessions are more likely to have lesser wellbeing, | . BIG IDEA 5: Make time for social connection . Very happy people prioritize social connections . Very happy people spend more time with others and less time alone Prioritize the stuff that works | Spend time with friends and families | . | Unhappy people spend more time alone than happy people | Experiments that spend time with strangers also showed that connecting with others showed significantly greater positivity than staying alone | .",
            "url": "https://ylu-1258.github.io/YLu-Blog/markdown/apcsp/human_day/2022/09/15/Psychology-and-a-good-life.html",
            "relUrl": "/markdown/apcsp/human_day/2022/09/15/Psychology-and-a-good-life.html",
            "date": " • Sep 15, 2022"
        }
        
    
  
    
        ,"post19": {
            "title": "College Board Create Performance Task",
            "content": "College Board Create Performance Task . Plan and Ideas . A program that creates a hang-man game for the user. . Row 1: Program Purpose and Function . Assesses students’ ability to explain how a code segment or program functions. Know the difference between program purpose and function The purpoes and function is what the program was meant to do Examples: The purpose of the Quiz program: To assess knowledge on a topic. The purpose of InfoDb: To store and access user information. . The purpose of this game is to create a word guessing game, where the user guesses certain letters, and the program will automatically fill in any letters present in the target word that the user inputs. After a given amount of tries, if the user hasn’t guessed the word, the game will fail itself. . Row 2: Data Abstraction . Assesses students’ ability to use abstraction to manage complexity in a program. The Abstraction and storage of data in a generalized container, e.g. variables, lists, dictionaries . The hangman game will predominantly use lists and variables to store the necessary data for the game. The variables could hold the different “stages” of the hangman ascii-art. On the otherhand, lists could be used to keep track of the already used letters, the target word, and the letters that were found in the target word. DOing so would allow for easy manipulation of the data stored and processed by the user. . Row 3: Managing Complexity . Assesses students’ ability to explain how abstraction manages complexity. The print_data function in the InfoDb project helps to reduce the amount of code that has to be written, as the code under print_data was abstracted from the function that was pre-defined. . Helps to reduce complexity in the code as everything is compartmentalized. . Individual functions and classes could be created to compartmentalize certain processes to reduce the clutter and repitition in the code. One function could be used to verify the prescence of a letter in a word, another function could be used to check if the game has been won, and a final main function could be made with a loop that runs the overall game. . Row 4: Prodecural Abstraction . Assesses students’ ability to use abstraction to manage complexity in a program. The print_data function used in the InfoDb project was abstracted into the various for loops that the program used. . The many functions and classes used in the hangman game could be abstracted in the program to manage the overall complexity of the game. . Row 5: Algorithm Implementation . Assesses students’ ability to implement and apply an algorithm. An algorithm is a methodology or a logical path that a computer follows to properly, . Ex: The percentage of the score in the quiz was an example of an algorithm Some algorithms that could be implemented in this project could be a way of keeping track of total number of attempts taken or checking if a game has been won, (e.g. all of the spaces have been filled out with letters) . Row 6: Testing . Assesses students’ ability to investigate the situation, context, or task. Testing is running over all the processes in a program and ensuring that everything works. The ability to find, detect, and fix any bugs apparent in the program. . Some testing that could be done on the project could be testing for accepting inputs from the user (subsituting a letter in when it’s found to be correct), or testing the proper game end and game start events. . How to Get a Five . REVIEW PAST NOTES FROM CLASS . Be sure to be clear on the vocab and content taught in the class. Be parepared to be able to properly define each vocabulary and provide examples to each vocab. Any questions that cannot be answered with my notes, ask the teammates first and then consult the internet. If the problems still remains unsolved, consult the teacher about it. . LOOK AT PREVIOUS COLLEGEBOARD SUBMISSIONS AND ASSIGNMENTS . Learn from the past submissions of other people, plan ahead of time what to consider and what to add to my project. .",
            "url": "https://ylu-1258.github.io/YLu-Blog/markdown/apcsp/week%202/2022/09/08/College-Board-Performance-Task.html",
            "relUrl": "/markdown/apcsp/week%202/2022/09/08/College-Board-Performance-Task.html",
            "date": " • Sep 8, 2022"
        }
        
    
  
    
        ,"post20": {
            "title": "Applab Quiz Write up",
            "content": "What is AppLab? . App Lab is a programming environment where you can make simple apps. Design an app, code in JavaScript with either blocks or text, then share your app with Teacher or Student Peers. The big limitations of App Lab, that makes it simple, is that the HTML and CSS are not available to developer/student coder, thus it is not directly transportable to an independent Web server. . Code.org account created. To create a new project, click on the AppLab section on the main page. . The block code used in code.org AppLab is a simplified version of the JavaScript language. Click on the Show Text button to edit the code directly. . Plan for the Quiz (Design) . ✔️ Verify answer . ✔️ Inccorect Answer Screen . ✔️ Counter to keep track of incorrect account . ✔️ Variable to keep track of current problem . ✔️ Free Response question prompts . Link to the quiz . . Successes . Was able to create a quiz where one screen was able to dynamically able to backtrack to the previous answer. | Free response input is now also supported by the quiz. | Background counter helps to keep a track of the number of inccorect attempts made. | Free response questions can accept multiple different forms of input. | . Discoveries / Challenges . Realized that you can convert between Int and string types with the toString() function. | Struggled to find a way to concatenate strings and integers but then found about about the concat() function. | The score box seemed to reset, realized that I just made the text box too small :/ | .",
            "url": "https://ylu-1258.github.io/YLu-Blog/markdown/apcsp/week%202/2022/09/07/Lecture-code-org-applab.html",
            "relUrl": "/markdown/apcsp/week%202/2022/09/07/Lecture-code-org-applab.html",
            "date": " • Sep 7, 2022"
        }
        
    
  
    
        ,"post21": {
            "title": "HTML writeup",
            "content": "Changing theme to Jekyll Midnight . The overall theme of the blog could be changed under the _config.yml file in the home directory of the blog. One can either edit the theme key pair value, or the remote_theme keypair value. However, the latter option must have the value jekyll-remote-theme defined under the plugins key. I decided to use the midnight theme becuase it looks cool as heck. . However, the first major problem I ran into was a porblem with my jekyll build, looking closer at the error message, I realized that there was a “Download Error”, presumably some problems occured while trying to install the new midnight theme. . . However, after doing some more research, I realized that the version of the theme must be specified with the @ at the end of the value, I initially didn’t supply this value, and thus received the error. After specifying the latest version, The Jekyll build passed, and my blog looked like this: . Top half of blog: . Bottom half of blog: . One immediate problems I noticed was that the chagne in theme completely removed parts of my blog’s navigation bar, as the tags and lecutre notes tab were no longer present at the top. This finding shows that other Jekyll themes are not fully compatible with the fastpages backend, and that the minima theme is prefered for compatibility. .",
            "url": "https://ylu-1258.github.io/YLu-Blog/markdown/apcsp/week%202/2022/09/03/HTML-writeup.html",
            "relUrl": "/markdown/apcsp/week%202/2022/09/03/HTML-writeup.html",
            "date": " • Sep 3, 2022"
        }
        
    
  
    
        ,"post22": {
            "title": "HTML Fragments",
            "content": "HTML Fragments and Markdown . HTML fragments are just a part of the total HTML source on our site. . Table Fragments Fastpages allows to build tables in HTML or Markdown. Everything on the site is built in HTML, the markdown is converted into individual HTML fragments which are assembled to form webpage. Tables that we create and use in markdown are also read, interpreted and turned into html tables. . tl;dr EVERYTHING ON THE WEBPAGE IS IN HTML . images “” element in HTML allows us to specify cetain image parameters, more flexible than markdown method. . links “” element with href in HTML . Web Page Layout . Design themes and layouts of the page. . In the _config.yml page, The theme can be changed with the remote_theme: , which could change the coloration of the text . NOTE: Fastpages only works with the Jekyll/minima theme, and not with any other theme . Additional coloration and customization could be attained with the sass (sassy) .",
            "url": "https://ylu-1258.github.io/YLu-Blog/markdown/apcsp/week%202/2022/08/31/HTML-fragments.html",
            "relUrl": "/markdown/apcsp/week%202/2022/08/31/HTML-fragments.html",
            "date": " • Aug 31, 2022"
        }
        
    
  
    
        ,"post23": {
            "title": "Summer Recommender System Algo Research",
            "content": "Import all of our required modules . import pandas as pd import numpy as np !pip install scikit-surprise from surprise import Dataset from surprise import Reader import scipy . Requirement already satisfied: scikit-surprise in /usr/local/lib/python3.7/dist-packages (1.1.1) Requirement already satisfied: scipy&gt;=1.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-surprise) (1.4.1) Requirement already satisfied: numpy&gt;=1.11.2 in /usr/local/lib/python3.7/dist-packages (from scikit-surprise) (1.19.5) Requirement already satisfied: six&gt;=1.10.0 in /usr/local/lib/python3.7/dist-packages (from scikit-surprise) (1.15.0) Requirement already satisfied: joblib&gt;=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-surprise) (1.0.1) . Import and process our data . ## Importing our ratings data ratings = pd.read_csv(&quot;rating.csv&quot;) anime = pd.read_csv(&quot;anime.csv&quot;) ## removing entries with no ratings and resetting indices ratings = ratings.head(125000) ## Apply to_numeric and removing all NaN values ratings = ratings.apply(pd.to_numeric, errors=&#39;coerce&#39;) ratings = ratings.dropna() ## Declaring some stuff for KNN calculation reader = Reader(rating_scale=(1, 10)) data = Dataset.load_from_df(ratings[[&quot;user_id&quot;, &quot;anime_id&quot;, &quot;rating&quot;]], reader) ratings . user_id anime_id rating . 0 1 | 20 | -1 | . 1 1 | 24 | -1 | . 2 1 | 79 | -1 | . 3 1 | 226 | -1 | . 4 1 | 241 | -1 | . ... ... | ... | ... | . 124995 1272 | 4382 | 10 | . 124996 1272 | 4672 | 5 | . 124997 1272 | 4792 | 6 | . 124998 1272 | 4898 | 8 | . 124999 1272 | 4910 | 6 | . 125000 rows × 3 columns . KNN . ratings[[&quot;user_id&quot;, &quot;anime_id&quot;, &quot;rating&quot;]] . user_id anime_id rating . 0 1 | 20 | -1 | . 1 1 | 24 | -1 | . 2 1 | 79 | -1 | . 3 1 | 226 | -1 | . 4 1 | 241 | -1 | . ... ... | ... | ... | . 124995 1272 | 4382 | 10 | . 124996 1272 | 4672 | 5 | . 124997 1272 | 4792 | 6 | . 124998 1272 | 4898 | 8 | . 124999 1272 | 4910 | 6 | . 125000 rows × 3 columns . from surprise import KNNWithMeans sim_options = { &quot;name&quot;: &quot;msd&quot;, &quot;min_support&quot;: 3, &quot;user_based&quot;: False, # Compute similarities between items } algo = KNNWithMeans(sim_options=sim_options) user_id = input(&quot;User Id &gt;&gt;&gt; &quot;) anime_id = input(&quot;Anime Id &gt;&gt;&gt; &quot;) trainingSet = data.build_full_trainset() algo.fit(trainingSet) prediction = algo.predict(int(user_id), int(anime_id)) from google.colab import output output.clear() print(&quot;User {0} would rate anime number {1}&quot;.format(str(user_id), str(anime_id))) prediction.est . User 3 would rate anime number 101 . 6.695240884293928 . from surprise import KNNWithZScore sim_options = { &quot;name&quot;: &quot;msd&quot;, &quot;min_support&quot;: 3, &quot;user_based&quot;: False, # Compute similarities between items } algo = KNNWithZScore(sim_options=sim_options) user_id = input(&quot;User Id &gt;&gt;&gt; &quot;) anime_id = input(&quot;Anime Id &gt;&gt;&gt; &quot;) trainingSet = data.build_full_trainset() algo.fit(trainingSet) prediction = algo.predict(int(user_id), int(anime_id)) from google.colab import output output.clear() print(&quot;User {0} would rate anime number {1}&quot;.format(str(user_id), str(anime_id))) prediction.est . User 3 would rate anime number 101 . 6.702780614370237 . SVD algorithm . Basic algorithms NormalPredictor NormalPredictor algorithm predicts a random rating based on the distribution of the training set, which is assumed to be normal. This is one of the most basic algorithms that do not do much work. BaselineOnly BaselineOnly algorithm predicts the baseline estimate for given user and item. k-NN algorithms KNNBasic KNNBasic is a basic collaborative filtering algorithm. KNNWithMeans KNNWithMeans is basic collaborative filtering algorithm, taking into account the mean ratings of each user. KNNWithZScore KNNWithZScore is a basic collaborative filtering algorithm, taking into account the z-score normalization of each user. KNNBaseline KNNBaseline is a basic collaborative filtering algorithm taking into account a baseline rating. Matrix Factorization-based algorithms SVD SVD algorithm is equivalent to Probabilistic Matrix Factorization SVDpp The SVDpp algorithm is an extension of SVD that takes into account implicit ratings. NMF NMF is a collaborative filtering algorithm based on Non-negative Matrix Factorization. It is very similar with SVD. Slope One SlopeOne is a straightforward implementation of the SlopeOne algorithm. Co-clustering Coclustering is a collaborative filtering algorithm based on co-clustering. We use “rmse” as our accuracy metric for the predictions. . KNNWithMeans . from surprise import KNNWithMeans from surprise import Dataset from surprise.model_selection import GridSearchCV data = Dataset.load_from_df(ratings[[&quot;user_id&quot;, &quot;anime_id&quot;, &quot;rating&quot;]], reader) sim_options = { &quot;name&quot;: [&quot;msd&quot;, &quot;cosine&quot;], &quot;min_support&quot;: [3, 4, 5], &quot;user_based&quot;: [False, True], } param_grid = {&quot;sim_options&quot;: sim_options} gs = GridSearchCV(KNNWithMeans, param_grid, measures=[&quot;rmse&quot;, &quot;mae&quot;], cv=3) gs.fit(data) print(gs.best_score[&quot;rmse&quot;]) print(gs.best_params[&quot;rmse&quot;]) print(gs.best_score[&quot;mae&quot;]) print(gs.best_params[&quot;mae&quot;]) # Cleaned # rmse 1.1961042065214869 # {&#39;sim_options&#39;: {&#39;name&#39;: &#39;msd&#39;, &#39;min_support&#39;: 3, &#39;user_based&#39;: True}} # mae 0.9066350800152176 #{&#39;sim_options&#39;: {&#39;name&#39;: &#39;msd&#39;, &#39;min_support&#39;: 3, &#39;user_based&#39;: False}} # Original #2.300630451838356 #{&#39;sim_options&#39;: {&#39;name&#39;: &#39;msd&#39;, &#39;min_support&#39;: 4, &#39;user_based&#39;: True}} #1.6207772901276576 #{&#39;sim_options&#39;: {&#39;name&#39;: &#39;msd&#39;, &#39;min_support&#39;: 5, &#39;user_based&#39;: True}} # runtime 4m 28s . Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the cosine similarity matrix... Done computing similarity matrix. Computing the cosine similarity matrix... Done computing similarity matrix. Computing the cosine similarity matrix... Done computing similarity matrix. Computing the cosine similarity matrix... Done computing similarity matrix. Computing the cosine similarity matrix... Done computing similarity matrix. Computing the cosine similarity matrix... Done computing similarity matrix. Computing the cosine similarity matrix... Done computing similarity matrix. Computing the cosine similarity matrix... Done computing similarity matrix. Computing the cosine similarity matrix... Done computing similarity matrix. Computing the cosine similarity matrix... Done computing similarity matrix. Computing the cosine similarity matrix... Done computing similarity matrix. Computing the cosine similarity matrix... Done computing similarity matrix. Computing the cosine similarity matrix... Done computing similarity matrix. Computing the cosine similarity matrix... Done computing similarity matrix. Computing the cosine similarity matrix... Done computing similarity matrix. Computing the cosine similarity matrix... Done computing similarity matrix. Computing the cosine similarity matrix... Done computing similarity matrix. Computing the cosine similarity matrix... Done computing similarity matrix. 2.300630451838356 {&#39;sim_options&#39;: {&#39;name&#39;: &#39;msd&#39;, &#39;min_support&#39;: 4, &#39;user_based&#39;: True}} 1.6207772901276576 {&#39;sim_options&#39;: {&#39;name&#39;: &#39;msd&#39;, &#39;min_support&#39;: 5, &#39;user_based&#39;: True}} . KNNWithZScore . from surprise import KNNWithZScore from surprise import Dataset from surprise.model_selection import GridSearchCV data = Dataset.load_from_df(ratings[[&quot;user_id&quot;, &quot;anime_id&quot;, &quot;rating&quot;]], reader) sim_options = { &quot;name&quot;: [&quot;msd&quot;, &quot;cosine&quot;], &quot;min_support&quot;: [3, 4, 5], &quot;user_based&quot;: [False, True], } param_grid = {&quot;sim_options&quot;: sim_options} gs = GridSearchCV(KNNWithZScore, param_grid, measures=[&quot;rmse&quot;, &quot;mae&quot;], cv=3) gs.fit(data) print(gs.best_score[&quot;rmse&quot;]) print(gs.best_params[&quot;rmse&quot;]) print(gs.best_score[&quot;mae&quot;]) print(gs.best_params[&quot;mae&quot;]) # Cleaned #rmse 1.2200524149502934 #{&#39;sim_options&#39;: {&#39;name&#39;: &#39;msd&#39;, &#39;min_support&#39;: 3, &#39;user_based&#39;: True}} #mae 0.9071607750307225 #{&#39;sim_options&#39;: {&#39;name&#39;: &#39;msd&#39;, &#39;min_support&#39;: 4, &#39;user_based&#39;: True}} # Original #2.3157416009374785 #{&#39;sim_options&#39;: {&#39;name&#39;: &#39;cosine&#39;, &#39;min_support&#39;: 4, &#39;user_based&#39;: True}} #1.608562753271584 #{&#39;sim_options&#39;: {&#39;name&#39;: &#39;cosine&#39;, &#39;min_support&#39;: 4, &#39;user_based&#39;: True}} #runtime 9m 28s . Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the cosine similarity matrix... Done computing similarity matrix. Computing the cosine similarity matrix... Done computing similarity matrix. Computing the cosine similarity matrix... Done computing similarity matrix. Computing the cosine similarity matrix... Done computing similarity matrix. Computing the cosine similarity matrix... Done computing similarity matrix. Computing the cosine similarity matrix... Done computing similarity matrix. Computing the cosine similarity matrix... Done computing similarity matrix. Computing the cosine similarity matrix... Done computing similarity matrix. Computing the cosine similarity matrix... Done computing similarity matrix. Computing the cosine similarity matrix... Done computing similarity matrix. Computing the cosine similarity matrix... Done computing similarity matrix. Computing the cosine similarity matrix... Done computing similarity matrix. Computing the cosine similarity matrix... Done computing similarity matrix. Computing the cosine similarity matrix... Done computing similarity matrix. Computing the cosine similarity matrix... Done computing similarity matrix. Computing the cosine similarity matrix... Done computing similarity matrix. Computing the cosine similarity matrix... Done computing similarity matrix. Computing the cosine similarity matrix... Done computing similarity matrix. 2.3157416009374785 {&#39;sim_options&#39;: {&#39;name&#39;: &#39;cosine&#39;, &#39;min_support&#39;: 4, &#39;user_based&#39;: True}} 1.608562753271584 {&#39;sim_options&#39;: {&#39;name&#39;: &#39;cosine&#39;, &#39;min_support&#39;: 4, &#39;user_based&#39;: True}} . BaselineOnly . from surprise import BaselineOnly from surprise import Dataset from surprise.model_selection import GridSearchCV # Misc data = Dataset.load_from_df(ratings[[&quot;user_id&quot;, &quot;anime_id&quot;, &quot;rating&quot;]], reader) bsl_options = {&#39;method&#39;: [&#39;als&#39;, &#39;sgd&#39;], &#39;n_epochs&#39;: [1,5,10], &#39;reg_u&#39;: [12], &#39;reg_i&#39;: [5] } param_grid = {&quot;bsl_options&quot;: bsl_options} gs = GridSearchCV(BaselineOnly, param_grid, measures=[&quot;rmse&quot;, &quot;mae&quot;], cv=3) gs.fit(data) print(gs.best_score[&quot;rmse&quot;]) print(gs.best_params[&quot;rmse&quot;]) print(gs.best_score[&quot;mae&quot;]) print(gs.best_params[&quot;mae&quot;]) # Original # rmse 1.1937039196145423 #{&#39;bsl_options&#39;: {&#39;method&#39;: &#39;als&#39;, &#39;n_epochs&#39;: 10, &#39;reg_u&#39;: 12, &#39;reg_i&#39;: 5}} # mae 0.9073417853583354 #{&#39;bsl_options&#39;: {&#39;method&#39;: &#39;als&#39;, &#39;n_epochs&#39;: 10, &#39;reg_u&#39;: 12, &#39;reg_i&#39;: 5}} # Cleaned #2.2457828423033965 #{&#39;bsl_options&#39;: {&#39;method&#39;: &#39;sgd&#39;, &#39;n_epochs&#39;: 10, &#39;reg_u&#39;: 12, &#39;reg_i&#39;: 5}} #1.6065790249351644 #{&#39;bsl_options&#39;: {&#39;method&#39;: &#39;sgd&#39;, &#39;n_epochs&#39;: 10, &#39;reg_u&#39;: 12, &#39;reg_i&#39;: 5}} # runtime 12s . Estimating biases using als... Estimating biases using als... Estimating biases using als... Estimating biases using als... Estimating biases using als... Estimating biases using als... Estimating biases using als... Estimating biases using als... Estimating biases using als... Estimating biases using sgd... Estimating biases using sgd... Estimating biases using sgd... Estimating biases using sgd... Estimating biases using sgd... Estimating biases using sgd... Estimating biases using sgd... Estimating biases using sgd... Estimating biases using sgd... 2.2457828423033965 {&#39;bsl_options&#39;: {&#39;method&#39;: &#39;sgd&#39;, &#39;n_epochs&#39;: 10, &#39;reg_u&#39;: 12, &#39;reg_i&#39;: 5}} 1.6065790249351644 {&#39;bsl_options&#39;: {&#39;method&#39;: &#39;sgd&#39;, &#39;n_epochs&#39;: 10, &#39;reg_u&#39;: 12, &#39;reg_i&#39;: 5}} . SVD . from surprise import SVD from surprise import Dataset from surprise.model_selection import GridSearchCV # Matrix facotization sim_options = { &quot;n_epochs&quot;: [5,10], &quot;lr_all&quot;: [0.002, 0.005], &quot;reg_all&quot;: [0.4,0.6] } gs = GridSearchCV(SVD, sim_options, measures=[&quot;rmse&quot;, &quot;mae&quot;], cv=3) gs.fit(data) print(gs.best_score[&quot;rmse&quot;]) print(gs.best_params[&quot;rmse&quot;]) print(gs.best_score[&quot;mae&quot;]) print(gs.best_params[&quot;mae&quot;]) # Original #rmse 1.2370006802584665 #{&#39;n_epochs&#39;: 10, &#39;lr_all&#39;: 0.005, &#39;reg_all&#39;: 0.4} #mae 0.9457554878434046 #{&#39;n_epochs&#39;: 10, &#39;lr_all&#39;: 0.005, &#39;reg_all&#39;: 0.4} # Cleaned #2.3105571863770495 #{&#39;n_epochs&#39;: 10, &#39;lr_all&#39;: 0.005, &#39;reg_all&#39;: 0.4} #1.7100669607848105 #{&#39;n_epochs&#39;: 10, &#39;lr_all&#39;: 0.005, &#39;reg_all&#39;: 0.4} #runtime 1m 44s . 2.3105571863770495 {&#39;n_epochs&#39;: 10, &#39;lr_all&#39;: 0.005, &#39;reg_all&#39;: 0.4} 1.7100669607848105 {&#39;n_epochs&#39;: 10, &#39;lr_all&#39;: 0.005, &#39;reg_all&#39;: 0.4} . KNNBasic . from surprise import KNNBasic from surprise import Dataset from surprise.model_selection import GridSearchCV # Clustering sim_options = { &quot;name&quot;: [&quot;msd&quot;, &quot;cosine&quot;], &quot;k&quot;: [20,40], &quot;min_k&quot;: [5, 7], &quot;verbose&quot;: [True] } gs = GridSearchCV(KNNBasic, sim_options, measures=[&quot;rmse&quot;, &quot;mae&quot;], cv=3) gs.fit(data) print(gs.best_score[&quot;rmse&quot;]) print(gs.best_params[&quot;rmse&quot;]) print(gs.best_score[&quot;mae&quot;]) print(gs.best_params[&quot;mae&quot;]) # Cleaned #rmse 1.3169600252251181 #{&#39;name&#39;: &#39;msd&#39;, &#39;k&#39;: 20, &#39;min_k&#39;: 5, &#39;verbose&#39;: True} #mae 0.9971319510312137 #{&#39;name&#39;: &#39;msd&#39;, &#39;k&#39;: 20, &#39;min_k&#39;: 5, &#39;verbose&#39;: True} # Original #2.61306062703497 #{&#39;name&#39;: &#39;msd&#39;, &#39;k&#39;: 20, &#39;min_k&#39;: 5, &#39;verbose&#39;: True} #1.7634939088675168 #{&#39;name&#39;: &#39;msd&#39;, &#39;k&#39;: 20, &#39;min_k&#39;: 5, &#39;verbose&#39;: True} # 4m 53 s . Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. Computing the msd similarity matrix... Done computing similarity matrix. 2.61306062703497 {&#39;name&#39;: &#39;msd&#39;, &#39;k&#39;: 20, &#39;min_k&#39;: 5, &#39;verbose&#39;: True} 1.7634939088675168 {&#39;name&#39;: &#39;msd&#39;, &#39;k&#39;: 20, &#39;min_k&#39;: 5, &#39;verbose&#39;: True} . NMF . from surprise import NMF from surprise import Dataset from surprise.model_selection import GridSearchCV # Matrix Factorization sim_options = { &quot;n_epochs&quot;: [5,10,15,20], &quot;biased&quot;: [True,False], &quot;reg_pu&quot;: [0.01, 0.06, 0.5, 1], &quot;reg_qi&quot;: [0.01, 0.06, 0.5, 1], &quot;verbose&quot;: [True] } gs = GridSearchCV(NMF, sim_options, measures=[&quot;rmse&quot;, &quot;mae&quot;], cv=3) gs.fit(data) print(gs.best_score[&quot;rmse&quot;]) print(gs.best_params[&quot;rmse&quot;]) print(gs.best_score[&quot;mae&quot;]) print(gs.best_params[&quot;mae&quot;]) # Cleaned #1.1889671285702674 #{&#39;n_epochs&#39;: 20, &#39;biased&#39;: True, &#39;reg_pu&#39;: 0.01, &#39;reg_qi&#39;: 1, &#39;verbose&#39;: True} #0.9011699311141764 #{&#39;n_epochs&#39;: 20, &#39;biased&#39;: True, &#39;reg_pu&#39;: 0.01, &#39;reg_qi&#39;: 1, &#39;verbose&#39;: True} # Original #2.2452077636572434 #{&#39;n_epochs&#39;: 10, &#39;biased&#39;: True, &#39;reg_pu&#39;: 0.5, &#39;reg_qi&#39;: 0.5, &#39;verbose&#39;: True} #1.6074999998727018 #{&#39;n_epochs&#39;: 20, &#39;biased&#39;: True, &#39;reg_pu&#39;: 1, &#39;reg_qi&#39;: 1, &#39;verbose&#39;: True} #runtime 11m 25s . Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 2.2452077636572434 {&#39;n_epochs&#39;: 10, &#39;biased&#39;: True, &#39;reg_pu&#39;: 0.5, &#39;reg_qi&#39;: 0.5, &#39;verbose&#39;: True} 1.6074999998727018 {&#39;n_epochs&#39;: 20, &#39;biased&#39;: True, &#39;reg_pu&#39;: 1, &#39;reg_qi&#39;: 1, &#39;verbose&#39;: True} . Normal Predictor . from surprise import NormalPredictor from surprise import Dataset from surprise.model_selection import GridSearchCV # Misc sim_options = { } gs = GridSearchCV(NormalPredictor, sim_options, measures=[&quot;rmse&quot;, &quot;mae&quot;], cv=3) gs.fit(data) print(gs.best_score[&quot;rmse&quot;]) print(gs.best_params[&quot;rmse&quot;]) print(gs.best_score[&quot;mae&quot;]) print(gs.best_params[&quot;mae&quot;]) # Cleaned #rmse 2.0923401858242667 # {} #mae 1.6582413347539269 # {} # Original #4.876980925979471 #{} #3.9231832724710536 #{} #runtime 2s . 4.876980925979471 {} 3.9231832724710536 {} . SlopeOne . from surprise import SlopeOne from surprise import Dataset from surprise.model_selection import GridSearchCV # Clustering param_grid = { } gs = GridSearchCV(SlopeOne, param_grid, measures=[&quot;rmse&quot;, &quot;mae&quot;], cv=3) gs.fit(data) print(gs.best_score[&quot;rmse&quot;]) print(gs.best_params[&quot;rmse&quot;]) print(gs.best_score[&quot;mae&quot;]) print(gs.best_params[&quot;mae&quot;]) # Cleaned #1.2261258397066668 #{} #0.9248136286120494 #{} # Original #2.3148602570198133 #{} #1.646961825093937 #{} #runtime 22s . 2.3148602570198133 {} 1.646961825093937 {} . CoClustering . from surprise import CoClustering from surprise import Dataset from surprise.model_selection import GridSearchCV # Clustering sim_options = { &quot;n_cltr_u&quot;: [2,3,4], &quot;n_cltr_i&quot;: [2,3,4], &quot;n_epochs&quot;: [10,15,20], &quot;verbose&quot; : [True] } gs = GridSearchCV(CoClustering, sim_options, measures=[&quot;rmse&quot;, &quot;mae&quot;], cv=3) gs.fit(data) print(gs.best_score[&quot;rmse&quot;]) print(gs.best_params[&quot;rmse&quot;]) print(gs.best_score[&quot;mae&quot;]) print(gs.best_params[&quot;mae&quot;]) # Cleaned #1.2533606254732572 #{&#39;n_cltr_u&#39;: 4, &#39;n_cltr_i&#39;: 2, &#39;n_epochs&#39;: 10, &#39;verbose&#39;: True} #0.9453075653296411 #{&#39;n_cltr_u&#39;: 4, &#39;n_cltr_i&#39;: 2, &#39;n_epochs&#39;: 10, &#39;verbose&#39;: True} # Original #2.3921675151385458 #{&#39;n_cltr_u&#39;: 3, &#39;n_cltr_i&#39;: 2, &#39;n_epochs&#39;: 20, &#39;verbose&#39;: True} #1.687391909272779 #{&#39;n_cltr_u&#39;: 4, &#39;n_cltr_i&#39;: 2, &#39;n_epochs&#39;: 15, &#39;verbose&#39;: True} #runtime 10m 37s . Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 Processing epoch 0 Processing epoch 1 Processing epoch 2 Processing epoch 3 Processing epoch 4 Processing epoch 5 Processing epoch 6 Processing epoch 7 Processing epoch 8 Processing epoch 9 Processing epoch 10 Processing epoch 11 Processing epoch 12 Processing epoch 13 Processing epoch 14 Processing epoch 15 Processing epoch 16 Processing epoch 17 Processing epoch 18 Processing epoch 19 2.3921675151385458 {&#39;n_cltr_u&#39;: 3, &#39;n_cltr_i&#39;: 2, &#39;n_epochs&#39;: 20, &#39;verbose&#39;: True} 1.687391909272779 {&#39;n_cltr_u&#39;: 4, &#39;n_cltr_i&#39;: 2, &#39;n_epochs&#39;: 15, &#39;verbose&#39;: True} . KNNBaseline . # Clustering from surprise import KNNBaseline from surprise import Dataset from surprise.model_selection import GridSearchCV data = Dataset.load_from_df(ratings[[&quot;user_id&quot;, &quot;anime_id&quot;, &quot;rating&quot;]], reader) sim_options = { &quot;name&quot;: [&quot;msd&quot;, &quot;cosine&quot;], &quot;min_support&quot;: [3, 4, 5], &quot;user_based&quot;: [False, True], } param_grid = {&quot;sim_options&quot;: sim_options} gs = GridSearchCV(KNNBaseline, param_grid, measures=[&quot;rmse&quot;, &quot;mae&quot;], cv=3) gs.fit(data) print(gs.best_score[&quot;rmse&quot;]) print(gs.best_params[&quot;rmse&quot;]) print(gs.best_score[&quot;mae&quot;]) print(gs.best_params[&quot;mae&quot;]) # Cleaned #1.1832580437001543 #{&#39;sim_options&#39;: {&#39;name&#39;: &#39;msd&#39;, &#39;min_support&#39;: 5, &#39;user_based&#39;: False}} #0.8893399468276982 #{&#39;sim_options&#39;: {&#39;name&#39;: &#39;msd&#39;, &#39;min_support&#39;: 5, &#39;user_based&#39;: False}} # Original #2.2179062356553265 #{&#39;sim_options&#39;: {&#39;name&#39;: &#39;msd&#39;, &#39;min_support&#39;: 5, &#39;user_based&#39;: False}} #1.54544242417866 #{&#39;sim_options&#39;: {&#39;name&#39;: &#39;msd&#39;, &#39;min_support&#39;: 5, &#39;user_based&#39;: False}} #runtime 5m 15s . Estimating biases using als... Computing the msd similarity matrix... Done computing similarity matrix. Estimating biases using als... Computing the msd similarity matrix... Done computing similarity matrix. Estimating biases using als... Computing the msd similarity matrix... Done computing similarity matrix. Estimating biases using als... Computing the msd similarity matrix... Done computing similarity matrix. Estimating biases using als... Computing the msd similarity matrix... Done computing similarity matrix. Estimating biases using als... Computing the msd similarity matrix... Done computing similarity matrix. Estimating biases using als... Computing the msd similarity matrix... Done computing similarity matrix. Estimating biases using als... Computing the msd similarity matrix... Done computing similarity matrix. Estimating biases using als... Computing the msd similarity matrix... Done computing similarity matrix. Estimating biases using als... Computing the msd similarity matrix... Done computing similarity matrix. Estimating biases using als... Computing the msd similarity matrix... Done computing similarity matrix. Estimating biases using als... Computing the msd similarity matrix... Done computing similarity matrix. Estimating biases using als... Computing the msd similarity matrix... Done computing similarity matrix. Estimating biases using als... Computing the msd similarity matrix... Done computing similarity matrix. Estimating biases using als... Computing the msd similarity matrix... Done computing similarity matrix. Estimating biases using als... Computing the msd similarity matrix... Done computing similarity matrix. Estimating biases using als... Computing the msd similarity matrix... Done computing similarity matrix. Estimating biases using als... Computing the msd similarity matrix... Done computing similarity matrix. Estimating biases using als... Computing the cosine similarity matrix... Done computing similarity matrix. Estimating biases using als... Computing the cosine similarity matrix... Done computing similarity matrix. Estimating biases using als... Computing the cosine similarity matrix... Done computing similarity matrix. Estimating biases using als... Computing the cosine similarity matrix... Done computing similarity matrix. Estimating biases using als... Computing the cosine similarity matrix... Done computing similarity matrix. Estimating biases using als... Computing the cosine similarity matrix... Done computing similarity matrix. Estimating biases using als... Computing the cosine similarity matrix... Done computing similarity matrix. Estimating biases using als... Computing the cosine similarity matrix... Done computing similarity matrix. Estimating biases using als... Computing the cosine similarity matrix... Done computing similarity matrix. Estimating biases using als... Computing the cosine similarity matrix... Done computing similarity matrix. Estimating biases using als... Computing the cosine similarity matrix... Done computing similarity matrix. Estimating biases using als... Computing the cosine similarity matrix... Done computing similarity matrix. Estimating biases using als... Computing the cosine similarity matrix... Done computing similarity matrix. Estimating biases using als... Computing the cosine similarity matrix... Done computing similarity matrix. Estimating biases using als... Computing the cosine similarity matrix... Done computing similarity matrix. Estimating biases using als... Computing the cosine similarity matrix... Done computing similarity matrix. Estimating biases using als... Computing the cosine similarity matrix... Done computing similarity matrix. Estimating biases using als... Computing the cosine similarity matrix... Done computing similarity matrix. 2.2179062356553265 {&#39;sim_options&#39;: {&#39;name&#39;: &#39;msd&#39;, &#39;min_support&#39;: 5, &#39;user_based&#39;: False}} 1.54544242417866 {&#39;sim_options&#39;: {&#39;name&#39;: &#39;msd&#39;, &#39;min_support&#39;: 5, &#39;user_based&#39;: False}} . SVDpp . # Matrix facotrization from surprise import SVDpp from surprise import Dataset from surprise.model_selection import GridSearchCV data = Dataset.load_from_df(ratings[[&quot;user_id&quot;, &quot;anime_id&quot;, &quot;rating&quot;]], reader) param_grid = { &quot;n_epochs&quot;: [5,10], &quot;lr_all&quot;: [0.002, 0.005], &quot;reg_all&quot;: [0.4,0.6] } gs = GridSearchCV(SVDpp, param_grid, measures=[&quot;rmse&quot;, &quot;mae&quot;], cv=3) gs.fit(data) print(gs.best_score[&quot;rmse&quot;]) print(gs.best_params[&quot;rmse&quot;]) print(gs.best_score[&quot;mae&quot;]) print(gs.best_params[&quot;mae&quot;]) # Cleaned #1.236439713814388 #{&#39;n_epochs&#39;: 10, &#39;lr_all&#39;: 0.005, &#39;reg_all&#39;: 0.4} #0.9482547980069017 #{&#39;n_epochs&#39;: 10, &#39;lr_all&#39;: 0.005, &#39;reg_all&#39;: 0.4} # Original #2.30757862188976 #{&#39;n_epochs&#39;: 10, &#39;lr_all&#39;: 0.005, &#39;reg_all&#39;: 0.4} #1.6811210573467745 #{&#39;n_epochs&#39;: 10, &#39;lr_all&#39;: 0.005, &#39;reg_all&#39;: 0.4} #runtime 27m 43s . 2.30757862188976 {&#39;n_epochs&#39;: 10, &#39;lr_all&#39;: 0.005, &#39;reg_all&#39;: 0.4} 1.6811210573467745 {&#39;n_epochs&#39;: 10, &#39;lr_all&#39;: 0.005, &#39;reg_all&#39;: 0.4} .",
            "url": "https://ylu-1258.github.io/YLu-Blog/jupyter/interests/2022/08/29/Summer-Research-ML-results.html",
            "relUrl": "/jupyter/interests/2022/08/29/Summer-Research-ML-results.html",
            "date": " • Aug 29, 2022"
        }
        
    
  
    
        ,"post24": {
            "title": "Data Abstraction in Python",
            "content": "Lists and Dictionaries . Data Abstraction . To properly manage our data while coding, we might require different types of data structures to successfully attain the goal of our program. Properly storing our data makes it easier for us to share information with fellow devs, across the internet, and also working in other technlogical fields. . The following are some comm ways to stored structured data in programing . Variables . Variables are contianers of data. They are the most primative and basic structure of data containing only a reference and a value, the value of the variable can be dynamic, and the value can be referenced to by the name of the variable. There are many primative data types in the python programing language, but a unique characteristic of python is that variables are given a data type at assignment. . Some primative data types are integers, floats, strings (array of chars), chars, etc. | msg = &quot;Hello World&quot; print(msg) . Hello World . Python Lists . Lists are sequential containers of data that could store multiple values per key. Lists in python are denoted by brackets. We can check that our list is of the list data type with the type() function . Always be precise and detailed when noting down variable names. . Each list has a length attribute that denotes how big or long the list is. in python, this is achieved via the len() function . langs = [&quot;cpp&quot;, &quot;python&quot;, &quot;html&quot;, &quot;css&quot;, &quot;java&quot;, &quot;javascript&quot;] print(langs) # Getting the data type print(type(langs)) # Printing the length print(len(langs)) # Printing the 3rd element print(langs[2]) . [&#39;cpp&#39;, &#39;python&#39;, &#39;html&#39;, &#39;css&#39;, &#39;java&#39;, &#39;javascript&#39;] &lt;class &#39;list&#39;&gt; 6 html . Dictionaries . Dictionaries are sequential containers of data that are similar to lists but contain pairs of keys and values. Dictionaries in python are denoted by curly braces. . We can access the keys of the dictionary with the .keys() method, and we can access the values of the dictionary with the .values() method. Additionally, we can also use the .get() method to get a value from a key. . Dictionaries are very similar to a JSON in how it stores its data. . info = { &quot;name&quot;: &quot;Alex Lu&quot;, &quot;age&quot;: 15, &quot;score&quot;: 3, &quot;langs&quot;: &quot;cpp&quot; } print(&quot;info: &quot;, info, &quot;type: &quot; + str(type(info)), &quot;Length: &quot; + str(len(info))) for key in info.keys(): print(key) print(&quot;&quot;) for val in info.values(): print(val) print(&quot; Now using .get()&quot;) for key in info.keys(): print(key + &quot;:&quot;, info.get(key)) . info: {&#39;name&#39;: &#39;Alex Lu&#39;, &#39;age&#39;: 15, &#39;score&#39;: 3, &#39;langs&#39;: &#39;cpp&#39;} type: &lt;class &#39;dict&#39;&gt; Length: 4 name age score langs Alex Lu 15 3 cpp Now using .get() name: Alex Lu age: 15 score: 3 langs: cpp . More Data Abstraction in Python . To add to a list, we can use the .append() method to add in more inputs to the list. I have modified the InfoDb List from class and added my own entries and values into the dictionary. . InfoDb = [] # InfoDB is a data structure with expected Keys and Values # Append to List a Dictionary of key/values related to a person InfoDb.append({ &quot;FirstName&quot;: &quot;John&quot;, &quot;LastName&quot;: &quot;Mortensen&quot;, &quot;Gender&quot;: &quot;Male&quot;, &quot;DOB&quot;: &quot;October 21&quot;, &quot;Residence&quot;: &quot;San Diego&quot;, &quot;Email&quot;: &quot;jmortensen@powayusd.com&quot;, &quot;Fav_Color&quot;: &quot;Unknown&quot;, &quot;Sleep_Schedule&quot; : &quot;Unknown&quot;, &quot;is_Teacher&quot;: True, &quot;Hobbies&quot;: [&quot;Teaching&quot;, &quot;Programming&quot;], &quot;Games&quot;: [] }) InfoDb.append({ &quot;FirstName&quot;: &quot;Alex&quot;, &quot;LastName&quot;: &quot;Lu&quot;, &quot;Gender&quot;: &quot;Male&quot;, &quot;DOB&quot;: &quot;November 29, 2006&quot;, &quot;Residence&quot;: &quot;San Diego&quot;, &quot;Email&quot;: &quot;maodou1258@gmail.com&quot;, &quot;Fav_Color&quot;: &quot;White&quot;, &quot;Sleep_Schedule&quot; : &quot;1am&quot;, &quot;is_Teacher&quot;: False, &quot;Hobbies&quot;: [&quot;Programming&quot;, &quot;Tennis&quot;, &quot;Reading&quot;, &quot;Sleeping&quot;], &quot;Games&quot;: [&quot;League of Legends&quot;, &quot;VALORANT&quot;, &quot;Minecraft&quot;, &quot;OSU&quot;] }) InfoDb.append({ &quot;FirstName&quot;: &quot;Evan&quot;, &quot;LastName&quot;: &quot;Aparri&quot;, &quot;Gender&quot;: &quot;Male&quot;, &quot;DOB&quot;: &quot;November 10, 2005&quot;, &quot;Residence&quot;: &quot;San Diego&quot;, &quot;Email&quot;: &quot;evanaparri@gmail.com&quot;, &quot;Fav_Color&quot;: &quot;Blue&quot;, &quot;Sleep_Schedule&quot; : &quot;2am&quot;, &quot;is_Teacher&quot;: False, &quot;Hobbies&quot;: [&quot;Running&quot;, &quot;Reading&quot;, &quot;Eating Asian Food/ Food Connoisseur&quot;, &quot;Sleeping&quot;, &quot;Homework&quot;, &quot;Programming&quot;], &quot;Games&quot;: [] }) # Print the data structure print(InfoDb) . [{&#39;FirstName&#39;: &#39;John&#39;, &#39;LastName&#39;: &#39;Mortensen&#39;, &#39;Gender&#39;: &#39;Male&#39;, &#39;DOB&#39;: &#39;October 21&#39;, &#39;Residence&#39;: &#39;San Diego&#39;, &#39;Email&#39;: &#39;jmortensen@powayusd.com&#39;, &#39;Fav_Color&#39;: &#39;Unknown&#39;, &#39;Hobbies&#39;: [&#39;Teaching&#39;, &#39;Programming&#39;], &#39;Sleep_Schedule&#39;: &#39;Unknown&#39;, &#39;Games&#39;: [], &#39;is_Teacher&#39;: True}, {&#39;FirstName&#39;: &#39;Alex&#39;, &#39;LastName&#39;: &#39;Lu&#39;, &#39;Gender&#39;: &#39;Male&#39;, &#39;DOB&#39;: &#39;November 29, 2006&#39;, &#39;Residence&#39;: &#39;San Diego&#39;, &#39;Email&#39;: &#39;maodou1258@gmail.com&#39;, &#39;Fav_Color&#39;: &#39;White&#39;, &#39;Hobbies&#39;: [&#39;Programming&#39;, &#39;Tennis&#39;, &#39;Reading&#39;, &#39;Sleeping&#39;], &#39;Games&#39;: [&#39;League of Legends&#39;, &#39;VALORANT&#39;, &#39;Minecraft&#39;, &#39;OSU&#39;], &#39;Sleep_Schedule&#39;: &#39;1am&#39;, &#39;is_Teacher&#39;: False}, {&#39;FirstName&#39;: &#39;Evan&#39;, &#39;LastName&#39;: &#39;Aparri&#39;, &#39;Gender&#39;: &#39;Male&#39;, &#39;DOB&#39;: &#39;November 10, 2005&#39;, &#39;Residence&#39;: &#39;San Diego&#39;, &#39;Email&#39;: &#39;evanaparri@gmail.com&#39;, &#39;Fav_Color&#39;: &#39;Blue&#39;, &#39;Hobbies&#39;: [&#39;Running&#39;, &#39;Reading&#39;, &#39;Eating Asian Food/ Food Connoisseur&#39;, &#39;Sleeping&#39;, &#39;Homework&#39;, &#39;Programming&#39;], &#39;Games&#39;: [], &#39;Sleep_Schedule&#39;: &#39;2am&#39;, &#39;is_Teacher&#39;: False}] . If we want to add values to InfoDb, this could easily be achived with the .append() method . def add_entry(): name = input(&quot;First and last name seperated by spaces: &quot;).split() gender = input(&quot;What is your gender? &quot;) birthdate = input(&quot;When were you born? &quot;) # Not gonna care about date time formatting cuz all programmers hate that residence = input(&quot;What city do you live in? &quot;) email = input(&quot;What is your email? &quot;) color = input(&quot;What is your favorite color? &quot;) sleep = input(&quot;When do you sleep? &quot;) is_teacher = bool_input(&quot;Are you a teacher? [yes/no] &quot;) hobbies = [] print(&quot;Now asking for Hobbies &quot;) hobbies = multi_input(hobbies) games = [] print(&quot;Now asking for Games &quot;) games = multi_input(games) entry = { &quot;FirstName&quot;: name[0], &quot;LastName&quot;: name[1], &quot;Gender&quot;: gender, &quot;DOB&quot;: birthdate, &quot;Residence&quot;: residence, &quot;Email&quot;: email, &quot;Fav_Color&quot;: color, &quot;Sleep_Schedule&quot;: sleep, &quot;is_Teacher&quot;: is_teacher, &quot;Hobbies&quot;: hobbies, &quot;Games&quot; : games } return entry def bool_input(prompt): while True: try: return {&quot;yes&quot;:True,&quot;no&quot;:False}[input(prompt).lower()] except KeyError: print(&quot;Invalid input please enter yes or no&quot;) def multi_input(arr): i = 0 temp = &quot;&quot; while temp != &quot;cancel&quot;: i+=1 temp = input(&quot;Please enter item number {0}, type cancel to proceede &quot;.format(str(i))) if temp.lower != &quot;cancel&quot;: arr.append(temp) return arr new_entry = add_entry() InfoDb.append(new_entry) print(InfoDb[-1]) . Now asking for Hobbies Now asking for Games {&#39;FirstName&#39;: &#39;Yixuan&#39;, &#39;LastName&#39;: &#39;Lu&#39;, &#39;Gender&#39;: &#39;Male&#39;, &#39;DOB&#39;: &#39;Nov 29&#39;, &#39;Residence&#39;: &#39;Shanghai&#39;, &#39;Email&#39;: &#39;maodou12581@gmail.com&#39;, &#39;Fav_Color&#39;: &#39;Black&#39;, &#39;Sleep_Schedule&#39;: &#39;3am&#39;, &#39;is_Teacher&#39;: False, &#39;Hobbies&#39;: [&#39;Programming&#39;, &#39;Soccer&#39;, &#39;cancel&#39;], &#39;Games&#39;: [&#39;Overwatch&#39;, &#39;Fortnite&#39;, &#39;Geometry Dash&#39;, &#39;cancel&#39;]} . Formatting and printing the data in the list . def print_data(d_rec): print(d_rec[&quot;FirstName&quot;], d_rec[&quot;LastName&quot;]) # using comma puts space between values print(&quot; t&quot;, &quot;Gender:&quot;, d_rec[&quot;Gender&quot;]) print(&quot; t&quot;, &quot;Residence:&quot;, d_rec[&quot;Residence&quot;]) # t is a tab indent print(&quot; t&quot;, &quot;Birth Day:&quot;, d_rec[&quot;DOB&quot;]) print(&quot; t&quot;, &quot;Email:&quot;, d_rec[&quot;Email&quot;]) print(&quot; t&quot;, &quot;Favorite Color:&quot;, d_rec[&quot;Fav_Color&quot;]) print(&quot; t&quot;, &quot;Is Teacher?:&quot;, d_rec[&quot;is_Teacher&quot;]) print(&quot; t&quot;, &quot;Hours of Sleep:&quot;, d_rec[&quot;Sleep_Schedule&quot;]) print(&quot; t&quot;, &quot;Hobbies: &quot; + &quot;, &quot;.join(d_rec[&quot;Hobbies&quot;])) print(&quot; t&quot;, &quot;Games: &quot; + &quot;, &quot;.join(d_rec[&quot;Games&quot;])) print() # for loop algorithm iterates on length of InfoDb def for_loop(): print(&quot;For loop output n&quot;) for record in InfoDb: print_data(record) for_loop() . For loop output John Mortensen Gender: Male Residence: San Diego Birth Day: October 21 Email: jmortensen@powayusd.com Favorite Color: Unknown Is Teacher?: True Hours of Sleep: Unknown Hobbies: Teaching, Programming Games: Alex Lu Gender: Male Residence: San Diego Birth Day: November 29, 2006 Email: maodou1258@gmail.com Favorite Color: White Is Teacher?: False Hours of Sleep: 1am Hobbies: Programming, Tennis, Reading, Sleeping Games: League of Legends, VALORANT, Minecraft, OSU Evan Aparri Gender: Male Residence: San Diego Birth Day: November 10, 2005 Email: evanaparri@gmail.com Favorite Color: Blue Is Teacher?: False Hours of Sleep: 2am Hobbies: Running, Reading, Eating Asian Food/ Food Connoisseur, Sleeping, Homework, Programming Games: Yixuan Lu Gender: Male Residence: Shanghai Birth Day: Nov 29 Email: maodou12581@gmail.com Favorite Color: Black Is Teacher?: False Hours of Sleep: 3am Hobbies: Programming, Soccer, cancel Games: Overwatch, Fortnite, Geometry Dash, cancel . Now we print the entires of InfoDb in reverse . The first method uses array slicing, while the second method iterates in reverse order starting from the last index of the array, this is accomplished through the reversed() function paired with the range() and len() functions. . def for_loop_reversed_1(arr): print(&quot;Cheesed Reverse For loop output:&quot;) for i in arr[::-1]: print_data(i) def for_loop_reversed_2(arr): for i in reversed(range(0,len(arr))): print_data(arr[i]) # for_loop_reversed_1(InfoDb) does the same thing for_loop_reversed_2(InfoDb) . Yixuan Lu Gender: Male Residence: Shanghai Birth Day: Nov 29 Email: maodou12581@gmail.com Favorite Color: Black Is Teacher?: False Hours of Sleep: 3am Hobbies: Programming, Soccer, cancel Games: Overwatch, Fortnite, Geometry Dash, cancel Evan Aparri Gender: Male Residence: San Diego Birth Day: November 10, 2005 Email: evanaparri@gmail.com Favorite Color: Blue Is Teacher?: False Hours of Sleep: 2am Hobbies: Running, Reading, Eating Asian Food/ Food Connoisseur, Sleeping, Homework, Programming Games: Alex Lu Gender: Male Residence: San Diego Birth Day: November 29, 2006 Email: maodou1258@gmail.com Favorite Color: White Is Teacher?: False Hours of Sleep: 1am Hobbies: Programming, Tennis, Reading, Sleeping Games: League of Legends, VALORANT, Minecraft, OSU John Mortensen Gender: Male Residence: San Diego Birth Day: October 21 Email: jmortensen@powayusd.com Favorite Color: Unknown Is Teacher?: True Hours of Sleep: Unknown Hobbies: Teaching, Programming Games: . Four methods of looping . In the methods below, I will use for-loops, for-loops with indices, while loops, and recursion to print all the data within InfoDB . For Loop . def for_loop(arr): print(&quot;Output of Database&quot;) # The for loop declares a local variable &quot;i&quot; and assigns each DB record to i over each iteration for i in arr: # Using the predermined print_data function to neatly format our data in a human friendly manner print_data(i) print(type(i)) # Note that &quot;i&quot; will retain its value outside of the for-loop, but not outside of the function for_loop(InfoDb) . Output of Database John Mortensen Gender: Male Residence: San Diego Birth Day: October 21 Email: jmortensen@powayusd.com Favorite Color: Unknown Is Teacher?: True Hours of Sleep: Unknown Hobbies: Teaching, Programming Games: Alex Lu Gender: Male Residence: San Diego Birth Day: November 29, 2006 Email: maodou1258@gmail.com Favorite Color: White Is Teacher?: False Hours of Sleep: 1am Hobbies: Programming, Tennis, Reading, Sleeping Games: League of Legends, VALORANT, Minecraft, OSU Evan Aparri Gender: Male Residence: San Diego Birth Day: November 10, 2005 Email: evanaparri@gmail.com Favorite Color: Blue Is Teacher?: False Hours of Sleep: 2am Hobbies: Running, Reading, Eating Asian Food/ Food Connoisseur, Sleeping, Homework, Programming Games: Yixuan Lu Gender: Male Residence: Shanghai Birth Day: Nov 29 Email: maodou12581@gmail.com Favorite Color: Black Is Teacher?: False Hours of Sleep: 3am Hobbies: Programming, Soccer, cancel Games: Overwatch, Fortnite, Geometry Dash, cancel &lt;class &#39;dict&#39;&gt; . For loop with index . def for_loop_with_index(arr): print(&quot;Output of Database&quot;) # Using the range function to print out data based on the index in the array for i in range(0,len(arr)): # Using index notation to print data print_data(arr[i]) for_loop_with_index(InfoDb) . Output of Database John Mortensen Gender: Male Residence: San Diego Birth Day: October 21 Email: jmortensen@powayusd.com Favorite Color: Unknown Is Teacher?: True Hours of Sleep: Unknown Hobbies: Teaching, Programming Games: Alex Lu Gender: Male Residence: San Diego Birth Day: November 29, 2006 Email: maodou1258@gmail.com Favorite Color: White Is Teacher?: False Hours of Sleep: 1am Hobbies: Programming, Tennis, Reading, Sleeping Games: League of Legends, VALORANT, Minecraft, OSU Evan Aparri Gender: Male Residence: San Diego Birth Day: November 10, 2005 Email: evanaparri@gmail.com Favorite Color: Blue Is Teacher?: False Hours of Sleep: 2am Hobbies: Running, Reading, Eating Asian Food/ Food Connoisseur, Sleeping, Homework, Programming Games: Yixuan Lu Gender: Male Residence: Shanghai Birth Day: Nov 29 Email: maodou12581@gmail.com Favorite Color: Black Is Teacher?: False Hours of Sleep: 3am Hobbies: Programming, Soccer, cancel Games: Overwatch, Fortnite, Geometry Dash, cancel . While Loop . def while_loop(arr): print(&quot;Output of Database&quot;) # Initializing a counter variable to keep track of the current index of InfoDb i = 0 # While loop to run certain processes before i becomes equal to the max index + 1, terminate once i is equal to len(arr) while i &lt; len(arr): print_data(arr[i]) i+=1 # I FORGOT TO DO THIS OH GOD WHY MY VSCODE IS BROKEN while_loop(InfoDb) . Output of Database John Mortensen Gender: Male Residence: San Diego Birth Day: October 21 Email: jmortensen@powayusd.com Favorite Color: Unknown Is Teacher?: True Hours of Sleep: Unknown Hobbies: Teaching, Programming Games: Alex Lu Gender: Male Residence: San Diego Birth Day: November 29, 2006 Email: maodou1258@gmail.com Favorite Color: White Is Teacher?: False Hours of Sleep: 1am Hobbies: Programming, Tennis, Reading, Sleeping Games: League of Legends, VALORANT, Minecraft, OSU Evan Aparri Gender: Male Residence: San Diego Birth Day: November 10, 2005 Email: evanaparri@gmail.com Favorite Color: Blue Is Teacher?: False Hours of Sleep: 2am Hobbies: Running, Reading, Eating Asian Food/ Food Connoisseur, Sleeping, Homework, Programming Games: Yixuan Lu Gender: Male Residence: Shanghai Birth Day: Nov 29 Email: maodou12581@gmail.com Favorite Color: Black Is Teacher?: False Hours of Sleep: 3am Hobbies: Programming, Soccer, cancel Games: Overwatch, Fortnite, Geometry Dash, cancel . Recursion . def recursion(arr, counter): # Check if the current index is within bounds if counter &lt; len(arr): # prints data print_data(arr[counter]) # Recalling the same function for counter+1 index, printing the next item in the list recursion(arr, counter+1) # If previous if statement does not pass, return 0 and terminate the recursion return 0 recursion(InfoDb, 0) . John Mortensen Gender: Male Residence: San Diego Birth Day: October 21 Email: jmortensen@powayusd.com Favorite Color: Unknown Is Teacher?: True Hours of Sleep: Unknown Hobbies: Teaching, Programming Games: Alex Lu Gender: Male Residence: San Diego Birth Day: November 29, 2006 Email: maodou1258@gmail.com Favorite Color: White Is Teacher?: False Hours of Sleep: 1am Hobbies: Programming, Tennis, Reading, Sleeping Games: League of Legends, VALORANT, Minecraft, OSU Evan Aparri Gender: Male Residence: San Diego Birth Day: November 10, 2005 Email: evanaparri@gmail.com Favorite Color: Blue Is Teacher?: False Hours of Sleep: 2am Hobbies: Running, Reading, Eating Asian Food/ Food Connoisseur, Sleeping, Homework, Programming Games: Yixuan Lu Gender: Male Residence: Shanghai Birth Day: Nov 29 Email: maodou12581@gmail.com Favorite Color: Black Is Teacher?: False Hours of Sleep: 3am Hobbies: Programming, Soccer, cancel Games: Overwatch, Fortnite, Geometry Dash, cancel . 0 . Other cool things to do with lists and Dictionaries . Some cool list operations including for loops and recursion in cluding reversing, list comprehension, and recursion . print(&quot;List comprehension, create a list on the spot&quot;) my_arr1 = [2&lt;&lt;i for i in range(0,10)] print(&quot;List of squares of 2: &quot;, my_arr1) print(&quot; nReversing a list&quot;) my_arr2 = [1,2,3,4,5] print(&quot;Original List: &quot;, my_arr2) print(&quot;Reversed List: &quot;, my_arr2[::-1]) print(&quot; nSorting a list&quot;) my_arr3 = [67,84,19,43,175,78] print(&quot;Sorted ascending: &quot;, sorted(my_arr3)) print(&quot;Sorted descending: &quot;, sorted(my_arr3, reverse=True)) print(&quot; nEXTRA: Qsort using list comprehension&quot;) def Qsort(arr): if arr == []: return [] pivot = arr[0] l = Qsort([i for i in arr[1:] if i &lt; pivot]) r = Qsort([i for i in arr[1:] if i &gt;= pivot]) return l + [pivot] + r print(Qsort(my_arr3)) . List comprehension, create a list on the spot List of squares of 2: [2, 4, 8, 16, 32, 64, 128, 256, 512, 1024] Reversing a list Original List: [1, 2, 3, 4, 5] Reversed List: [5, 4, 3, 2, 1] Sorting a list Sorted ascending: [19, 43, 67, 78, 84, 175] Sorted descending: [175, 84, 78, 67, 43, 19] EXTRA: Qsort using list comprehension [19, 43, 67, 78, 84, 175] . Creating a Quiz with Dictionaries . q_a = { &quot;What is the derivative of position?&quot; : [&quot;velocity&quot;], &quot;What is the integral of acceleration?&quot; : [&quot;velocity&quot;], &quot;Name a product of photosynthesis&quot; : [&quot;glucose&quot;, &quot;oxygen&quot;], &quot;What rhetorical device includes the repitition of a word or phrase at the end of clauses?&quot; : [&quot;epistrophe&quot;] } # Returns a tuple of lists of questions and values def get_pair(dict): return [q for q, a in dict.items()], [a for q, a in dict.items()] # Function to calculate percentage def percentage(x, y): return &#39;{0:.2f}&#39;.format(100 * float(x)/float(y)) # Some States correct = 0 total = len(q_a) # Get the answer + question pairs q_a_pairs = get_pair(q_a) # Ask and evaluate questions for i in range(0,len(q_a_pairs[0])): rsp = input(&quot;QUESTION: &quot; + q_a_pairs[0][i]) # Use .lower to maintain case insensitivity if rsp.lower() in q_a_pairs[1][i]: print(rsp + &quot; was correct!&quot;) correct+=1 else: print(&quot;Wrong, {0} was incorrect, the correct answer was: {1}&quot;.format(rsp, &quot;, &quot;.join(q_a_pairs[1][i]))) print(&quot;Congrats, you got {0}% on this quiz&quot;.format(percentage(correct,total))) . velocity was correct! Wrong, position was incorrect, the correct answer was: velocity oxygen was correct! EpiStroPhe was correct! Congrats, you got 75.00% on this quiz . It&#39;s just too free 😎 .",
            "url": "https://ylu-1258.github.io/YLu-Blog/jupyter/apcsp/week%202/2022/08/29/Lecture-Data-Abstraction.html",
            "relUrl": "/jupyter/apcsp/week%202/2022/08/29/Lecture-Data-Abstraction.html",
            "date": " • Aug 29, 2022"
        }
        
    
  
    
        ,"post25": {
            "title": "2022 08 29 Alumni Panel",
            "content": "Alumni Panel . Questions and answers from CS alumni panel. . Questions . Q: How did you realize whant you wanted to do as your major? . A: . Colein: Realized that one can code pretty well, other friends and family members suggested to learn compsci. Comp-sci is a competitive field that’s about perspective. | Anthony: Did programing from a young age, found it interesting and kept doing it. | Andrew: Took Intro to CS and really liked it and liked the techy and teamwork aspect of cs. | Maybel: Didn’t really like the cs class or subject, new that it was important to learn. Interned with northrop with their respective tasks. You are still going have to learn scripting and programming even if transitiong into another field. | Allison: Cognitive sciences really helped with the UI/UX field of CS, especially in the front-end dev part. | Nitya: Made own games and programs in free time and during summer. Took CSP and saw how she could apply cs to real world projects. | . Q: What were some challenges in comp sci . A: . Colein: Starting out sucks a lot, error messages and stuff made things annoying and hard to approach. Stuff on the internet will help you | Anthony: Clashing Egos, working together is hard because CS majors have a big ego, very frustrating. Follow your own curve and focus on your path. | Andrew: The theory of CS is hard and the curriculumn in college is not structured and introduced new topics that people didn’t really focus on. Different from high school computer science courses. | Maybel: Hard to find the motivation to CS, there is always someone better than you at CS. The robotics team had a lot of people that were good at CS, never had the motivation to work or study cs. Got really frustrated with the syntax of the problem. Realized that google was the best friend for syntax. | Allison: Hard time starting out with CS. The temptation to BS one’s work. Temptation to cut corners, must have a mindset to go above and beyond in order to truly succeed at CS. | Nitya: Getting started in a new area of computer science is ver yahrd and it’s daunting to try and get used to how you get started on coding. Got confused on where to start coding. | . Q: Are there any PBL scenarios in College . A: . Colein: Not that many project based learning scenarios in college, probably only 1. However, even if schools are theory based, projects are bound to show up in college. | Anthony: CS courses are bound to have projects. | Andrew: PBL is argely embedded with computer science, there are a lot of tests based on theory though. | Maybel: College in the first few years is just common education, the teacher does not have the time to answer all questions and not everyone can get 1 on 1 sessions. People struggle with this so hey used PBL to create study groups with one another. Some project classes are based on a lot of proejcts such as machine learning or artificial intellegence classes. Some people are going to freeload, but that’s okay, it’s an opportunity to learn. Always ask questions whie interning. | Allison: - | Nitya: - | . Q: How have you applied CS in your major if you are not majoring in CS? . Colein: A lot of menial tasks such as spreadsheet organizing and copy pasting could be sped up by a script. Coding is everywhere, mechanical students will use MATLAB and a lot of the computer science topics to help them with the work | Anthony: - | Andrew: Computer science can help in other fields that require mathematics, it can run many operations really fast. | Maybel: A lot of softwares may require certain stress tests. 3D printing could be simplified with CS and other algorithms.Makes stuff a lot faster in order to reduce manual labor. CS extends far beyond the CS major. A lot of people use jupyter notebooks for techincal papers for interactivity and such. Really nice for a research paper. | Allison: Psychology + computer science for Cog-sci. A lot of front-end programming with UI/UX. A lot of courses for stats also require a lot of calculations, programming certain programs could help with many tasks. It is inevitable. Psychology classes also have CS. | Nitya: - | . Q: For existing college students, have you had a job at your school? . Colein: Really important to create a resume for job applications. Push yourself to be above and beyond in order to be different from the other students. Visit office hours regularly. Reference yourself to the professor to think about how you are different rom the others. Becoming a TA is alsovery valuable and important. Sometimes you just have to figure it out! Visualized fight data, databases, web development, python automation, cyber security, etc. | Anthony: Code ninjas, franchise. | Andrew: Tutor at the college. Worked with a lot of tools such as APIs, web dev, javascript, html, and python. | Maybel: Robotics prfessor is a part time professor at UCSD that trains autonomous vehicals. Some professors have clubs and some have research clubs. As the professor if there’s any slots open in their labs and just pass them your resume. Interned at Northrop with Colein. Creating websites for flight data. | Allison: Had an internship with a church group, frontend UI dev. | Nitya: Northrop Grumman high school internship program. https://ngc.avature.net/events/Register?folderld=%2012369 &lt;—— note to self | . Q: Did you ever focus on personal projects along with school . Colein: In mechanical engineering, used computer science to mirror components to create mechanical parts. A lot of manipulation for commands. Simulations are useful, Computer Science can help with that. Set up servers to run the processes. | Anthony: Made a project for basketball teams. | Andrew: Companies will look for personal projects to see that you actually know what you are doing. Just 3 or 4 could go the extra mile of making the job. Good way to show off knowledge about algorithms and data structures. Pathfinding algorithms, graph theory, and also sorting algorithms. Data visualization is a really big field. | Maybel: Triton AI, improve autonomous tracking abilities. | Allison: - | Nitya: - | . Q: Clubs at Del Norte . Colein: CS in people managing, organizastion skills are very important | Anthony: | Andrew: | Maybel: School and robotics. | Allison: | Nitya: | . Q: Tips for internships and jobs . Colein: | Anthony: Do not look at other people’s linkedin, huge demoralizer. | Andrew: Build a good network of people in order to get refferals from others. Really important if you want to get into a FAANG company. Make a lot of connectons on LinkedIn. | Maybel: Have a resume, get a list of your achievements. Know when to find your internships, starts hiring from Sept-November, when most of the interns there are finishing their internships. Get a resume done by early september for summer internships. Have proof of work that you have done. Call a bunch of engineers to decide what to do. Personal projects and teams are important, differentiates you from other students. MORE THAN JUST GPA AND MAJOR. SKILLS &gt; NUMBERS. Ask local and smaller companies about internships, ask about high school internships and such. Find stuff you are interested in. Referrels are really important in the filtering process. | Allison: | Nitya: | .",
            "url": "https://ylu-1258.github.io/YLu-Blog/markdown/apcsp/week%202/2022/08/29/Alumni-panel.html",
            "relUrl": "/markdown/apcsp/week%202/2022/08/29/Alumni-panel.html",
            "date": " • Aug 29, 2022"
        }
        
    
  
    
        ,"post26": {
            "title": "An Empirical Study On Performance Measures Of Collaborative Filtering Recommendation Algorithms",
            "content": "An Empirical Study on Performance Measures of Collaborative Filtering Recommendation Algorithms . Alex Lu . maodou1258@gmail.com . Abstract . In recent times, recommendation engines have become increasingly popular within many industries. The focus of such an engine is to implement an algorithm to successfully make recommendations based on user preferences. Because of the utility provided by such tools, many industries heavily rely on such engines to provide accurate product recommendations to customers. Some popular algorithms implemented include clustering, matrix factorization, and deep learning implementations. Such algorithms are utilized within Collaborative Filtering methods and are simulated using pre-made datasets containing user and item information. Performance metrics are then applied to the algorithm to test the accuracy of the model. Results show that deep learning algorithms provide greater . performance when compared to other applications. . Keywords - Collaborative filtering, RMSE, Deep Learning, MAE, matrix factorization, Performance Measure . 1. Introduction . Latterly, recommendation engines serve a great purpose in many online services and enhance the consumer experience by providing lists of recommended items to users. Many big corporations such as Netflix, Amazon, Youtube, and others provide item recommendations through such methods to promote the sales and usage of their goods. The general idea of a recommender system is to return a list of items that the user would find interesting. Implementing various different types of engines provides insight towards which algorithm is the most suitable for certain scenarios. . Different approaches by different algorithms could have varying performances depending on the type of recommendation required, and the core nature of the dataset used. This research is primarily conducted on collaborative-filtering techniques, but similar research could be conducted on content-based implementations. Examples of potential implementations could be clustering and matrix factorization approaches. The main accuracy metrics utilized in the research are the RMSE and MAE metrics which are further explained later in the paper. . Two main research questions would serve as the focus of the research. More conclusions could be drawn from the data collected, but the main aim of the research are as follows: . I) What CF implementation provides the accuracy measure for a standard data set containing user, item and rating information? II) How does data sparsity affect the prediction accuracy of the models implemented? . 2. Background . Recommendation engines are programs used to provide item recommendations to users based on filtering each item and returning a . possible predicted rating for the user. The two main implementations of recommendation engines are content-based filtering and collaborative filtering. . 2.1 Content-based Filtering . A content-based filtering engine takes into account the user’s own item history and focuses on keywords in items rather than similarity between users [1]. However, such implementation can lead to a scenario where providing a broad range of recommendations would become impossible. Because of the nature of content-based filtering engines, items that were utilized by similar users wouldn’t be recommended to the main user purely because of the lack of a keyword or phrase. A dataset for such an engine would incorporate items along with a detailed profile for each item. . Despite its narrower scope when providing recommendations, content-based filtering systems often reduce the amount of data needed to make accurate predictions. For collaborative filtering, a greater range of data is required for a proper calculation of an item. To put it simply, the more data the engine has, the more . accurate the prediction is. However, with the nature of content-based filtering, any amount of data would suffice, providing that there are items that match the recommendation requirements. . 2.2 Collaborative Filtering . A collaborative filtering engine takes into account other user’s ratings, and returns . algorithms. This paper would primarily focus on model-based algorithms and approaches. The similarity between users could be found in various methods. For this study, a total of twelve algorithms were implemented. The main focus was placed on nearest neighbors, matrix factorization, and deep learning algorithms. Figure 2.1 maps out the various . . recommendations based on similarity. Collaborative filtering is split into two main approaches, model-based and memory based . algorithms implemented in this research. 2.3 Nearest Neighbor . Nearest neighbor, or more specifically, k nearest neighbors is a collaborative filtering . algorithm used to find similarities between items and users based on the total distance between two neighbors [2]. A weight system could also be applied so that a neighbor closer to the main user would have more weight on the recommendations than one who is further away. . Four of the twelve algorithms used in the research were from this category. KNNBasic, KNNWithZScore, KNNWithMeans, and KNNBaseline were such implementations of this category of algorithms. . 2.4 Matrix Factorization . Matrix Factorization is yet again another implementation of collaborative filtering. Simply put, a matrix factorization relates two separate values together under a specific value to create a grid or matrix of the data [3]. However, as data is not evenly distributed, some cells in the matrix are left empty and would need to be filled in to provide recommendations. Such values are known as “latent features”. . Some matrix factorizations implemented in the research were Negative Matrix Factorization, Singular Value Decomposition, and SVD++. . 2.5 Deep Learning . A deep learning model utilizes a neural network to process and calculate information. Much like a human brain, deep learning attempts to make predictions based on data much like a human brain. . The deep learning model implemented in this experiment utilizes an embedding layer structure [4]. Using such a structure could organize data into a vector of discrete values which could then produce similarity results and tests through the distance between vectors. These types of embedding layers could be generated through frameworks such as “Pytorch” or “Tensorflow”, however, this research would be implemented in Pytorch. The model could then be trained through a series of epochs and eventually provide predictions. . 2.6 Miscellaneous surprise algorithms . The miscellaneous category comprises the leftover algorithms in the surprise package that do not fit into any of the other categories in the research. Such algorithms were as listed: Co . Clustering, Normal Predictor, Baseline-Only, and Slope-One. . According to Sahar Karat Co-Clustering is a collaborative filtering algorithm used to provide recommendations by “A simultaneous clustering of the rows and columns of a matrix” [5]. Classical clustering algorithms only focus on one specific type of data, while co . clustering could be used to accommodate two simultaneously. . Normal predictor is an algorithm provided by the surprise package that provides random user recommendations based on the data distribution in the dataset. The “Maximum Likelihood Estimation” method is utilized in this calculation. . Baseline-Only is yet another surprise algorithm that makes baseline predictions based on the data provided. There are two main ways to implement this algorithm. The first implementation method is “Stochastic Gradient . Descent” (SGD), which calculates a gradient of the dataset by a random selection of data. The other implementation is to use “Alternating Least Squares” (ALS) which is a matrix factorization algorithm that works well with sparser sets of data. . Slope-One is an example of an item-based collaborative filtering recommendation algorithm. Predictions made with this model are generally based on personal ratings as well as similar community ratings. . Further information and implementations on the methods listed could be found on the surprise documentations [6]. . 2.7 Accuracy Metrics . Two accuracy metrics were used in the process of this research. Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE) [11]. . RMSE is a quadratic model that assigns much larger weights to larger errors observed. Accuracy is calculated through RMSE by taking the square root of the mean of the sum of the squared difference of each predicted . value and its corresponding actual value. The formula is displayed in Figure 2.2 . . (Figure 2.2) . MAE is a linear metric model that linearly scales error. This model is typically used when higher errors are not important to the observation. Accuracy is calculated through MAE by taking the mean of the sum of the absolute values of the differences between the predicted and actual values. The formula is displayed in Figure 2.3 . . (Figure 2.3) . 3. Related Work . Various research topics have already been analyzed in the field of collaborative filtering. A few similar pieces of research provide similar methods and implementations. A plethora of CF models has been implemented and researched. . 3.1 Similar Research . Similar research was conducted by Mojdeh Saadati et al. [7] on the implementations of various models on movie recommender systems. The two models used in the experiment were the matrix factorization model SVD and a deep learning implementation of the Restricted Boltzmann Machine. The performance of the implementations was measured in the RMSE metric. However, in this research, we go over a broader spectrum of algorithms from each group of implementations and produce results for a larger picture between the different implementations. . 3.2 Further Research . He et al. [8] Conducted Research on how Matrix Factorization could be implemented with Deep Learning to create a better performing model for collaborative filtering. The proposed idea was to use two main models, the Generalized Matrix Factorization (GMF), and the Multi-Layer Perceptron (MLP) to create a hybrid between deep learning and matrix factorization implementations to create a new neural matrix factorization model . dubbed “NeuMF”. NeuMf was then compared to other well-known models such as KNN and ALS implementations and proved to have better accuracy and less training loss. . Yedder et al. [9] Researched the performance of the restricted Boltzmann machine. Various hidden units, learning rates, and other factors were incorporated into the research. Problems such as data sparsity were also encountered in the research and required other methods of implementation. This research also utilized the RMSE accuracy metric to rate the performance of the model. The research indicated an excellent RMSE measure of 0.46. . 4. Approach . This study was broken up into three parts, cleaning, implementation, and evaluation. We will first check for invalid values throughout the dataset, then begin to implement algorithms, and finally calculate performance values. . 4.1 Dataset . The data that would be primarily used in this study is the Kaggle dataset “Anime . Recommendations Database” [10] based on the data collected from myanimelist.net and is included in the reference section of the study. The two files contained within the database are the “ratings.csv” and “anime.csv”. The research would mostly work with the ratings file, but the anime dataset could be further implemented in future research incorporating variables such as show genre and overall ratings. The anime dataset is separated into three separate columns. The user_id, anime_id, and ratings columns are provided for analyzing similarities between users. The format and example of the file are illustrated in Figure 4.1. . (Figure 4.1) . The python module “pandas” was used to import the data into a data frame which could . then be cleaned and analyzed. Making a copy of the dataset to work on is recommended. Maintaining two different data frames would help compare the results on the original to the results in the cleaned dataset. . The first thing to take note of while cleaning is to remove all rows with a rating of “-1”. Such rows represent user data that does not have a specified rating for the specific show that they watched. This step is crucial as having such data in the project would result in a sparse dataset, which directly decreases performance as shown later in the results. A total of 7.8 million rows of data were in the dataset, after cleaning, about 6.3 million rows remained. The distribution of this data is shown in figure 4.2. . . (Figure 4.2) . Another issue arose again while trying to apply the dataset to our engine. Certain cells in . our dataset possibly had ‘NaN’ values, which would in turn cause an error in actual predictions where a value of ‘NaN’ would be returned instead of an integer of the predicted rating. To resolve this issue, we first applied the ‘to_numeric’ method from the pandas module to convert each value into workable integer or float values. After, we could then finally lower the runtime of our experiment by decreasing the size of our dataset from 6 million down to 125,000. This number was picked for easier splitting while implementing other algorithms and deep learning which require testing and training datasets. From there, we then calculated the distribution of the ratings by counting the number of each rating in our dataset. After clearing, it’s paramount to re index the rows in the data frame. Removing the invalid A distribution of the ratings is demonstrated in Figure 4.3. . . (Figure 4.3) . A correlation heatmap could also be used to detect any possible correlations between the values given, however, as of now, there is no apparent relation. . . (Figure 4.4) . 4.2 Implementation . The python package scikit-surprise contains most of the algorithms implemented in this research. First and foremost, we must declare a rating scale for our dataset by utilizing the reader class, and also initializing our dataset with the Dataset class. We set a new . “ratings“ object over the range of one to ten, which is represented in our data. Using the three columns from our pandas data frame, we could then load the data into surprise using the Dataset.load_from_df() method. . After the data is loaded, we could then specify a “param_list” dictionary. In the dictionary, the name of the specific parameters would be stored as the keys and the possible values as a list in the values. Further explanation of what specific params do for each algorithm is also provided in the surprise documentation. . The param_list dictionary could then be imputed into the GridSearchCV() method along with the algorithm name, performance measures, and a cross-validation iterator of 3. There are other arguments as well, but for the sake of our research, these four should be enough. The final step is to then fit our premade data with our GridSearchCV method with the .fit() function. Once everything is set up, the accuracy metrics could then just be retrieved via the best_score iterator. . The deep learning algorithm specifically was generated using an embedding layer with . dropout layers. The entire network consists of 4 total layers. After setting up the net, we can begin to train our model and loop over varying amounts of epochs to find the accuracy that is desired. . To determine the RMSE and the MAE values of this approach, we could separate our dataset into predictions and truth arrays. We can then apply the formulas for both RMSE and MAE as shown in Figures 2.2 and 2.3. We can implement these metrics in two ways. The first method is to use a NumPy array to subject each vector of values from each other and then to apply the formula to our newly generated values. Our second approach is more basic and rudimentary. We could subtract each truth value from each prediction value in our function by declaring a function and then apply the formula to our sum. . 5. Results . In this research, two sets of results were collected. The data collected on the cleaned dataset would serve to be our solution to the first question. However, the data collected on the original dataset would be utilized to answer . the second question concerning the performance measures on sparse datasets. 5.1 Cleaned Dataset . We have gathered both RMSE and MAE values for each algorithm used on our cleaned dataset from our implementations. In the first part of this experiment, the data set has already been cleaned of any invalid values and has reduced sparsity. The majority of the algorithms implemented all showed similar results except a couple of outliers and certain points. A table of the data collected is shown in Figure 5.1 . . (Figure 5.1) . Using the RMSE as the x-axis and MAE for the y-axis, we can plot a scatter plot of our data (Figure 5.2). . . (Figure 5.2) . Removing the Normal Predictor outlier value provides us with a clearer image of the differences in the lower valued points (Figure 5.3). . . (Figure 5.3) . Our data shows that the worst-performing algorithm that we had implemented was the Normal Predictor algorithm from the surprise package, while the best-performing implementation was the Deep Learning algorithm based on neural networks. . 5.1 Pre-cleaned Dataset . Loading up a new dataset, we can effectively run all our algorithms again while maintaining the sparsity of the original dataset. The only cleaning that had to be done was to remove all NaN values and also convert all data to numeric values. The results of running the . implementations on the sparser dataset are recorded in the figure below (Figure 4.5) . . (Figure 5.4) . The results of the various algorithms on this sparse dataset were also plotted on a scatter plot as shown below in Figure 5.5. . . (Figure 5.5) . Once again, removing the normal predictor outlier gives a clearer representation of our other data points (Figure 5.6). . . (Figure 5.6) . Although the values have increased by a considerable amount in the sparse dataset, the common trend remains between the various data points, and no changes are observed in the best and worst algorithms. Comparing the . results of the cleaned and original data sets, a clear difference could be observed in the RMSE and MAE measures (Figures 5.7-8) . (Figure 5.7) . (Figure 5.8) . 6. Discussion . After analyzing the data collected from the research, the deep learning algorithm and the KNNBaseline implementations were observed to be the best performing with the least error observed with both RMSE and MAE metrics. . A possible reason for the results could be the usage of randomness in the calculation of the Normal Predictor algorithm. The deep learning algorithm may have performed the best because of the various training cycles allocated to it which helped to create a more accurate model after each iteration. Vice versa, the opposite could also be applied to the two worst performing algorithms KNNBasic and Normal Predictor. Such implementations . had basic calculations and weren’t able to take into account outliers and other potential biases in the data. . There was an attempt in the research to implement a Restricted Boltzmann Machine (RBM) model, however, the implementation gave varying results and was difficult to judge the extra ratings of the implementation. This . research could be further pursued in the future with the addition of more deep learning implementations and a narrower focus on the subject of deep learning as a whole. From the results acquired, deep learning has been shown to have improved results compared to other algorithms. Focused research on deep learning implementations would provide the reasoning behind deep learning accuracy. . 7. Conclusion . In this research, we explored various machine learning algorithms, K-Nearest neighbors, Matrix Factorization, Deep learning, etc. Several approaches were implemented from the categories mentioned then tested for accuracy measures. . In both the cleaned and original datasets, the deep learning implementation was shown to the least margin of error when making recommendations. From this, it could be deduced that deep learning is a viable method for collaborative filtering engines working with user, item and rating data. Although different sets of data have varying optimal algorithms, . deep learning was still shown to be extremely accurate compared to other tested algorithms. Analyzing the data collected from the sparse dataset, we can conclude that a sparser set of data would result in less accurate recommendations, sometimes up to double the margin of error observed. Because of this observation, it can be concluded that collaborative filtering best performs with dense datasets. . References . [1] Kirzhner, Elena. “Machine Learning. Explanation of Collaborative . Filtering vs Content Based . Filtering.” Medium, Codeburst, 11 . May 2018, codeburst.io/explanation of-recommender-systems-in . information-retrieval-13077e1d916c. [2] Harrison, Onel. “Machine Learning Basics with the K-Nearest Neighbors Algorithm.” Medium, Towards Data Science, 14 July 2019, . towardsdatascience.com/machine . learning-basics-with-the-k-nearest . neighbors-algorithm-6a6e71d01761. . [3] Chen, Denise. “Recommendation System - Matrix Factorization.” . Medium, Towards Data Science, 9 . July 2020, . towardsdatascience.com/recommend ation-system-matrix-factorization . d61978660b4b. . [4] Sivanantham, Balavivek. . “Recommendation System Implementation With Deep Learning and PyTorch.” Medium, The Startup, 18 Aug. 2020, . medium.com/swlh/recommendation system-implementation-with-deep learning-and-pytorch-a03ee84a96f4. . [5] Karat, Sahar. “Co . Clustering.” Data Science . Made Simpler, 5 Mar. 2016, datasciencemadesimpler.wor dpress.com/tag/co . clustering/. . [6] Hug, Nicholas. “Welcome to Surprise’ Documentation.” Welcome to Surprise’ . Documentation! - Surprise 1 Documentation, 2015, . surprise.readthedocs.io/en/sta ble/. . [7] Mojdeh Saadati, Syed Shihab, Mohammed Shaiqur Rahman “Movie Recommender Systems: Implementation and Performance . Evaluation.” Semantic Scholar, 2019, . www.semanticscholar.org/paper/Mo vie-Recommender-Systems%3A Implementation-and-Saadati Shihab/01470f39285213e53f365ce0 1417b18d12467563#citing-papers. . [8] Xiangnan, He, et al. “Neural Collaborative Filtering.” . International World Wide Web Conference Committee, 3 Apr. 2017. . [9] Yedder, Hanene Ben, et al. “Modeling Prediction in . Recommender Systems Using Restricted Boltzmann Machine.” . IEEE Explore, IEEE, 5 Oct. 2017, ieeexplore.ieee.org/abstract/documen t/8122923. . [10] CooperUnion. “Anime . Recommendations Database.” . Kaggle, Kaggle, 21 Dec. 2016, . www.kaggle.com/CooperUnion/ani me-recommendations-database. . [11] Kampakis, Stylianos. “Performance Measures: RMSE and MAE.” The Data Scientist, The Data Scientist, 26 Nov. 2020, . thedatascientist.com/performance measures-rmse-mae/. .",
            "url": "https://ylu-1258.github.io/YLu-Blog/2022/08/28/An-Empirical-Study-on-Performance-Measures-of-Collaborative-Filtering-Recommendation-Algorithms.html",
            "relUrl": "/2022/08/28/An-Empirical-Study-on-Performance-Measures-of-Collaborative-Filtering-Recommendation-Algorithms.html",
            "date": " • Aug 28, 2022"
        }
        
    
  
    
        ,"post27": {
            "title": "Alex Lu   Lab #1_ Measurements And Graphical Analysis",
            "content": "Lab #1: Measurements and Graphical Analysis . Alex Lu . Purpose: . Given disks of different radii, determine the relationship between the mass and the radius of the disks through graphical method and calculate the uncertainty associated with the measured value. We will learn about linearization and use it to create a mathematical model. . Materials and Equipment: . Balance . Meter stick . Circular disks (identical thickness and uniform density but different radii) . Graphic Calculator or online graphing tool . Procedure: . Substitute equations to get a relationship between mass and radius. . | Use the relationship between mass and radius to determine what variable should be processed to linearize the data . | Measure the radius (in cm) of the cylinder with the ruler with one end of the ruler at the center of the disk and the other on the edge. . | Zero out the balance and then measure the mass (in grams) of the cylinder. . | Stack the metal cylinders and measure the collective height of all cylinders. Divide by the total number of cylinders multiplied by 2 to the power of the number of folds to find the height of each individual cylinder. . | Organize the collected data into a data table . | Plot the original radius and mass . | Plot the processed radius and mass . | Plug height and other constants to create a relationship between mass and radius. . | Equations and calculations: . Let: . ( rho) be density . | (m) be mass . | (v) be volume . | (a) be the surface area of the cylinder . | (r) be the radii of the cylinder . | (h) be the height of the cylinder . | . Thus: . If ( rho = frac{m}{v}) and (v = ah) then . (m = rho v) . (m = rho text{ah}) . If (a = pi r^{2}), then . (m = rho pi r^{2}h) . Since both ( rho) and ( pi) are constants and (h) is negligible, ∴ (m propto r^{2}) . Since mass is directly proportional to radius squared, the graph could be linearized if (r^{2}) is plotted instead. . Precision and Uncertainty . The balanced used to measure the mass of the metal disks was accurate up to a hundredth of a gram, while the ruler used to measure the radius and height of the disks was accurate up to 1 millimeter (0.1 cm). As such, the radius was measured to the nearest hundredth of a centimeter or a tenth of a millimeter. . | Data Tables . m = mass . r = radii . H = height . Mass in Grams, Radius and height in cm, and Radius Squared of Each Metal Disk . Disk m (g) h (cm) r (cm)   r^2 (cm^2) . 1 | 0.07 | 0.0025 | 2.30 |   | 5.29 | . 2 | 0.14 | 0.0025 | 3.00 |   | 9.0 | . 3 | 0.25 | 0.0025 | 4.30 |   | 18.49 | . 4 | 0.37 | 0.0025 | 4.90 |   | 24.01 | . 5 | 0.73 | 0.0025 | 7.10 |   | 50.41 | . Graph of Non-Linearized Data: . . Equation: (y = 0.0133921x^{2} + 0.112825x - 0.0238674) . R2 : (0.9967) . This graph has non-linear data as it’s represented by a quadratic model. . Graph of Linearized Data: . . Line of the best fit equation: (y = 0.0145569x - 0.000100527) . r = (0.9983) . r2 = (0.9966 ) . This graph has linear data as it’s represented by a linear model. . Analysis Questions: . 1) What is the independent variable in your y = mx +b formula? . Considering the mathematic relationship between disk radius r and disk mass m, the independent variable from my line of best fit represents the radius of the disk squared. . 2) What does the slope represent in your y = mx +b formula? Show dimensionally that indeed that is what your slope represents and that the formula is valid dimensionally. . Using the (m = rho v) equation and the volume relationship, we can express the disk’s mass in terms of the radius and the height in this relationship: (m = rho pi r^{2}h). Since density (( rho)) and ( pi) are constants, and height ((h)) is negligible, we can essentially group these 3 constants into one value serving as the coefficient to the radius squared term. Thus, the slope is the product of the density, height, and ( pi). Since height has its units expressed in cm, density has units expressed in g/cm³, and ( pi) is a constant wth no units, the slope is a combination of all 3 constants and has units of g/cm2. . Because our slope has units of g/cm2, and x represents the radius squared expressed by cm2, and since our desired output ((y)) is mass in grams (g), the y-intercept ((b)) must have a unit of grams (g) in order for both sides of the equation to have the same units (grams). . (g = frac{g}{cm^{2}} cdot cm^{2} + g) . (g = g + g) . (g = g) . 3) Should the “b” in your y = mx + b formula be zero? Explain your answer. . The y-intercept ((b)) in my (y = mx + b) formula shouldn’t be zero unless the data naturally generates a line of best fit that passes through the origin. Forcing the y-intercept to pass through the origin would either alter the slope ((m)) or remove the y-intercept completely. This would result in an inaccurate interpretation of the model as the line is no longer set evenly between the data points. Thus we cannot guarantee the model’s integrity for future data points, rendering our model useless. . 4) Measure/estimate the “thickness” of your cylinders. Use that value to find the experimental density of your cylinders. Find a percent difference between your found density and the actual density. The actual material is aluminum. . (y = 0.0145569x - 0.000100527) . (slope = pi rho h) . ( rho = density) . ( rho = frac{ text{slope}}{h pi} = frac{0.0145569 frac{g}{cm^{2}}}{ pi(0.0025cm)} = 1.85 frac{g}{cm^{3}}) . The density of the metal disks according to my linear model is (1.85 frac{g}{cm^{3}}) , Since the metal used in the disk is Aluminum (density = (2.70 frac{g}{cm^{3}})) we can use the following calculations to determine our percent error . ( % Error = left | frac{Actual - Expected}{ text{Expected}} right | *100 % ) | . ( % Error = left | frac{1.85 frac{g}{cm^{3}} - 2.70 frac{g}{cm^{3}}}{2.70 frac{g}{cm^{3}}} right | *100 % = 31.5 % ) | . ∴ Percent difference = 31.5% . 5) Errors. Make sure you explain why your number is bigger or smaller than (if positive or negative difference.) . The number that I obtained is smaller than the actual value (31.5% error) because the instruments that we used to measure the disks’ dimensions had inaccuracies. The ruler that we used to measure the disks’ radius is only precise up to millimeters, and the balanced used to measure mass is only accurate up to the hundredth’s place, while actual instruments used to measure the actual value are much more precise. Additionally, the method I used to measure the height of the disks also had inherent inaccuracies, as folding the aluminum disks over each other may create tiny air pockets that add additional magnitude to my measured height, resulting in an inaccurate height measurement. The uncertainty of the ruler and also the inaccurate way of measuring height may have contributed to a greater height value than the actual value, resulting in a lower density value when I divided. Lastly, the disks weren’t in a perfectly circular shape, thus the direction that I measured the disk in might also have affected the end result, as measuring in different directions would produce different radii. . Synthesis Questions: . 1) In this experiment, if we had used disks with a greater thickness, would the slope of your best fit line have been different? Would your experimental value for density be the same? Explain. . Since slope represents the product of thickness, ( pi), and density, having a greater measured thickness would result in a much larger slope value. However, our experimental value of density would still be around the same, as an increase in thickness would also result in a proportionally large increase in mass, which will offset the thickness increase. . 2. How would your graph of m versus r2 be different if you had used disks of the same . thickness but made out of steel? Draw a second line on your m versus r2 plot that . represents disks made of steel. . The graph would be of m versus r2 would be different in that the slope of the graph would be much greater, this is because steel has a much higher density of (7.85 frac{g}{cm^{3}}) compared to aluminum’s (2.7 frac{g}{cm^{3}}) . . (Note: The blue line is a rough sketch of what the line of best fit would look like on the same scale as the aluminum graph is steel was used instead. This is not an accurate representation, just an approximation) . 3. Another group of students has acquired data for the exact same experiment; however, their disks are made of an unknown material that they are trying to determine. The group’s m versus r2 data produced a line of best fit with slope equal to 122 kg/m2. Each disk they measured had the same 0.5 cm thickness. Calculate the density of the unknown material and use the table below to help determine what material their disks are made of. . Work: . (Slope = 122kg/m^{2} , h = 0.5cm, slope = rho pi h) . (slope = 122 frac{ text{kg}}{m^{2}}( frac{1000g}{1kg})( frac{1m}{100cm})^{2} = 12.2 frac{g}{cm^{2}} = rho pi h) . ( rho = frac{ text{slope}}{ pi h} = frac{12.2 frac{g}{cm^{2}}}{ pi*0.5cm} = 7.77 frac{g}{cm^{3}}) . The closet material to a density of (7.77 frac{g}{cm^{3}}) is iron, which has a density of (7.8 frac{g}{cm^{3}}). The disks are most likely made out of iron. . Multiple Choice Questions: . 1. You perform the same experiment, but this time you plot a linear relationship between mass and the circumference of the disks rather than the radius. What is the slope of the linear plot? . Work: . (slope = rho pi h) . (m = rho v = rho pi r^{2}h) . (v = pi r^{2}h) . Let (c) be circumference . (c = 2 pi r) . (c^{2} = 4 pi^{2}r^{2}) . (r^{2} = frac{c^{2}}{4 pi^{2}}) . (m = rho pi h( frac{c^{2}}{4 pi^{2}}) = frac{ rho hc^{2}}{4 pi} = ( frac{ rho h}{4 pi})c^{2}) . () . Since ( rho) (density), h (height), and (4 pi) are all constants, these 3 values could be combined to form the slope for the linear plot, which is (( frac{ text{ph}}{4 pi})), Hence, E is the correct answer choice. . 2. Skipped . 3. Consider an experiment in which a student measures the mass and diameter of 10 . different-sized spheres, all made of the same material of uniform density ρ. For this . student to create a linear graph relating the mass of the sphere to its radius r, the . student would need to plot mass m versus which quantity: . ( text{let v be the volume of a sphere}) . (v = frac{4}{3} pi r^{3}) . (m = pv = ( frac{4p pi}{3})r^{3}) . Since (( frac{4p pi}{3})) is a constant, the student must plot a linear graph relating m versus the quality of radius cubed. Hence, option C is correct. .",
            "url": "https://ylu-1258.github.io/YLu-Blog/2022/08/28/Alex-Lu-Lab-1_-Measurements-and-Graphical-Analysis.html",
            "relUrl": "/2022/08/28/Alex-Lu-Lab-1_-Measurements-and-Graphical-Analysis.html",
            "date": " • Aug 28, 2022"
        }
        
    
  
    
        ,"post28": {
            "title": "Installation Checks for Alex Lu",
            "content": "Defining color vars . RED=&#39; 033[0;31m&#39; GREEN=&#39; 033[0;32m&#39; BLUE=&#39; 033[0;34m&#39; MAGENTA=&#39; 033[0;35m&#39; CYAN=&#39; 033[0;36m&#39; NC=&#39; 033[0;0m&#39; . Checking for python installation . function CheckPythonInstall() { echo -e &quot;${BLUE}Checking python version${NC}&quot; if [[ $(python --version) ]]; then pyversion=$(python --version | cut -d&quot; &quot; -f2) echo -e &quot;${GREEN} - Python version ${MAGENTA}$pyversion${GREEN} has been installed! ${NC}&quot; else echo -e &quot;${RED} - Python not found ${NC}&quot; fi } CheckPythonInstall #python --version #python3 --version . Checking python version - Python version 3.9.12 has been installed! . Checking for Java intsallation . function CheckJavaInstall() { echo -e &quot;${CYAN}Checking java version${NC}&quot; if [[ $(java --version) ]]; then javaversion=$(java --version | head -n 1 | cut -d&quot; &quot; -f2) echo -e &quot;${GREEN} - Java version ${MAGENTA}$javaversion${GREEN} has been installed! ${NC}&quot; else echo -e &quot;${RED} - Java not found ${NC}&quot; fi } function CheckJavaCInstall() { echo -e &quot;${CYAN}Checking java compiler version${NC}&quot; if [[ $(javac --version) ]]; then javacversion=$(javac --version | head -n 1 | cut -d&quot; &quot; -f2) echo -e &quot;${GREEN} - JavaC version ${MAGENTA}$javacversion${GREEN} has been installed! ${NC}&quot; else echo -e &quot;${RED} - JavaC not found ${NC}&quot; fi } CheckJavaInstall CheckJavaCInstall . Checking java version - Java version 11.0.16 has been installed! Checking java compiler version - JavaC version 11.0.16 has been installed! . Checking For Anaconda installation . function CheckJupyterInstall() { echo -e &quot;${CYAN}Checking anaconda version${NC}&quot; if [[ $(conda --version) ]]; then condaversion=$(conda --version | cut -d&quot; &quot; -f2) echo -e &quot;${GREEN} - Anaconda version ${MAGENTA}$condaversion${GREEN} has been installed! ${NC}&quot; else echo -e &quot;${RED} - Anaconda not found ${NC}&quot; fi } CheckJupyterInstall . Checking jupyter version - Anaconda version 4.13.0 has been installed! . Checking for Anaconda package installation . function CheckCondaPackageInstall() { echo -e &quot;${CYAN}Checking jupyter package version${NC}&quot; if [[ $(conda list | grep $1) ]]; then packageversion=$(conda list | grep $1 | awk &#39;{print $2}&#39;) echo -e &quot;${GREEN} - Conda pacakge ${MAGENTA}$1${GREEN} version ${MAGENTA}$packageversion${GREEN} has been installed! ${NC}&quot; else echo -e &quot;${RED} - Conda package $1 not found ${NC}&quot; fi } CheckCondaPackageInstall &quot;nodejs&quot; . Checking jupyter package version - Conda pacakge nodejs version 6.11.2 has been installed! . Checking installed Jupyter kernels . function CheckJupyterKernelInstall() { echo -e &quot;${CYAN}Checking jupyter ${MAGENTA}$1${CYAN} kernel installation${NC}&quot; if [[ $(jupyter kernelspec list | grep $1) ]]; then kernelpath=$(jupyter kernelspec list | grep $1 | awk &#39;{print $2}&#39;) echo -e &quot;${GREEN} - Jupyter ${MAGENTA}$1${GREEN} kernel has been found at ${MAGENTA}$kernelpath${NC}&quot; else echo -e &quot;${RED} - Jupyter ${MAGENTA}$1${GREEN} kernel not found ${NC}&quot; fi } kernels=&quot;bash javascript python3&quot; for i in $kernels; do CheckJupyterKernelInstall $i done . Checking jupyter bash kernel installation - Jupyter bash kernel has been found at /home/eris29/.local/share/jupyter/kernels/bash Checking jupyter javascript kernel installation - Jupyter javascript kernel has been found at /home/eris29/.local/share/jupyter/kernels/javascript Checking jupyter python3 kernel installation - Jupyter python3 kernel has been found at /home/eris29/.local/share/jupyter/kernels/python3 .",
            "url": "https://ylu-1258.github.io/YLu-Blog/bash_checks",
            "relUrl": "/bash_checks",
            "date": " • Aug 26, 2022"
        }
        
    
  
    
        ,"post29": {
            "title": "Intro to Pandas",
            "content": "Pandas . I will use a dataset of Fortune 1000 from Kaggle datasets to explore pandas library here. . The Fortune 1000 dataset is from the Fortune website. It contains U.S. company data for the year 2021. The dataset is 1000 rows and 18 columns. . Features: . Company - values are the name of the company Rank - The 2021 rank established by Fortune (1-1000) | Rank Change - The change in the rank from 2020 to 2021. There is only a rank change listed if the company is currently in the top 500 and was previously in the top 500. | Revenue - Revenue of each company in millions. This is the criteria used to rank each company. | Profit - Profit of each company in millions. Num. of Employees - The number of employees each company employs. | Sector - The sector of the market the company operates in. | City - The city where the company&#39;s headquarters is located. | State - The state where the company&#39;s headquarters is located | Newcomer - Indicates whether or not the company is new to the top Fortune 500 (&quot;yes&quot; or &quot;no&quot;). No value will be listed for companies outside of the top 500. | CEO Founder - Indicates whether the CEO of the company is also the founder (&quot;yes&quot; or &quot;no&quot;). | CEO Woman - Indicates whether the CEO of the company is a woman (&quot;yes&quot; or &quot;no&quot;). | Profitable - Indicates whether the company is profitable or not (&quot;yes&quot; or &quot;no&quot;). | Prev. Rank - The 2020 rank of the company, as established by Fortune. There will only be previous rank data for the top 500 companies. | CEO - The name of the CEO of the company | Website - The url of the company website | Ticker - The stock ticker symbol of public companies. Some rows will have empty values because the company is a private corporation. | Market Cap - The market cap (or value) of the company in millions. Some rows will have empty values because the company is private. Market valuations were determined on January 20, 2021. | . !wget -nc /content/ https://datasets21.s3-us-west-1.amazonaws.com/Fortune_1000.csv . /content/: Scheme missing. --2022-05-28 20:30:03-- https://datasets21.s3-us-west-1.amazonaws.com/Fortune_1000.csv Resolving datasets21.s3-us-west-1.amazonaws.com (datasets21.s3-us-west-1.amazonaws.com)... 52.219.192.90 Connecting to datasets21.s3-us-west-1.amazonaws.com (datasets21.s3-us-west-1.amazonaws.com)|52.219.192.90|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 138487 (135K) [text/csv] Saving to: ‘Fortune_1000.csv’ Fortune_1000.csv 100%[===================&gt;] 135.24K --.-KB/s in 0.07s 2022-05-28 20:30:03 (2.02 MB/s) - ‘Fortune_1000.csv’ saved [138487/138487] FINISHED --2022-05-28 20:30:03-- Total wall clock time: 0.3s Downloaded: 1 files, 135K in 0.07s (2.02 MB/s) . import pandas as pd f1000 = pd.read_csv(&#39;Fortune_1000.csv&#39;,index_col=0) . f1000.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Index: 1000 entries, Walmart to Liberty Oilfield Services Data columns (total 17 columns): # Column Non-Null Count Dtype -- -- 0 rank 1000 non-null int64 1 rank_change 1000 non-null float64 2 revenue 1000 non-null float64 3 profit 998 non-null float64 4 num. of employees 1000 non-null int64 5 sector 1000 non-null object 6 city 1000 non-null object 7 state 1000 non-null object 8 newcomer 500 non-null object 9 ceo_founder 1000 non-null object 10 ceo_woman 1000 non-null object 11 profitable 1000 non-null object 12 prev_rank 1000 non-null object 13 CEO 992 non-null object 14 Website 1000 non-null object 15 Ticker 938 non-null object 16 Market Cap 960 non-null object dtypes: float64(3), int64(2), object(12) memory usage: 140.6+ KB . f1000.head(3) . rank rank_change revenue profit num. of employees sector city state newcomer ceo_founder ceo_woman profitable prev_rank CEO Website Ticker Market Cap . company . Walmart 1 | 0.0 | 523964.0 | 14881.0 | 2200000 | Retailing | Bentonville | AR | no | no | no | yes | 1.0 | C. Douglas McMillon | https://www.stock.walmart.com | WMT | 411690 | . Amazon 2 | 3.0 | 280522.0 | 11588.0 | 798000 | Retailing | Seattle | WA | no | yes | no | yes | 5.0 | Jeffrey P. Bezos | https://www.amazon.com | AMZN | 1637405 | . Exxon Mobil 3 | -1.0 | 264938.0 | 14340.0 | 74900 | Energy | Irving | TX | no | no | no | yes | 2.0 | Darren W. Woods | https://www.exxonmobil.com | XOM | 177923 | . Select data using those labels . Because the axes in pandas have labels, I can select data using those labels — unlike in NumPy, where I needed to know the exact index location. To do this, I can use the DataFrame.loc[] attribute. The syntax for DataFrame.loc[] is: . df.loc[row_label, column_label] . Select Single Column . companies = f1000.loc[:,&#39;company&#39;] print(companies) print(type(f1000)) print(type(companies)) . Series object . print(f1000.loc[:,&#39;revenue&#39;]) print(f1000.loc[&#39;Apple&#39;,&#39;revenue&#39;]) . Select multiple columns . List of columns | Slice of columns | . f1000[[&#39;rank&#39;,&#39;revenue&#39;]] #f1000.loc[:,&#39;rank&#39;:&#39;sector&#39;] . Select rows by labels . f1000.loc[[&#39;Amazon&#39;, &#39;Apple&#39;]] #f1000.loc[&#39;Amazon&#39;:&#39;Apple&#39;] . Series.value_counts() method . Series.value_counts() method. This method displays each unique non-null value in a column and their counts in order. . sector_value_counts = f1000[&#39;sector&#39;].value_counts(ascending=True) print(sector_value_counts) . f1000[&#39;sector&#39;].value_counts().loc[&#39;Technology&#39;] . #f1000[&#39;rank_change&#39;].max() #f1000[&#39;rank_change&#39;].min() f1000[&#39;rank_change&#39;].describe() . f1000[&#39;rank_change&#39;].value_counts() . 0.0 544 -1.0 22 -2.0 18 2.0 16 4.0 15 ... 59.0 1 91.0 1 -30.0 1 28.0 1 98.0 1 Name: rank_change, Length: 118, dtype: int64 . Exercise: . List out the numbers of companies in the Fortune 1000 of the top 3 states . top_3_states = f1000[&#39;state&#39;].value_counts().head(3) print(top_3_states) . CA 121 TX 95 NY 89 Name: state, dtype: int64 . Exercise: . find the company that employs the most people in California in the dataset. . I can use the DataFrame.sort_values() method to sort the rows on the employees column . f1000[f1000[&#39;state&#39;]==&#39;CA&#39;].sort_values(&#39;num. of employees&#39;,ascending=False).head(1) . rank rank_change revenue profit num. of employees sector city state newcomer ceo_founder ceo_woman profitable prev_rank CEO Website Ticker Market Cap . company . Wells Fargo 30 | -1.0 | 103915.0 | 19549.0 | 259800 | Financials | San Francisco | CA | no | no | no | yes | 29.0 | Charles W. Scharf | https://www.wellsfargo.com | WFC | 99941 | . Exercise: . find the unique list of states in the dataset . To identify the unique states, I can use the Series.unique() method. This method returns an array of unique values from any series. . states = f1000[&#39;state&#39;].unique() print(states) . [&#39;AR&#39; &#39;WA&#39; &#39;TX&#39; &#39;CA&#39; &#39;RI&#39; &#39;NE&#39; &#39;MN&#39; &#39;PA&#39; &#39;MI&#39; &#39;CT&#39; &#39;OH&#39; &#39;NY&#39; &#39;IL&#39; &#39;DC&#39; &#39;NC&#39; &#39;GA&#39; &#39;IN&#39; &#39;MA&#39; &#39;NJ&#39; &#39;VA&#39; &#39;MO&#39; &#39;TN&#39; &#39;KY&#39; &#39;ID&#39; &#39;MD&#39; &#39;OR&#39; &#39;FL&#39; &#39;WI&#39; &#39;CO&#39; &#39;OK&#39; &#39;LA&#39; &#39;DE&#39; &#39;AZ&#39; &#39;IA&#39; &#39;NV&#39; &#39;KS&#39; &#39;AL&#39; &#39;SC&#39; &#39;ND&#39; &#39;NH&#39; &#39;MS&#39; &#39;PR&#39; &#39;UT&#39; &#39;HI&#39; &#39;VT&#39; &#39;ME&#39;] . Practice: . I&#39;m going to produce the following dictionary of the top employer in each state: . create an empty dictionary, top_employer_by_state to store the results of the exercise. . | Use the Series.unique() method to create an array of unique values from the state column . | Use a for loop to iterate over the array unique states. In each iteration: . | Select only the rows that have a state name equal to the current iteration. | Use DataFrame.sort_values() to sort those rows by the num. of employees column in descending order. | Select the first row from the sorted dataframe and convert the Dataframe into a series using DataFrame.squeeze() | Extract the company name from the index label company by Series.name. | Assign the results to the top_employer_by_state dictionary, using the state name as the key, and the company name as the value. | . top_exmployer_by_state = {} states = f1000[&#39;state&#39;].unique() for state in states: selected_companies = f1000[f1000[&#39;state&#39;]==state] top_exmployer_by_state[state] = selected_companies.sort_values(&#39;num. of employees&#39;, ascending=False).head(1).squeeze().name . for key in top_exmployer_by_state: print(key, &#39; : &#39;, top_exmployer_by_state[key]) . AR : Walmart WA : Amazon TX : Yum China Holdings CA : Wells Fargo RI : CVS Health NE : Berkshire Hathaway MN : Target PA : Aramark MI : Ford Motor CT : XPO Logistics OH : Kroger NY : IBM IL : Walgreens Boots Alliance DC : Danaher NC : Lowe&amp;#8217;s GA : Home Depot IN : Anthem MA : TJX NJ : Cognizant Technology Solutions VA : Hilton Worldwide Holdings MO : Emerson Electric TN : FedEx KY : Humana ID : Albertsons MD : Marriott International OR : Nike FL : Publix Super Markets WI : Kohl&amp;#8217;s CO : VF OK : Helmerich &amp; Payne LA : Lumen Technologies DE : DuPont AZ : Republic Services IA : Casey&amp;#8217;s General Stores NV : MGM Resorts International KS : Yellow AL : Encompass Health SC : Sonoco Products ND : MDU Resources Group NH : PC Connection MS : Sanderson Farms PR : Popular UT : Nu Skin Enterprises HI : Hawaiian Holdings VT : NLV Financial ME : IDEXX Laboratories . Reference . pandas API reference . pandas vs. NumPy .",
            "url": "https://ylu-1258.github.io/YLu-Blog/jupyter/interests/2022/08/25/Pandas-for-Beginners.html",
            "relUrl": "/jupyter/interests/2022/08/25/Pandas-for-Beginners.html",
            "date": " • Aug 25, 2022"
        }
        
    
  
    
        ,"post30": {
            "title": "Intro to Python Lecture",
            "content": "Lecture - 08/25/2022 . To build up a relationshhip with my partner, I should comment in his blog and also open up review tickets to communicate with him . When working with others, always pull before you make any additions, this ensures that we are on the latest version and that there are no difference conflicts. . There are multiple “shells” installed on a linux machine, such as bash, dash and zsh. The commands that we enter into terminal are like a pseudo-language. We can create bash scripts to automate actions in terminal for us. . Bash - Analyzing part 1 . cd $project #cd means &quot;Change Directory&quot;, $project is a variable named project ls # lists directory ls -a # lists directory with hidden files ls -al # lists directory with hidden files in long format . By using bash we can easily create scripts that automate terminal operations for us. . The Cloud . The cloud contains all git repositories. Individual computers can clone repositories from the cloud and down to our own SSD. This creates a link between our local repository and the remote repository. . A pull action will pull any new updates made to the repository down to our local repository and update it with the latest changes. . A push action will push any new updates from our local repository up to our remote directory in the cloud and contribute to the git repository. .",
            "url": "https://ylu-1258.github.io/YLu-Blog/markdown/apcsp/week%201/2022/08/25/Lecture-Intro-Python.html",
            "relUrl": "/markdown/apcsp/week%201/2022/08/25/Lecture-Intro-Python.html",
            "date": " • Aug 25, 2022"
        }
        
    
  
    
        ,"post31": {
            "title": "Intro to Numpy",
            "content": "NumPy . NumPy is the fundamental package for scientific computing with Python. . At the core of the NumPy package, is the ndarray object or n-dimensional array. In programming, array describes a collection of elements, similar to a list. The word n-dimensional refers to the fact that ndarrays can have one or more dimensions. . Here I will directly convert a list to an ndarray using the numpy.array() constructor. To create a 1D ndarray, I can pass in a single list . import numpy as np #using the alias np array_data = np.array([1,2,3,4]) . print(array_data) print(type(array_data)) . [1 2 3 4] &lt;class &#39;numpy.ndarray&#39;&gt; . It&#39;s often useful to know the number of rows and columns in a ndarray. I can use the ndarray.shape attribute. . array_2D = np.array([[5, 10, 15], [20, 25, 30]]) print(array_2D.shape) . (2, 3) . Vectorization in NumPy . The NumPy library takes advantage of a processor feature called Single Instruction Multiple Data (SIMD) to process data faster. SIMD allows a processor to perform the same operation on multiple data points in a single processor cycle. . The concept of replacing for loops with operations applied to multiple data points at once is called vectorization, and ndarrays make vectorization possible. . numbers = [[6, 5], [1, 3], [5, 6], [1, 4], [3, 7], [5, 8], [3, 5], [8, 4]] print(*numbers, sep=&#39; n&#39;) . [6, 5] [1, 3] [5, 6] [1, 4] [3, 7] [5, 8] [3, 5] [8, 4] . sums = [] for row in numbers: row_sum = row[0] + row[1] sums.append(row_sum) print(sums) . [11, 4, 11, 5, 10, 13, 8, 12] . # Convert the list of lists to an ndarray np_numbers = np.array(numbers) sums = np_numbers[:,0] + np_numbers[:,1] print(sums) . [11 4 11 5 10 13 8 12] . When I selected each column, we used the syntax ndarray[:,c] where c is the column index I wanted to select. The colon selects all rows. . #print(np_numbers) #print(np_numbers[1]) #print(np_numbers[2:]) #print(np_numbers[3,1]) #print(np_numbers[:,1]) # Select specific row rows = [0, 2, 4] print(np_numbers[rows, :]) . [[6 5] [5 6] [3 7]] . Explore the numerical dataset . To explore two-dimensional (2D) ndarrays, I&#39;ll analyze New York City taxi trip data released by the city of New York. . !wget -nc /content/ https://datasets21.s3-us-west-1.amazonaws.com/nyc_taxis.csv . /content/: Scheme missing. File ‘nyc_taxis.csv’ already there; not retrieving. . !wget --help . GNU Wget 1.19.4, a non-interactive network retriever. Usage: wget [OPTION]... [URL]... Mandatory arguments to long options are mandatory for short options too. Startup: -V, --version display the version of Wget and exit -h, --help print this help -b, --background go to background after startup -e, --execute=COMMAND execute a `.wgetrc&#39;-style command Logging and input file: -o, --output-file=FILE log messages to FILE -a, --append-output=FILE append messages to FILE -d, --debug print lots of debugging information -q, --quiet quiet (no output) -v, --verbose be verbose (this is the default) -nv, --no-verbose turn off verboseness, without being quiet --report-speed=TYPE output bandwidth as TYPE. TYPE can be bits -i, --input-file=FILE download URLs found in local or external FILE -F, --force-html treat input file as HTML -B, --base=URL resolves HTML input-file links (-i -F) relative to URL --config=FILE specify config file to use --no-config do not read any config file --rejected-log=FILE log reasons for URL rejection to FILE Download: -t, --tries=NUMBER set number of retries to NUMBER (0 unlimits) --retry-connrefused retry even if connection is refused -O, --output-document=FILE write documents to FILE -nc, --no-clobber skip downloads that would download to existing files (overwriting them) --no-netrc don&#39;t try to obtain credentials from .netrc -c, --continue resume getting a partially-downloaded file --start-pos=OFFSET start downloading from zero-based position OFFSET --progress=TYPE select progress gauge type --show-progress display the progress bar in any verbosity mode -N, --timestamping don&#39;t re-retrieve files unless newer than local --no-if-modified-since don&#39;t use conditional if-modified-since get requests in timestamping mode --no-use-server-timestamps don&#39;t set the local file&#39;s timestamp by the one on the server -S, --server-response print server response --spider don&#39;t download anything -T, --timeout=SECONDS set all timeout values to SECONDS --dns-timeout=SECS set the DNS lookup timeout to SECS --connect-timeout=SECS set the connect timeout to SECS --read-timeout=SECS set the read timeout to SECS -w, --wait=SECONDS wait SECONDS between retrievals --waitretry=SECONDS wait 1..SECONDS between retries of a retrieval --random-wait wait from 0.5*WAIT...1.5*WAIT secs between retrievals --no-proxy explicitly turn off proxy -Q, --quota=NUMBER set retrieval quota to NUMBER --bind-address=ADDRESS bind to ADDRESS (hostname or IP) on local host --limit-rate=RATE limit download rate to RATE --no-dns-cache disable caching DNS lookups --restrict-file-names=OS restrict chars in file names to ones OS allows --ignore-case ignore case when matching files/directories -4, --inet4-only connect only to IPv4 addresses -6, --inet6-only connect only to IPv6 addresses --prefer-family=FAMILY connect first to addresses of specified family, one of IPv6, IPv4, or none --user=USER set both ftp and http user to USER --password=PASS set both ftp and http password to PASS --ask-password prompt for passwords --use-askpass=COMMAND specify credential handler for requesting username and password. If no COMMAND is specified the WGET_ASKPASS or the SSH_ASKPASS environment variable is used. --no-iri turn off IRI support --local-encoding=ENC use ENC as the local encoding for IRIs --remote-encoding=ENC use ENC as the default remote encoding --unlink remove file before clobber --xattr turn on storage of metadata in extended file attributes Directories: -nd, --no-directories don&#39;t create directories -x, --force-directories force creation of directories -nH, --no-host-directories don&#39;t create host directories --protocol-directories use protocol name in directories -P, --directory-prefix=PREFIX save files to PREFIX/.. --cut-dirs=NUMBER ignore NUMBER remote directory components HTTP options: --http-user=USER set http user to USER --http-password=PASS set http password to PASS --no-cache disallow server-cached data --default-page=NAME change the default page name (normally this is &#39;index.html&#39;.) -E, --adjust-extension save HTML/CSS documents with proper extensions --ignore-length ignore &#39;Content-Length&#39; header field --header=STRING insert STRING among the headers --max-redirect maximum redirections allowed per page --proxy-user=USER set USER as proxy username --proxy-password=PASS set PASS as proxy password --referer=URL include &#39;Referer: URL&#39; header in HTTP request --save-headers save the HTTP headers to file -U, --user-agent=AGENT identify as AGENT instead of Wget/VERSION --no-http-keep-alive disable HTTP keep-alive (persistent connections) --no-cookies don&#39;t use cookies --load-cookies=FILE load cookies from FILE before session --save-cookies=FILE save cookies to FILE after session --keep-session-cookies load and save session (non-permanent) cookies --post-data=STRING use the POST method; send STRING as the data --post-file=FILE use the POST method; send contents of FILE --method=HTTPMethod use method &#34;HTTPMethod&#34; in the request --body-data=STRING send STRING as data. --method MUST be set --body-file=FILE send contents of FILE. --method MUST be set --content-disposition honor the Content-Disposition header when choosing local file names (EXPERIMENTAL) --content-on-error output the received content on server errors --auth-no-challenge send Basic HTTP authentication information without first waiting for the server&#39;s challenge HTTPS (SSL/TLS) options: --secure-protocol=PR choose secure protocol, one of auto, SSLv2, SSLv3, TLSv1, TLSv1_1, TLSv1_2 and PFS --https-only only follow secure HTTPS links --no-check-certificate don&#39;t validate the server&#39;s certificate --certificate=FILE client certificate file --certificate-type=TYPE client certificate type, PEM or DER --private-key=FILE private key file --private-key-type=TYPE private key type, PEM or DER --ca-certificate=FILE file with the bundle of CAs --ca-directory=DIR directory where hash list of CAs is stored --crl-file=FILE file with bundle of CRLs --pinnedpubkey=FILE/HASHES Public key (PEM/DER) file, or any number of base64 encoded sha256 hashes preceded by &#39;sha256//&#39; and separated by &#39;;&#39;, to verify peer against --random-file=FILE file with random data for seeding the SSL PRNG HSTS options: --no-hsts disable HSTS --hsts-file path of HSTS database (will override default) FTP options: --ftp-user=USER set ftp user to USER --ftp-password=PASS set ftp password to PASS --no-remove-listing don&#39;t remove &#39;.listing&#39; files --no-glob turn off FTP file name globbing --no-passive-ftp disable the &#34;passive&#34; transfer mode --preserve-permissions preserve remote file permissions --retr-symlinks when recursing, get linked-to files (not dir) FTPS options: --ftps-implicit use implicit FTPS (default port is 990) --ftps-resume-ssl resume the SSL/TLS session started in the control connection when opening a data connection --ftps-clear-data-connection cipher the control channel only; all the data will be in plaintext --ftps-fallback-to-ftp fall back to FTP if FTPS is not supported in the target server WARC options: --warc-file=FILENAME save request/response data to a .warc.gz file --warc-header=STRING insert STRING into the warcinfo record --warc-max-size=NUMBER set maximum size of WARC files to NUMBER --warc-cdx write CDX index files --warc-dedup=FILENAME do not store records listed in this CDX file --no-warc-digests do not calculate SHA1 digests --no-warc-keep-log do not store the log file in a WARC record --warc-tempdir=DIRECTORY location for temporary files created by the WARC writer Recursive download: -r, --recursive specify recursive download -l, --level=NUMBER maximum recursion depth (inf or 0 for infinite) --delete-after delete files locally after downloading them -k, --convert-links make links in downloaded HTML or CSS point to local files --convert-file-only convert the file part of the URLs only (usually known as the basename) --backups=N before writing file X, rotate up to N backup files -K, --backup-converted before converting file X, back up as X.orig -m, --mirror shortcut for -N -r -l inf --no-remove-listing -p, --page-requisites get all images, etc. needed to display HTML page --strict-comments turn on strict (SGML) handling of HTML comments Recursive accept/reject: -A, --accept=LIST comma-separated list of accepted extensions -R, --reject=LIST comma-separated list of rejected extensions --accept-regex=REGEX regex matching accepted URLs --reject-regex=REGEX regex matching rejected URLs --regex-type=TYPE regex type (posix|pcre) -D, --domains=LIST comma-separated list of accepted domains --exclude-domains=LIST comma-separated list of rejected domains --follow-ftp follow FTP links from HTML documents --follow-tags=LIST comma-separated list of followed HTML tags --ignore-tags=LIST comma-separated list of ignored HTML tags -H, --span-hosts go to foreign hosts when recursive -L, --relative follow relative links only -I, --include-directories=LIST list of allowed directories --trust-server-names use the name specified by the redirection URL&#39;s last component -X, --exclude-directories=LIST list of excluded directories -np, --no-parent don&#39;t ascend to the parent directory Mail bug reports and suggestions to &lt;bug-wget@gnu.org&gt; . from csv import reader #Load data into the notebook with open(&#39;nyc_taxis.csv&#39;, &#39;r&#39;) as taxis_file: taxis = list(reader(taxis_file)) print(len(taxis)) print(len(taxis[0])) . 2014 15 . import numpy as np np_taxis = np.array(taxis) . print(np_taxis[:3]) np_taxis.shape . [[&#39;pickup_year&#39; &#39;pickup_month&#39; &#39;pickup_day&#39; &#39;pickup_dayofweek&#39; &#39;pickup_time&#39; &#39;pickup_location_code&#39; &#39;dropoff_location_code&#39; &#39;trip_distance&#39; &#39;trip_length&#39; &#39;fare_amount&#39; &#39;fees_amount&#39; &#39;tolls_amount&#39; &#39;tip_amount&#39; &#39;total_amount&#39; &#39;payment_type&#39;] [&#39;2016&#39; &#39;1&#39; &#39;1&#39; &#39;5&#39; &#39;0&#39; &#39;2&#39; &#39;4&#39; &#39;21.00&#39; &#39;2037&#39; &#39;52.00&#39; &#39;0.80&#39; &#39;5.54&#39; &#39;11.65&#39; &#39;69.99&#39; &#39;1&#39;] [&#39;2016&#39; &#39;1&#39; &#39;1&#39; &#39;5&#39; &#39;0&#39; &#39;2&#39; &#39;1&#39; &#39;16.29&#39; &#39;1520&#39; &#39;45.00&#39; &#39;1.30&#39; &#39;0.00&#39; &#39;8.00&#39; &#39;54.30&#39; &#39;1&#39;]] . (2014, 15) . I&#39;ll only work with a subset of the real data — approximately 90,000 yellow taxi trips to and from New York City airports between January and June 2016. This data set includes a 1/50th random sample. Below is information about selected columns from the dataset: . pickup_year: the year of the trip pickup_month: the month of the trip (January is 1, December is 12) | pickup_day: the day of the month of the trip | pickup_location_code: the airport or borough where the trip started | dropoff_location_code: the airport or borough where the trip ended | trip_distance: the distance of the trip in miles | trip_length: the length of the trip in seconds | fare_amount: the base fare of the trip, in dollars | total_amount: the total amount charged to the passenger, including all fees, tolls and tips | . Detailed information on all columns could be found here. . # Access the column by names np_taxis = np.genfromtxt(&#39;nyc_taxis.csv&#39;,delimiter=&#39;,&#39;, names= True) . References: . NumPy genfromtxt function - API Reference . Tutorial for genfromtxt . print(np_taxis.shape) print(np_taxis[10][&#39;pickup_location_code&#39;]) . z = np.array([1,2]) print(z.shape) . y = np.array([[1],[2]]) print(y.shape) . np_taxis = np.genfromtxt(&#39;nyc_taxis.csv&#39;,delimiter=&#39;,&#39;, skip_header=1) . print(np_taxis.shape) . print(np_taxis[10]) . np.set_printoptions(suppress=True) . print(np_taxis[10]) . print(np_taxis[1]) . cols = [5, 6, 9, 10] print(np_taxis[:,cols]) . Get the Frequency Table of taxis based on the Pickup Location . print(np.unique(np_taxis[:,6])) . unique, counts = np.unique(np_taxis[:,6], return_counts=True) print(unique) print(counts) . print(np_taxis[:,-2]) . # Calculate the mph of each trip trip_distance_miles = np_taxis[:,7] trip_length_seconds = np_taxis[:,8] trip_length_hours = trip_length_seconds / 3600 # 3600 seconds is one hour trip_mph = trip_distance_miles / trip_length_hours print(trip_mph) . total_amount = np_taxis[:,-2] # measures of central tendency mean = np.mean(total_amount) median = np.median(total_amount) # measures of dispersion min = np.amin(total_amount) max = np.amax(total_amount) range = np.ptp(total_amount) varience = np.var(total_amount) sd = np.std(total_amount) print(&quot;Descriptive analysis&quot;) print(&quot; n&quot;) print(&quot;Measures of Central Tendency&quot;) print(&quot;Mean =&quot;, mean) print(&quot;Median =&quot;, median) print(&quot;Measures of Dispersion&quot;) print(&quot;Minimum =&quot;, min) print(&quot;Maximum =&quot;, max) print(&quot;Range =&quot;, range) print(&quot;Varience =&quot;, varience) print(&quot;Standard Deviation =&quot;, sd) .",
            "url": "https://ylu-1258.github.io/YLu-Blog/jupyter/interests/2022/08/24/Numpy-for-Beginners.html",
            "relUrl": "/jupyter/interests/2022/08/24/Numpy-for-Beginners.html",
            "date": " • Aug 24, 2022"
        }
        
    
  
    
        ,"post32": {
            "title": "Site Organization and Configuration Lecture",
            "content": "Lecture - 08/24/2022 . Using documents as blog posts . To import past assignemnts and documents from document-based editors such as word and google docs, we can import such files as .docx documents and place them under the _word subdirectory in our blog. . NOTE: Doing so does not keep the formatting of the document, any font colors, size, styling is not preserved, additionall work with CSS is required for original effects. . Blog Front-Matter . Each markdown or jupyter post contains a set configurations at the head of the file known as Front-Matter Front-matter settings are seperated into two main groups, keys, and values. . keys: The name of the configuration or setting we wish to edit value: The value or data we grant to a specific configuration . IMPORTANT: It is crucial to always pair a key with a value, a blank value on the key overwrites the default value, making the key take on a null value and breaking the front-matter To define front matter in markdown, use the following format toc: true layout: post description: APCSP Lecture 2 categories: [markdown, notes] title: 08/24/2022 Lecture author: Alex Lu show_tags: true hide: true comments: true ... . To define front-matter in computational notebooks, use the following format # Jupyter Notebook Demonstration &gt; My first Jupyter notebook on my blog! - toc: true - title: First Jupyter Notebook - author: Alex Lu - badges: true - comments: true - categories: [jupyter] . NOTE: A title and and description must be specified with the # and &gt; characters respectively, furthermore, each front-matter key and value should be prefixed with a hyphen (-) similar to a markdown list. . Adding pages on the navbar . If we ever find the need to add a special page on the top of our site in the navbar, simply move the post into the _pages directory, and change the front matter key layout from post to page . NOTE: setting a table of contents in the front-matter does not work for a page, further tinkering with html is required. . _config.yml . Most of the blog’s default keys and values are defined within the _config.yml configuration file in the base directory of the blog. The values under _config.yml are in the standard key: value syntax prevalent in most .yml files. NOTABLE KEYS: . Key function . title | Title of site in upper left hand corner | . baseurl | The url path to the blog | . show_description | Display brief description of blog post uner blog lists | . show_image | Display image on post card | .",
            "url": "https://ylu-1258.github.io/YLu-Blog/markdown/apcsp/week%201/2022/08/24/Lecture-site-organization-and-configuration.html",
            "relUrl": "/markdown/apcsp/week%201/2022/08/24/Lecture-site-organization-and-configuration.html",
            "date": " • Aug 24, 2022"
        }
        
    
  
    
        ,"post33": {
            "title": "Python Quiz",
            "content": "Here is the code for my quiz . Colors really help to spruce things up a little, can you get 100% without looking at the code? . Example Playthrough: . import getpass class Colors: &#39;&#39;&#39; Colors class to print colored text to terminal, Does not work in fastpages :/ &#39;&#39;&#39; # Man I love ANSI PINK = &#39; 033[95m&#39; LILAC = &#39; 033[94m&#39; BLUE = &#39; 033[96m&#39; GREEN = &#39; 033[92m&#39; YELLOW = &#39; 033[93m&#39; RED = &#39; 033[91m&#39; ENDC = &#39; 033[0m&#39; BOLD = &#39; 033[1m&#39; UNDERLINE = &#39; 033[4m&#39; class Quiz(): # Main class for the quiz def __init__(self): # Quiz &quot;constructor&quot;, initiates list of questions and answers and other settings. self.ques = [] self.ans = [] self.colors = Colors() self.showans = False # Show correct answers after a wrong input self.total = 0 # Total questions self.correct = 0 # Total right self.skipped = 0 # Total skipped def addQues(self, question, answer): &#39;&#39;&#39; Adding Questions and answers to the quiz &#39;&#39;&#39; self.ques.append(question) self.ans.append(answer) self.total += 1 # increment total by 1 def askQues(self, idx): &#39;&#39;&#39; Helper function to ask one question, check answer, increment student data &#39;&#39;&#39; print(self.colors.PINK + &quot;QUESTION: &quot; + self.colors.ENDC + self.colors.BOLD + self.ques[idx] + self.colors.ENDC) # Print Question rsp = input(self.colors.YELLOW + &quot;What is your response? &quot; + self.colors.ENDC) # Get Response if rsp.lower() == self.ans[idx].lower(): # Check answer print(self.colors.GREEN + &quot;YOU ARE CORRECT!&quot; + self.colors.ENDC + &quot; Response &quot; + self.colors.BLUE + rsp + self.colors.ENDC + &quot; is correct!&quot;) self.correct += 1 elif rsp.lower() == &quot;/s&quot;: if self.showans: print(self.colors.YELLOW + &quot;Skipping....&quot; + &quot; The right answer was &quot; + self.colors.BLUE + self.ans[idx] + self.colors.ENDC) else: rint(self.colors.YELLOW + &quot;Skipping....&quot; + self.colors.ENDC) self.skipped += 1 else: if self.showans: print(self.colors.RED + &quot;Response &#39;&quot; + rsp + &quot;&#39; is incorrect.&quot; + self.colors.ENDC + &quot; The right answer was &quot; + self.colors.BLUE + self.ans[idx] + self.colors.ENDC) else: print(self.colors.RED + &quot;Response &#39;&quot; + rsp + &quot;&#39; is incorrect.&quot; + self.colors.ENDC) def percentage(self, x, y): &#39;&#39;&#39; Function to calculate percentage correct &#39;&#39;&#39; return 100 * float(x)/float(y) def playQuiz(self): &#39;&#39;&#39; Main Quiz loop, set settings and ask questions &#39;&#39;&#39; show_ans = input(self.colors.UNDERLINE + &quot;Would you like to show correct answers after incorrect responses? [y/n]&quot; + self.colors.ENDC + &quot; &quot;) print(self.colors.YELLOW + &quot;GOOD LUCK {0}!&quot;.format(getpass.getuser().upper()) + &quot; Type &#39;&quot; + self.colors.BLUE + &quot;/s&quot; + self.colors.YELLOW + &quot;&#39; to skip!&quot;) if show_ans in [&quot;y&quot;, &quot;yes&quot;, &quot;Y&quot;, &quot;YES&quot;]: # Multiple cases of user inputs, assume any other input / no input == False. self.showans = True for i in range(0,self.total): # Iterate over all questions, no repetitive code here self.askQues(i) # Print a little congratulations message print(self.colors.LILAC + &quot;Congratulations! you got &quot; + self.colors.GREEN + &#39;{0:.2f}&#39;.format(self.percentage(self.correct, self.total)) + &quot;%&quot; + self.colors.LILAC + &quot; and {0} questions skipped on this quiz!&quot;.format(self.colors.YELLOW + str(self.skipped) + self.colors.LILAC)) # Creating our quiz q1 = Quiz() # AYO CHEATER STOP LOOKIN HERE q1.addQues(&quot;Name the Python output command mentioned in this lesson?&quot;, &quot;print&quot;) q1.addQues(&quot;If you see many lines of code in order, what would College Board call it?&quot;, &quot;sequence&quot;) q1.addQues(&quot;What keyword in python is used to describe a function?&quot;, &quot;def&quot;) q1.addQues(&quot;What command is used to include other functions that were previously developed?&quot;, &quot;import&quot;) q1.addQues(&quot;What command is used to evaluate correct or incorrect response in this quiz?&quot;, &quot;if&quot;) q1.addQues(&quot;Each &#39;if&#39; command contains an &#39;_________&#39; to determine a true or false condition?&quot;, &quot;expression&quot;) q1.addQues(&quot;What is an input to a function or method called?&quot;, &quot;parameter&quot;) q1.addQues(&quot;If Input is data the computer receives, what is the data that the computer sends back?&quot;, &quot;output&quot;) q1.addQues(&quot;What is a reusable block of code called?&quot;, &quot;function&quot;) q1.addQues(&quot;What operator is used for string concatenation in Python?&quot;, &quot;+&quot;) q1.playQuiz() . GOOD LUCK ERIS29! Type &#39;/s&#39; to skip! QUESTION: Name the Python output command mentioned in this lesson? YOU ARE CORRECT! Response print is correct! QUESTION: If you see many lines of code in order, what would College Board call it? YOU ARE CORRECT! Response sequence is correct! QUESTION: What keyword in python is used to describe a function? YOU ARE CORRECT! Response def is correct! QUESTION: What command is used to include other functions that were previously developed? Response &#39;include&#39; is incorrect. The right answer was import QUESTION: What command is used to evaluate correct or incorrect response in this quiz? YOU ARE CORRECT! Response if is correct! QUESTION: Each &#39;if&#39; command contains an &#39;_________&#39; to determine a true or false condition? Skipping.... The right answer was expression QUESTION: What is an input to a function or method called? YOU ARE CORRECT! Response parameter is correct! QUESTION: If Input is data the computer receives, what is the data that the computer sends back? YOU ARE CORRECT! Response output is correct! QUESTION: What is a reusable block of code called? YOU ARE CORRECT! Response function is correct! QUESTION: What operator is used for string concatenation in Python? YOU ARE CORRECT! Response + is correct! Congratulations! you got 80.00% and 1 questions skipped on this quiz! .",
            "url": "https://ylu-1258.github.io/YLu-Blog/jupyter/apcsp/week%201/2022/08/23/alex-quiz.html",
            "relUrl": "/jupyter/apcsp/week%201/2022/08/23/alex-quiz.html",
            "date": " • Aug 23, 2022"
        }
        
    
  
    
        ,"post34": {
            "title": "Stein EC precis",
            "content": "Alex Lu Mrs. DaFoe APEL, Period 5 21 August, 2022 . The Hypocritical Truth of American Education . In his op-ed, “We’re teaching kids to follow their dreams. Maybe teach them to be helpful instead”, Joel Stein asserts that the American education system is hypocritical in its attempts to teach young Americans about stories of helping others, but never teach them the necessary skills to help others. To prove the sanctimoniousness of our education system, Stein begins with a personal anecdote from his elementary school days, a time where teachers struggled to teach their self-absorbed students about important historical figures. Joel then uses this anecdote in order to introduce his main point of criticism: The so-called “living wax museums” used by teachers today to dress up their students as heroic people allows the students to perform speeches in front of their classmates, giving them a taste of leadership through the lens of a past American hero. While this may sound good on paper, Stein provides yet another anecdote in paragraph 2, this time in the classroom of his 4th-grade son instead, wryly reminiscing the ironic atmosphere of the room一 opposite of what one would expect from an empowering speech. For example, he compares the classroom to a standup comedy show, where “each comedian has to drag in two audience members to fill the house.” Stein continues to scoff at the lunacy of the situation by recalling his son’s preposterous stories about JFK being a sailor and discovering “14 bags of Whoppahs and a Mahs bah.” Through the usage of such personal experiences, Stein develops pathos that appeals to the audience on the failures of our educational system to deliver the very values it must prioritize: knowledge, character, and education. It is essential that Stein appeals to his audience in this way so as to emotionally stir other teachers and parents in his audience to realize the fundamental issues in our schools and the lack of action to amend such ailments in society. Besides anecdotes and pathos, Stein uses exaggeration and satire to convey the ironic state of things. In paragraph 5, Stein openly reveals that each student concluded their speeches with the good qualities of their model and not with the impact or morals behind their actions. Due to this, Stein jokingly states that Malala Yousafzai, a Pakistani activist for female education, would “rescind the right of girls to go to this school” if she were ever to witness these children focusing on her “pursuing her dreams” and not her altruistic actions. Using Malala’s status as a child activist, Stein generates a satirical effect and humorously addresses the misrepresentation of these significant figures by clueless young children and the naive pedagogy constructed by the teachers who organize these events. Stein uses this same example to address how schools have drifted from teaching children to be helpful to others and instead molding them into potential leaders. Finally, he addresses such schools’ hypocrisy once again as they aim to teach students how to become good leaders but quickly turn critical if their students want to become the next “Nixon or Trump”. Through his anecdotes and satire, Stein humorously criticizes the hypocrisy of American schools while maintaining a serious attitude towards the end in an attempt to educate fellow parents and teachers on this crisis. .",
            "url": "https://ylu-1258.github.io/YLu-Blog/markdown/apel/2022/08/21/APEL-Stein-Precis.html",
            "relUrl": "/markdown/apel/2022/08/21/APEL-Stein-Precis.html",
            "date": " • Aug 21, 2022"
        }
        
    
  
    
        ,"post35": {
            "title": "First Jupyter Notebook",
            "content": "Let&#39;s Print Something! . Allow me to demonstrate the print() function in python! . var = &quot;World!&quot; print(&quot;Hello &quot; + var) . Hello World! . Let&#39;s Print some more! . Loops are really fun sometimes, let&#39;s say it a couple more times 🔁 . for i in range(5): print(&quot;Hello &quot; + var) . Hello World! Hello World! Hello World! Hello World! Hello World! . Have a Bogosort algorithm! . The real question is what time complexity this algorithm has 🤔 . from random import shuffle as sh def sorted(arr): length = len(arr) for i in range(0, length-1): if (arr[i] &gt; arr[i+1] ): return False return True def shuffle(arr): sh(arr) def Bogosort(arr): while not sorted(arr): shuffle(arr) return arr array = [23,10,49,9] print(Bogosort(array)) . [9, 10, 23, 49] .",
            "url": "https://ylu-1258.github.io/YLu-Blog/jupyter/2022/08/20/example-post.html",
            "relUrl": "/jupyter/2022/08/20/example-post.html",
            "date": " • Aug 20, 2022"
        }
        
    
  
    
        ,"post36": {
            "title": "Hello World!",
            "content": "Hello World! 🌎 . This is my first blog post! I’m Alex Lu, and I’m excited to learn more about how to build and develop my blog. . Have some code! 💻 . Ayo is that a recursive function? . def fibo(n): if n == 1: return 0 if n == 2: return 1 return fibo(n-1) + fibo(n-2) print(fibo(10)) # prints 34 . That was sick! . Thanks for visiting 🥳 . Have a cookie 🍪 .",
            "url": "https://ylu-1258.github.io/YLu-Blog/markdown/2022/08/19/hello-world.html",
            "relUrl": "/markdown/2022/08/19/hello-world.html",
            "date": " • Aug 19, 2022"
        }
        
    
  
    
        ,"post37": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://ylu-1258.github.io/YLu-Blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post38": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://ylu-1258.github.io/YLu-Blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  

  
      ,"page2": {
          "title": "Vocab",
          "content": "Vocab . The Vocab should be oriented in this fashion . Vocab | Definition | Example | . Vocab Definition Example . Pseudo-Code | A mere representation of actual code, used as a blue print of something we wish to write | DISPLAY(msg) | . Sequence | Any block of code that contains more than one singular line | #include &lt;bits/stdc++.h&gt;; using name space std; int main() { cout «“Hello World”; return 0 } | . Procedural Extraction and Data Extraction | taking little parts of code and putting them into different files and locations to be used as part of a whole | Index.html and other html fragments | .",
          "url": "https://ylu-1258.github.io/YLu-Blog/vocab/",
          "relUrl": "/vocab/",
          "date": ""
      }
      
  

  
      ,"page3": {
          "title": "Lecture Notes",
          "content": "What is this page? . This page contains all of the notes I’ve taken during Mr. Mortensen’s lectures, this might be useful while I’m trouble shooting my blog or other bugs in my code. If you’re another APCSP student, feel free to use this page to further your knowledge or review any points you missed in class! . Lectures TOC . Date Link and subject . 08 - 31 - 2022 | HTML Fragments | . 08 - 29 - 2022 | Data Abstraction in Python | . 08 - 25 - 2022 | Bash and Cloud Lecture | . 08 - 24 - 2022 | Blog posting and configuration Lecture | .",
          "url": "https://ylu-1258.github.io/YLu-Blog/notes/",
          "relUrl": "/notes/",
          "date": ""
      }
      
  

  
      ,"page4": {
          "title": "Javascript Submenu",
          "content": "| Javascript Kernel Usage | Javascript Table Generator | Coin Flip | .",
          "url": "https://ylu-1258.github.io/YLu-Blog/submenu/",
          "relUrl": "/submenu/",
          "date": ""
      }
      
  

  
      ,"page5": {
          "title": "About Me",
          "content": "Hello There! . My name is Alex Lu and if you want to reach out to me about anything, you can contact me through the methods below! . Discord: Eris29#2693 🎮 | Email: maodou1258@gmail.com 📧 | Phone: (858)-688-4567 📱 | . About Me 📓: . I am currently a Junior at Del Norte High School taking the APCSP course, I hope to major in computer science in the future and also pursue it as a prospective carreer path in the future . My Hobbies 🎾 . I have many hobbies that I do to pass my time, here are just a few of them: . Reading | Programming | Playing tennis | Playing video games | Watching youtube videos | And many more! | . My Interests 🔬: . I am interested in anything related to computers, I am currently working on various projects centered around: . Machine Learning 🤖 | Image processing 🖼️ | Webscraping 🌐 | Cyber Security 🐱‍💻 | .",
          "url": "https://ylu-1258.github.io/YLu-Blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  

  
  

  
  

  
  

  
  

  
  

  

  

  
  

  
      ,"page16": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://ylu-1258.github.io/YLu-Blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}